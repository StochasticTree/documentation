"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[2308],{9306:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"r-documentation-markdown/bcf","title":"bcf","description":"Description","source":"@site/docs/r-documentation-markdown/bcf.md","sourceDirName":"r-documentation-markdown","slug":"/r-documentation-markdown/bcf","permalink":"/documentation/docs/r-documentation-markdown/bcf","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/r-documentation-markdown/bcf.md","tags":[],"version":"current","frontMatter":{"title":"bcf"},"sidebar":"tutorialSidebar","previous":{"title":"bart","permalink":"/documentation/docs/r-documentation-markdown/bart"},"next":{"title":"calibrate_inverse_gamma_error_variance","permalink":"/documentation/docs/r-documentation-markdown/calibrate_inverse_gamma_error_variance"}}');var t=r(4848),a=r(8453);const s={title:"bcf"},o="Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation.",d={},l=[{value:"Description",id:"description",level:2},{value:"Usage",id:"usage",level:2},{value:"Arguments",id:"arguments",level:2},{value:"Value",id:"value",level:2},{value:"Examples",id:"examples",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"run-the-bayesian-causal-forest-bcf-algorithm-for-regularized-causal-effect-estimation",children:"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation."})}),"\n",(0,t.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,t.jsx)(n.p,{children:"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation."}),"\n",(0,t.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-r",children:"bcf(\n  X_train,\n  Z_train,\n  y_train,\n  pi_train = NULL,\n  group_ids_train = NULL,\n  rfx_basis_train = NULL,\n  X_test = NULL,\n  Z_test = NULL,\n  pi_test = NULL,\n  group_ids_test = NULL,\n  rfx_basis_test = NULL,\n  num_gfr = 5,\n  num_burnin = 0,\n  num_mcmc = 100,\n  params = list()\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"arguments",children:"Arguments"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"X_train"}),": Covariates used to split trees in the ensemble. May be provided either as a dataframe or a matrix.\nMatrix covariates will be assumed to be all numeric. Covariates passed as a dataframe will be\npreprocessed based on the variable types (e.g. categorical columns stored as unordered factors will be one-hot encoded,\ncategorical columns stored as ordered factors will passed as integers to the core algorithm, along with the metadata\nthat the column is ordered categorical)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"Z_train"}),": Vector of (continuous or binary) treatment assignments."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"y_train"}),": Outcome to be modeled by the ensemble."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"pi_train"}),": (Optional) Vector of propensity scores. If not provided, this will be estimated from the data."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"group_ids_train"}),": (Optional) Group labels used for an additive random effects model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"rfx_basis_train"}),': (Optional) Basis for "random-slope" regression in an additive random effects model.\nIf ',(0,t.jsx)(n.code,{children:"group_ids_train"})," is provided with a regression basis, an intercept-only random effects model\nwill be estimated."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"X_test"}),': (Optional) Test set of covariates used to define "out of sample" evaluation data.\nMay be provided either as a dataframe or a matrix, but the format of ',(0,t.jsx)(n.code,{children:"X_test"})," must be consistent with\nthat of ",(0,t.jsx)(n.code,{children:"X_train"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"Z_test"}),": (Optional) Test set of (continuous or binary) treatment assignments."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"pi_test"}),": (Optional) Vector of propensity scores. If not provided, this will be estimated from the data."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"group_ids_test"}),": (Optional) Test set group labels used for an additive random effects model.\nWe do not currently support (but plan to in the near future), test set evaluation for group labels\nthat were not in the training set."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"rfx_basis_test"}),': (Optional) Test set basis for "random-slope" regression in additive random effects model.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"num_gfr"}),': Number of "warm-start" iterations run using the grow-from-root algorithm (He and Hahn, 2021). Default: 5.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"num_burnin"}),': Number of "burn-in" iterations of the MCMC sampler. Default: 0.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"num_mcmc"}),': Number of "retained" iterations of the MCMC sampler. Default: 100.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"params"}),": The list of model parameters, each of which has a default value.",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"1. Global Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"cutpoint_grid_size"}),' Maximum size of the "grid" of potential cutpoints to consider. Default: ',(0,t.jsx)(n.code,{children:"100"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"a_global"})," Shape parameter in the ",(0,t.jsx)(n.code,{children:"IG(a_global, b_global)"})," global error variance model. Default: ",(0,t.jsx)(n.code,{children:"0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"b_global"})," Scale parameter in the ",(0,t.jsx)(n.code,{children:"IG(a_global, b_global)"})," global error variance model. Default: ",(0,t.jsx)(n.code,{children:"0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"sigma2_init"})," Starting value of global error variance parameter. Calibrated internally as ",(0,t.jsx)(n.code,{children:"pct_var_sigma2_init*var((y-mean(y))/sd(y))"})," if not set."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"pct_var_sigma2_init"})," Percentage of standardized outcome variance used to initialize global error variance parameter. Default: ",(0,t.jsx)(n.code,{children:"1"}),". Superseded by ",(0,t.jsx)(n.code,{children:"sigma2_init"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"variable_weights"})," Numeric weights reflecting the relative probability of splitting on each variable. Does not need to sum to 1 but cannot be negative. Defaults to ",(0,t.jsx)(n.code,{children:"rep(1/ncol(X_train), ncol(X_train))"})," if not set here. Note that if the propensity score is included as a covariate in either forest, its weight will default to ",(0,t.jsx)(n.code,{children:"1/ncol(X_train)"}),". A workaround if you wish to provide a custom weight for the propensity score is to include it as a column in ",(0,t.jsx)(n.code,{children:"X_train"})," and then set ",(0,t.jsx)(n.code,{children:"propensity_covariate"})," to ",(0,t.jsx)(n.code,{children:"'none'"})," adjust ",(0,t.jsx)(n.code,{children:"keep_vars_mu"}),", ",(0,t.jsx)(n.code,{children:"keep_vars_tau"})," and ",(0,t.jsx)(n.code,{children:"keep_vars_variance"})," accordingly."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"propensity_covariate"})," Whether to include the propensity score as a covariate in either or both of the forests. Enter ",(0,t.jsx)(n.code,{children:'"none"'})," for neither, ",(0,t.jsx)(n.code,{children:'"mu"'})," for the prognostic forest, ",(0,t.jsx)(n.code,{children:'"tau"'})," for the treatment forest, and ",(0,t.jsx)(n.code,{children:'"both"'})," for both forests. If this is not ",(0,t.jsx)(n.code,{children:'"none"'})," and a propensity score is not provided, it will be estimated from (",(0,t.jsx)(n.code,{children:"X_train"}),", ",(0,t.jsx)(n.code,{children:"Z_train"}),") using ",(0,t.jsx)(n.code,{children:"stochtree::bart()"}),". Default: ",(0,t.jsx)(n.code,{children:'"mu"'}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"adaptive_coding"}),' Whether or not to use an "adaptive coding" scheme in which a binary treatment variable is not coded manually as (0,1) or (-1,1) but learned via parameters ',(0,t.jsx)(n.code,{children:"b_0"})," and ",(0,t.jsx)(n.code,{children:"b_1"})," that attach to the outcome model ",(0,t.jsx)(n.code,{children:"[b_0 (1-Z) + b_1 Z] tau(X)"}),". This is ignored when Z is not binary. Default: ",(0,t.jsx)(n.code,{children:"TRUE"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"b_0"}),' Initial value of the "control" group coding parameter. This is ignored when Z is not binary. Default: ',(0,t.jsx)(n.code,{children:"-0.5"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"b_1"}),' Initial value of the "treatment" group coding parameter. This is ignored when Z is not binary. Default: ',(0,t.jsx)(n.code,{children:"0.5"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"random_seed"})," Integer parameterizing the C++ random number generator. If not specified, the C++ random number generator is seeded according to ",(0,t.jsx)(n.code,{children:"std::random_device"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"keep_burnin"}),' Whether or not "burnin" samples should be included in cached predictions. Default ',(0,t.jsx)(n.code,{children:"FALSE"}),". Ignored if ",(0,t.jsx)(n.code,{children:"num_mcmc = 0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"keep_gfr"}),' Whether or not "grow-from-root" samples should be included in cached predictions. Default ',(0,t.jsx)(n.code,{children:"FALSE"}),". Ignored if ",(0,t.jsx)(n.code,{children:"num_mcmc = 0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"verbose"})," Whether or not to print progress during the sampling loops. Default: ",(0,t.jsx)(n.code,{children:"FALSE"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"sample_sigma_global"})," Whether or not to update the ",(0,t.jsx)(n.code,{children:"sigma^2"})," global error variance parameter based on ",(0,t.jsx)(n.code,{children:"IG(a_global, b_global)"}),". Default: ",(0,t.jsx)(n.code,{children:"TRUE"}),".",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"2. Prognostic Forest Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"num_trees_mu"})," Number of trees in the prognostic forest. Default: ",(0,t.jsx)(n.code,{children:"200"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"sample_sigma_leaf_mu"})," Whether or not to update the ",(0,t.jsx)(n.code,{children:"sigma_leaf_mu"})," leaf scale variance parameter in the prognostic forest based on ",(0,t.jsx)(n.code,{children:"IG(a_leaf_mu, b_leaf_mu)"}),". Default: ",(0,t.jsx)(n.code,{children:"TRUE"}),".",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"2.1. Tree Prior Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"alpha_mu"})," Prior probability of splitting for a tree of depth 0 for the prognostic forest. Tree split prior combines ",(0,t.jsx)(n.code,{children:"alpha"})," and ",(0,t.jsx)(n.code,{children:"beta"})," via ",(0,t.jsx)(n.code,{children:"alpha_mu*(1+node_depth)^-beta_mu"}),". Default: ",(0,t.jsx)(n.code,{children:"0.95"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"beta_mu"})," Exponent that decreases split probabilities for nodes of depth > 0 for the prognostic forest. Tree split prior combines ",(0,t.jsx)(n.code,{children:"alpha"})," and ",(0,t.jsx)(n.code,{children:"beta"})," via ",(0,t.jsx)(n.code,{children:"alpha_mu*(1+node_depth)^-beta_mu"}),". Default: ",(0,t.jsx)(n.code,{children:"2.0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"min_samples_leaf_mu"})," Minimum allowable size of a leaf, in terms of training samples, for the prognostic forest. Default: ",(0,t.jsx)(n.code,{children:"5"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"max_depth_mu"})," Maximum depth of any tree in the mu ensemble. Default: ",(0,t.jsx)(n.code,{children:"10"}),". Can be overridden with ",(0,t.jsx)(n.code,{children:"-1"})," which does not enforce any depth limits on trees.",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"2.2. Leaf Model Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"keep_vars_mu"})," Vector of variable names or column indices denoting variables that should be included in the prognostic (",(0,t.jsx)(n.code,{children:"mu(X)"}),") forest. Default: ",(0,t.jsx)(n.code,{children:"NULL"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"drop_vars_mu"})," Vector of variable names or column indices denoting variables that should be excluded from the prognostic (",(0,t.jsx)(n.code,{children:"mu(X)"}),") forest. Default: ",(0,t.jsx)(n.code,{children:"NULL"}),". If both ",(0,t.jsx)(n.code,{children:"drop_vars_mu"})," and ",(0,t.jsx)(n.code,{children:"keep_vars_mu"})," are set, ",(0,t.jsx)(n.code,{children:"drop_vars_mu"})," will be ignored."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"sigma_leaf_mu"})," Starting value of leaf node scale parameter for the prognostic forest. Calibrated internally as ",(0,t.jsx)(n.code,{children:"1/num_trees_mu"})," if not set here."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"a_leaf_mu"})," Shape parameter in the ",(0,t.jsx)(n.code,{children:"IG(a_leaf_mu, b_leaf_mu)"})," leaf node parameter variance model for the prognostic forest. Default: ",(0,t.jsx)(n.code,{children:"3"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"b_leaf_mu"})," Scale parameter in the ",(0,t.jsx)(n.code,{children:"IG(a_leaf_mu, b_leaf_mu)"})," leaf node parameter variance model for the prognostic forest. Calibrated internally as ",(0,t.jsx)(n.code,{children:"0.5/num_trees"})," if not set here.",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"3. Treatment Effect Forest Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"num_trees_tau"})," Number of trees in the treatment effect forest. Default: ",(0,t.jsx)(n.code,{children:"50"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"sample_sigma_leaf_tau"})," Whether or not to update the ",(0,t.jsx)(n.code,{children:"sigma_leaf_tau"})," leaf scale variance parameter in the treatment effect forest based on ",(0,t.jsx)(n.code,{children:"IG(a_leaf_tau, b_leaf_tau)"}),". Default: ",(0,t.jsx)(n.code,{children:"TRUE"}),".",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"3.1. Tree Prior Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"alpha_tau"})," Prior probability of splitting for a tree of depth 0 for the treatment effect forest. Tree split prior combines ",(0,t.jsx)(n.code,{children:"alpha"})," and ",(0,t.jsx)(n.code,{children:"beta"})," via ",(0,t.jsx)(n.code,{children:"alpha_tau*(1+node_depth)^-beta_tau"}),". Default: ",(0,t.jsx)(n.code,{children:"0.25"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"beta_tau"})," Exponent that decreases split probabilities for nodes of depth > 0 for the treatment effect forest. Tree split prior combines ",(0,t.jsx)(n.code,{children:"alpha"})," and ",(0,t.jsx)(n.code,{children:"beta"})," via ",(0,t.jsx)(n.code,{children:"alpha_tau*(1+node_depth)^-beta_tau"}),". Default: ",(0,t.jsx)(n.code,{children:"3.0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"min_samples_leaf_tau"})," Minimum allowable size of a leaf, in terms of training samples, for the treatment effect forest. Default: ",(0,t.jsx)(n.code,{children:"5"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"max_depth_tau"})," Maximum depth of any tree in the tau ensemble. Default: ",(0,t.jsx)(n.code,{children:"5"}),". Can be overridden with ",(0,t.jsx)(n.code,{children:"-1"})," which does not enforce any depth limits on trees.",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"3.2. Leaf Model Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"a_leaf_tau"})," Shape parameter in the ",(0,t.jsx)(n.code,{children:"IG(a_leaf, b_leaf)"})," leaf node parameter variance model for the treatment effect forest. Default: ",(0,t.jsx)(n.code,{children:"3"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"b_leaf_tau"})," Scale parameter in the ",(0,t.jsx)(n.code,{children:"IG(a_leaf, b_leaf)"})," leaf node parameter variance model for the treatment effect forest. Calibrated internally as ",(0,t.jsx)(n.code,{children:"0.5/num_trees"})," if not set here."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"keep_vars_tau"})," Vector of variable names or column indices denoting variables that should be included in the treatment effect (",(0,t.jsx)(n.code,{children:"tau(X)"}),") forest. Default: ",(0,t.jsx)(n.code,{children:"NULL"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"drop_vars_tau"})," Vector of variable names or column indices denoting variables that should be excluded from the treatment effect (",(0,t.jsx)(n.code,{children:"tau(X)"}),") forest. Default: ",(0,t.jsx)(n.code,{children:"NULL"}),". If both ",(0,t.jsx)(n.code,{children:"drop_vars_tau"})," and ",(0,t.jsx)(n.code,{children:"keep_vars_tau"})," are set, ",(0,t.jsx)(n.code,{children:"drop_vars_tau"})," will be ignored.",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"4. Conditional Variance Forest Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"num_trees_variance"})," Number of trees in the (optional) conditional variance forest model. Default: ",(0,t.jsx)(n.code,{children:"0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"variance_forest_init"})," Starting value of root forest prediction in conditional (heteroskedastic) error variance model. Calibrated internally as ",(0,t.jsx)(n.code,{children:"log(pct_var_variance_forest_init*var((y-mean(y))/sd(y)))/num_trees_variance"})," if not set."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"pct_var_variance_forest_init"})," Percentage of standardized outcome variance used to initialize global error variance parameter. Default: ",(0,t.jsx)(n.code,{children:"1"}),". Superseded by ",(0,t.jsx)(n.code,{children:"variance_forest_init"}),".",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"4.1. Tree Prior Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"alpha_variance"})," Prior probability of splitting for a tree of depth 0 in the (optional) conditional variance model. Tree split prior combines ",(0,t.jsx)(n.code,{children:"alpha_variance"})," and ",(0,t.jsx)(n.code,{children:"beta_variance"})," via ",(0,t.jsx)(n.code,{children:"alpha_variance*(1+node_depth)^-beta_variance"}),". Default: ",(0,t.jsx)(n.code,{children:"0.95"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"beta_variance"})," Exponent that decreases split probabilities for nodes of depth > 0 in the (optional) conditional variance model. Tree split prior combines ",(0,t.jsx)(n.code,{children:"alpha_variance"})," and ",(0,t.jsx)(n.code,{children:"beta_variance"})," via ",(0,t.jsx)(n.code,{children:"alpha_variance*(1+node_depth)^-beta_variance"}),". Default: ",(0,t.jsx)(n.code,{children:"2.0"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"min_samples_leaf_variance"})," Minimum allowable size of a leaf, in terms of training samples, in the (optional) conditional variance model. Default: ",(0,t.jsx)(n.code,{children:"5"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"max_depth_variance"})," Maximum depth of any tree in the ensemble in the (optional) conditional variance model. Default: ",(0,t.jsx)(n.code,{children:"10"}),". Can be overridden with ",(0,t.jsx)(n.code,{children:"-1"})," which does not enforce any depth limits on trees.",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"4.2. Leaf Model Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"a_forest"})," Shape parameter in the ",(0,t.jsx)(n.code,{children:"IG(a_forest, b_forest)"})," conditional error variance model (which is only sampled if ",(0,t.jsx)(n.code,{children:"num_trees_variance \\> 0"}),"). Calibrated internally as ",(0,t.jsx)(n.code,{children:"num_trees_variance / 1.5^2 + 0.5"})," if not set."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"b_forest"})," Scale parameter in the ",(0,t.jsx)(n.code,{children:"IG(a_forest, b_forest)"})," conditional error variance model (which is only sampled if ",(0,t.jsx)(n.code,{children:"num_trees_variance \\> 0"}),"). Calibrated internally as ",(0,t.jsx)(n.code,{children:"num_trees_variance / 1.5^2"})," if not set."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"keep_vars_variance"})," Vector of variable names or column indices denoting variables that should be included in the (optional) conditional variance forest. Default: ",(0,t.jsx)(n.code,{children:"NULL"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"drop_vars_variance"})," Vector of variable names or column indices denoting variables that should be excluded from the (optional) conditional variance forest. Default: NULL. If both ",(0,t.jsx)(n.code,{children:"drop_vars_variance"})," and ",(0,t.jsx)(n.code,{children:"keep_vars_variance"})," are set, ",(0,t.jsx)(n.code,{children:"drop_vars_variance"})," will be ignored.",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"5. Random Effects Parameters"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"rfx_prior_var"})," Prior on the (diagonals of the) covariance of the additive group-level random regression coefficients. Must be a vector of length ",(0,t.jsx)(n.code,{children:"ncol(rfx_basis_train)"}),". Default: ",(0,t.jsx)(n.code,{children:"rep(1, ncol(rfx_basis_train))"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"value",children:"Value"}),"\n",(0,t.jsx)(n.p,{children:"List of sampling outputs and a wrapper around the sampled forests (which can be used for in-memory prediction on new data, or serialized to JSON on disk)."}),"\n",(0,t.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-r",children:"n \\<- 500\nx1 \\<- rnorm(n)\nx2 \\<- rnorm(n)\nx3 \\<- rnorm(n)\nx4 \\<- as.numeric(rbinom(n,1,0.5))\nx5 \\<- as.numeric(sample(1:3,n,replace=TRUE))\nX \\<- cbind(x1,x2,x3,x4,x5)\np \\<- ncol(X)\ng \\<- function(x) \\{ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))\\}\nmu1 \\<- function(x) \\{1+g(x)+x[,1]*x[,3]\\}\nmu2 \\<- function(x) \\{1+g(x)+6*abs(x[,3]-1)\\}\ntau1 \\<- function(x) \\{rep(3,nrow(x))\\}\ntau2 \\<- function(x) \\{1+2*x[,2]*x[,4]\\}\nmu_x \\<- mu1(X)\ntau_x \\<- tau2(X)\npi_x \\<- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10\nZ \\<- rbinom(n,1,pi_x)\nE_XZ \\<- mu_x + Z*tau_x\nsnr \\<- 4\ny \\<- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr)\nX \\<- as.data.frame(X)\nX$x4 \\<- factor(X$x4, ordered = TRUE)\nX$x5 \\<- factor(X$x5, ordered = TRUE)\ntest_set_pct \\<- 0.2\nn_test \\<- round(test_set_pct*n)\nn_train \\<- n - n_test\ntest_inds \\<- sort(sample(1:n, n_test, replace = FALSE))\ntrain_inds \\<- (1:n)[!((1:n) %in% test_inds)]\nX_test \\<- X[test_inds,]\nX_train \\<- X[train_inds,]\npi_test \\<- pi_x[test_inds]\npi_train \\<- pi_x[train_inds]\nZ_test \\<- Z[test_inds]\nZ_train \\<- Z[train_inds]\ny_test \\<- y[test_inds]\ny_train \\<- y[train_inds]\nmu_test \\<- mu_x[test_inds]\nmu_train \\<- mu_x[train_inds]\ntau_test \\<- tau_x[test_inds]\ntau_train \\<- tau_x[train_inds]\nbcf_model \\<- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train, \n```r\n             X_test = X_test, Z_test = Z_test, pi_test = pi_test)\n"})}),"\n",(0,t.jsx)(n.h1,{id:"plotrowmeansbcf_modelmu_hat_test-mu_test-xlab--predicted-ylab--actual-main--prognostic-function",children:'plot(rowMeans(bcf_model$mu_hat_test), mu_test, xlab = "predicted", ylab = "actual", main = "Prognostic function")'}),"\n",(0,t.jsx)(n.h1,{id:"abline01colredlty3lwd3",children:'abline(0,1,col="red",lty=3,lwd=3)'}),"\n",(0,t.jsx)(n.h1,{id:"plotrowmeansbcf_modeltau_hat_test-tau_test-xlab--predicted-ylab--actual-main--treatment-effect",children:'plot(rowMeans(bcf_model$tau_hat_test), tau_test, xlab = "predicted", ylab = "actual", main = "Treatment effect")'}),"\n",(0,t.jsx)(n.h1,{id:"abline01colredlty3lwd3-1",children:'abline(0,1,col="red",lty=3,lwd=3)'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var i=r(6540);const t={},a=i.createContext(t);function s(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);