"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[3254],{2712:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"r-documentation-markdown/bart","title":"bart","description":"Description","source":"@site/docs/r-documentation-markdown/bart.md","sourceDirName":"r-documentation-markdown","slug":"/r-documentation-markdown/bart","permalink":"/documentation/docs/r-documentation-markdown/bart","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/r-documentation-markdown/bart.md","tags":[],"version":"current","frontMatter":{"title":"bart"},"sidebar":"tutorialSidebar","previous":{"title":"RandomEffectsTracker","permalink":"/documentation/docs/r-documentation-markdown/RandomEffectsTracker"},"next":{"title":"bcf","permalink":"/documentation/docs/r-documentation-markdown/bcf"}}');var a=r(4848),t=r(8453);const s={title:"bart"},o="Run the BART algorithm for supervised learning.",d={},l=[{value:"Description",id:"description",level:2},{value:"Usage",id:"usage",level:2},{value:"Arguments",id:"arguments",level:2},{value:"Value",id:"value",level:2},{value:"Examples",id:"examples",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"run-the-bart-algorithm-for-supervised-learning",children:"Run the BART algorithm for supervised learning."})}),"\n",(0,a.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,a.jsx)(n.p,{children:"Run the BART algorithm for supervised learning."}),"\n",(0,a.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-r",children:"bart(\n  X_train,\n  y_train,\n  W_train = NULL,\n  group_ids_train = NULL,\n  rfx_basis_train = NULL,\n  X_test = NULL,\n  W_test = NULL,\n  group_ids_test = NULL,\n  rfx_basis_test = NULL,\n  num_gfr = 5,\n  num_burnin = 0,\n  num_mcmc = 100,\n  params = list()\n)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"arguments",children:"Arguments"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"X_train"}),": Covariates used to split trees in the ensemble. May be provided either as a dataframe or a matrix.\nMatrix covariates will be assumed to be all numeric. Covariates passed as a dataframe will be\npreprocessed based on the variable types (e.g. categorical columns stored as unordered factors will be one-hot encoded,\ncategorical columns stored as ordered factors will passed as integers to the core algorithm, along with the metadata\nthat the column is ordered categorical)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"y_train"}),": Outcome to be modeled by the ensemble."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"W_train"}),": (Optional) Bases used to define a regression model ",(0,a.jsx)(n.code,{children:"y ~ W"})," in\neach leaf of each regression tree. By default, BART assumes constant leaf node\nparameters, implicitly regressing on a constant basis of ones (i.e. ",(0,a.jsx)(n.code,{children:"y ~ 1"}),")."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"group_ids_train"}),": (Optional) Group labels used for an additive random effects model."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"rfx_basis_train"}),': (Optional) Basis for "random-slope" regression in an additive random effects model.\nIf ',(0,a.jsx)(n.code,{children:"group_ids_train"})," is provided with a regression basis, an intercept-only random effects model\nwill be estimated."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"X_test"}),': (Optional) Test set of covariates used to define "out of sample" evaluation data.\nMay be provided either as a dataframe or a matrix, but the format of ',(0,a.jsx)(n.code,{children:"X_test"})," must be consistent with\nthat of ",(0,a.jsx)(n.code,{children:"X_train"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"W_test"}),': (Optional) Test set of bases used to define "out of sample" evaluation data.\nWhile a test set is optional, the structure of any provided test set must match that\nof the training set (i.e. if both X_train and W_train are provided, then a test set must\nconsist of X_test and W_test with the same number of columns).']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"group_ids_test"}),": (Optional) Test set group labels used for an additive random effects model.\nWe do not currently support (but plan to in the near future), test set evaluation for group labels\nthat were not in the training set."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"rfx_basis_test"}),': (Optional) Test set basis for "random-slope" regression in additive random effects model.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"num_gfr"}),': Number of "warm-start" iterations run using the grow-from-root algorithm (He and Hahn, 2021). Default: 5.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"num_burnin"}),': Number of "burn-in" iterations of the MCMC sampler. Default: 0.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"num_mcmc"}),': Number of "retained" iterations of the MCMC sampler. Default: 100.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"params"}),": The list of model parameters, each of which has a default value.",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"1. Global Parameters"})})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"cutpoint_grid_size"}),' Maximum size of the "grid" of potential cutpoints to consider. Default: ',(0,a.jsx)(n.code,{children:"100"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sigma2_init"})," Starting value of global error variance parameter. Calibrated internally as ",(0,a.jsx)(n.code,{children:"pct_var_sigma2_init*var((y-mean(y))/sd(y))"})," if not set."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"pct_var_sigma2_init"})," Percentage of standardized outcome variance used to initialize global error variance parameter. Default: ",(0,a.jsx)(n.code,{children:"1"}),". Superseded by ",(0,a.jsx)(n.code,{children:"sigma2_init"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"variance_scale"})," Variance after the data have been scaled. Default: ",(0,a.jsx)(n.code,{children:"1"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"a_global"})," Shape parameter in the ",(0,a.jsx)(n.code,{children:"IG(a_global, b_global)"})," global error variance model. Default: ",(0,a.jsx)(n.code,{children:"0"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"b_global"})," Scale parameter in the ",(0,a.jsx)(n.code,{children:"IG(a_global, b_global)"})," global error variance model. Default: ",(0,a.jsx)(n.code,{children:"0"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"random_seed"})," Integer parameterizing the C++ random number generator. If not specified, the C++ random number generator is seeded according to ",(0,a.jsx)(n.code,{children:"std::random_device"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sample_sigma_global"})," Whether or not to update the ",(0,a.jsx)(n.code,{children:"sigma^2"})," global error variance parameter based on ",(0,a.jsx)(n.code,{children:"IG(a_global, b_global)"}),". Default: ",(0,a.jsx)(n.code,{children:"TRUE"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"keep_burnin"}),' Whether or not "burnin" samples should be included in cached predictions. Default ',(0,a.jsx)(n.code,{children:"FALSE"}),". Ignored if ",(0,a.jsx)(n.code,{children:"num_mcmc = 0"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"keep_gfr"}),' Whether or not "grow-from-root" samples should be included in cached predictions. Default ',(0,a.jsx)(n.code,{children:"TRUE"}),". Ignored if ",(0,a.jsx)(n.code,{children:"num_mcmc = 0"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"verbose"})," Whether or not to print progress during the sampling loops. Default: ",(0,a.jsx)(n.code,{children:"FALSE"}),".",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"2. Mean Forest Parameters"})})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"num_trees_mean"})," Number of trees in the ensemble for the conditional mean model. Default: ",(0,a.jsx)(n.code,{children:"200"}),". If ",(0,a.jsx)(n.code,{children:"num_trees_mean = 0"}),", the conditional mean will not be modeled using a forest, and the function will only proceed if ",(0,a.jsx)(n.code,{children:"num_trees_variance \\> 0"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sample_sigma_leaf"})," Whether or not to update the ",(0,a.jsx)(n.code,{children:"tau"})," leaf scale variance parameter based on ",(0,a.jsx)(n.code,{children:"IG(a_leaf, b_leaf)"}),". Cannot (currently) be set to true if ",(0,a.jsx)(n.code,{children:"ncol(W_train)\\>1"}),". Default: ",(0,a.jsx)(n.code,{children:"FALSE"}),".",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"2.1. Tree Prior Parameters"})})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"alpha_mean"})," Prior probability of splitting for a tree of depth 0 in the mean model. Tree split prior combines ",(0,a.jsx)(n.code,{children:"alpha_mean"})," and ",(0,a.jsx)(n.code,{children:"beta_mean"})," via ",(0,a.jsx)(n.code,{children:"alpha_mean*(1+node_depth)^-beta_mean"}),". Default: ",(0,a.jsx)(n.code,{children:"0.95"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"beta_mean"})," Exponent that decreases split probabilities for nodes of depth > 0 in the mean model. Tree split prior combines ",(0,a.jsx)(n.code,{children:"alpha_mean"})," and ",(0,a.jsx)(n.code,{children:"beta_mean"})," via ",(0,a.jsx)(n.code,{children:"alpha_mean*(1+node_depth)^-beta_mean"}),". Default: ",(0,a.jsx)(n.code,{children:"2"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"min_samples_leaf_mean"})," Minimum allowable size of a leaf, in terms of training samples, in the mean model. Default: ",(0,a.jsx)(n.code,{children:"5"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"max_depth_mean"})," Maximum depth of any tree in the ensemble in the mean model. Default: ",(0,a.jsx)(n.code,{children:"10"}),". Can be overridden with ",(0,a.jsx)(n.code,{children:"-1"})," which does not enforce any depth limits on trees.",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"2.2. Leaf Model Parameters"})})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"variable_weights_mean"})," Numeric weights reflecting the relative probability of splitting on each variable in the mean forest. Does not need to sum to 1 but cannot be negative. Defaults to ",(0,a.jsx)(n.code,{children:"rep(1/ncol(X_train), ncol(X_train))"})," if not set here."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sigma_leaf_init"})," Starting value of leaf node scale parameter. Calibrated internally as ",(0,a.jsx)(n.code,{children:"1/num_trees_mean"})," if not set here."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"a_leaf"})," Shape parameter in the ",(0,a.jsx)(n.code,{children:"IG(a_leaf, b_leaf)"})," leaf node parameter variance model. Default: ",(0,a.jsx)(n.code,{children:"3"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"b_leaf"})," Scale parameter in the ",(0,a.jsx)(n.code,{children:"IG(a_leaf, b_leaf)"})," leaf node parameter variance model. Calibrated internally as ",(0,a.jsx)(n.code,{children:"0.5/num_trees_mean"})," if not set here.",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"3. Conditional Variance Forest Parameters"})})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"num_trees_variance"})," Number of trees in the ensemble for the conditional variance model. Default: ",(0,a.jsx)(n.code,{children:"0"}),". Variance is only modeled using a tree / forest if ",(0,a.jsx)(n.code,{children:"num_trees_variance \\> 0"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"variance_forest_init"})," Starting value of root forest prediction in conditional (heteroskedastic) error variance model. Calibrated internally as ",(0,a.jsx)(n.code,{children:"log(pct_var_variance_forest_init*var((y-mean(y))/sd(y)))/num_trees_variance"})," if not set."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"pct_var_variance_forest_init"})," Percentage of standardized outcome variance used to initialize global error variance parameter. Default: ",(0,a.jsx)(n.code,{children:"1"}),". Superseded by ",(0,a.jsx)(n.code,{children:"variance_forest_init"}),".",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"3.1. Tree Prior Parameters"})})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"alpha_variance"})," Prior probability of splitting for a tree of depth 0 in the variance model. Tree split prior combines ",(0,a.jsx)(n.code,{children:"alpha_variance"})," and ",(0,a.jsx)(n.code,{children:"beta_variance"})," via ",(0,a.jsx)(n.code,{children:"alpha_variance*(1+node_depth)^-beta_variance"}),". Default: ",(0,a.jsx)(n.code,{children:"0.95"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"beta_variance"})," Exponent that decreases split probabilities for nodes of depth > 0 in the variance model. Tree split prior combines ",(0,a.jsx)(n.code,{children:"alpha_variance"})," and ",(0,a.jsx)(n.code,{children:"beta_variance"})," via ",(0,a.jsx)(n.code,{children:"alpha_variance*(1+node_depth)^-beta_variance"}),". Default: ",(0,a.jsx)(n.code,{children:"2"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"min_samples_leaf_variance"})," Minimum allowable size of a leaf, in terms of training samples, in the variance model. Default: ",(0,a.jsx)(n.code,{children:"5"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"max_depth_variance"})," Maximum depth of any tree in the ensemble in the variance model. Default: ",(0,a.jsx)(n.code,{children:"10"}),". Can be overridden with ",(0,a.jsx)(n.code,{children:"-1"})," which does not enforce any depth limits on trees.",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"3.2. Leaf Model Parameters"})})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"variable_weights_variance"})," Numeric weights reflecting the relative probability of splitting on each variable in the variance forest. Does not need to sum to 1 but cannot be negative. Defaults to ",(0,a.jsx)(n.code,{children:"rep(1/ncol(X_train), ncol(X_train))"})," if not set here."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sigma_leaf_init"})," Starting value of leaf node scale parameter. Calibrated internally as ",(0,a.jsx)(n.code,{children:"1/num_trees_mean"})," if not set here."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"a_forest"})," Shape parameter in the ",(0,a.jsx)(n.code,{children:"IG(a_forest, b_forest)"})," conditional error variance model (which is only sampled if ",(0,a.jsx)(n.code,{children:"num_trees_variance \\> 0"}),"). Calibrated internally as ",(0,a.jsx)(n.code,{children:"num_trees_variance / 1.5^2 + 0.5"})," if not set."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"b_forest"})," Scale parameter in the ",(0,a.jsx)(n.code,{children:"IG(a_forest, b_forest)"})," conditional error variance model (which is only sampled if ",(0,a.jsx)(n.code,{children:"num_trees_variance \\> 0"}),"). Calibrated internally as ",(0,a.jsx)(n.code,{children:"num_trees_variance / 1.5^2"})," if not set."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"value",children:"Value"}),"\n",(0,a.jsx)(n.p,{children:"List of sampling outputs and a wrapper around the sampled forests (which can be used for in-memory prediction on new data, or serialized to JSON on disk)."}),"\n",(0,a.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-r",children:"n \\<- 100\np \\<- 5\nX \\<- matrix(runif(n*p), ncol = p)\nf_XW \\<- (\n```r\n((0 \\<= X[,1]) & (0.25 \\> X[,1])) * (-7.5) + \n((0.25 \\<= X[,1]) & (0.5 \\> X[,1])) * (-2.5) + \n((0.5 \\<= X[,1]) & (0.75 \\> X[,1])) * (2.5) + \n((0.75 \\<= X[,1]) & (1 \\> X[,1])) * (7.5)\n"})}),"\n",(0,a.jsxs)(n.p,{children:[")\nnoise_sd <- 1\ny <- f_XW + rnorm(n, 0, noise_sd)\ntest_set_pct <- 0.2\nn_test <- round(test_set_pct*n)\nn_train <- n - n_test\ntest_inds <- sort(sample(1",":n",", n_test, replace = FALSE))\ntrain_inds <- (1",":n",")[!((1",":n",") %in% test_inds)]\nX_test <- X[test_inds,]\nX_train <- X[train_inds,]\ny_test <- y[test_inds]\ny_train <- y[train_inds]\nbart_model <- bart(X_train = X_train, y_train = y_train, X_test = X_test)"]}),"\n",(0,a.jsx)(n.h1,{id:"plotrowmeansbart_modely_hat_test-y_test-xlab--predicted-ylab--actual",children:'plot(rowMeans(bart_model$y_hat_test), y_test, xlab = "predicted", ylab = "actual")'}),"\n",(0,a.jsx)(n.h1,{id:"abline01colredlty3lwd3",children:'abline(0,1,col="red",lty=3,lwd=3)'}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var i=r(6540);const a={},t=i.createContext(a);function s(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);