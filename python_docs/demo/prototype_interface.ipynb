{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Level Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the functions `bart()` and `bcf()` provide simple and performant \n",
    "interfaces for supervised learning / causal inference, `stochtree` also \n",
    "offers access to many of the \"low-level\" data structures that are typically \n",
    "implemented in C++.\n",
    "This low-level interface is not designed for performance or even\n",
    "simplicity --- rather the intent is to provide a \"prototype\" interface\n",
    "to the C++ code that doesn't require modifying any C++.\n",
    "\n",
    "To illustrate when such a prototype interface might be useful, consider\n",
    "that that \"classic\" BART algorithm is essentially a Metropolis-within-Gibbs \n",
    "sampler, in which the forest is sampled by MCMC, conditional on all of the \n",
    "other model parameters, and then the model parameters are updated by Gibbs.\n",
    "\n",
    "While the algorithm itself is conceptually simple, much of the core \n",
    "computation is carried out in low-level languages such as C or C++ \n",
    "because of the tree data structures. As a result, any changes to this \n",
    "algorithm, such as supporting heteroskedasticity and categorical outcomes (Murray 2021) \n",
    "or causal effect estimation (Hahn et al 2020) require modifying low-level code. \n",
    "\n",
    "The prototype interface exposes the core components of the \n",
    "loop above at the R level, thus making it possible to interchange \n",
    "C++ computation for steps like \"update forest via Metropolis-Hastings\" \n",
    "with R computation for a custom variance model, other user-specified additive \n",
    "mean model components, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from stochtree import (\n",
    "    RNG,\n",
    "    Dataset,\n",
    "    Forest,\n",
    "    ForestContainer,\n",
    "    ForestSampler,\n",
    "    GlobalVarianceModel,\n",
    "    LeafVarianceModel,\n",
    "    Residual, \n",
    "    ForestModelConfig, \n",
    "    GlobalModelConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNG\n",
    "random_seed = 1234\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "# Generate covariates and basis\n",
    "n = 500\n",
    "p_X = 10\n",
    "p_W = 1\n",
    "X = rng.uniform(0, 1, (n, p_X))\n",
    "W = rng.uniform(0, 1, (n, p_W))\n",
    "\n",
    "# Define the outcome mean function\n",
    "def outcome_mean(X, W):\n",
    "    return np.where(\n",
    "        (X[:, 0] >= 0.0) & (X[:, 0] < 0.25),\n",
    "        -7.5 * W[:, 0],\n",
    "        np.where(\n",
    "            (X[:, 0] >= 0.25) & (X[:, 0] < 0.5),\n",
    "            -2.5 * W[:, 0],\n",
    "            np.where((X[:, 0] >= 0.5) & (X[:, 0] < 0.75), 2.5 * W[:, 0], 7.5 * W[:, 0]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate outcome\n",
    "epsilon = rng.normal(0, 1, n)\n",
    "y = outcome_mean(X, W) + epsilon\n",
    "\n",
    "# Standardize outcome\n",
    "y_bar = np.mean(y)\n",
    "y_std = np.std(y)\n",
    "resid = (y - y_bar) / y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some sampling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "beta = 1.25\n",
    "min_samples_leaf = 1\n",
    "max_depth = -1\n",
    "num_trees = 100\n",
    "cutpoint_grid_size = 100\n",
    "global_variance_init = 1.0\n",
    "tau_init = 0.5\n",
    "leaf_prior_scale = np.array([[tau_init]], order=\"C\")\n",
    "a_global = 4.0\n",
    "b_global = 2.0\n",
    "a_leaf = 2.0\n",
    "b_leaf = 0.5\n",
    "leaf_regression = True\n",
    "feature_types = np.repeat(0, p_X).astype(int)  # 0 = numeric\n",
    "var_weights = np.repeat(1 / p_X, p_X)\n",
    "if not leaf_regression:\n",
    "    leaf_model = 0\n",
    "    leaf_dimension = 1\n",
    "elif leaf_regression and p_W == 1:\n",
    "    leaf_model = 1\n",
    "    leaf_dimension = 1\n",
    "else:\n",
    "    leaf_model = 2\n",
    "    leaf_dimension = p_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data from numpy to `StochTree` representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset (covariates and basis)\n",
    "dataset = Dataset()\n",
    "dataset.add_covariates(X)\n",
    "dataset.add_basis(W)\n",
    "\n",
    "# Residual\n",
    "residual = Residual(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize tracking and sampling classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_container = ForestContainer(num_trees, W.shape[1], False, False)\n",
    "active_forest = Forest(num_trees, W.shape[1], False, False)\n",
    "global_model_config = GlobalModelConfig(global_error_variance=global_variance_init)\n",
    "forest_model_config = ForestModelConfig(\n",
    "    num_trees=num_trees,\n",
    "    num_features=p_X,\n",
    "    num_observations=n,\n",
    "    feature_types=feature_types,\n",
    "    variable_weights=var_weights,\n",
    "    leaf_dimension=1,\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    max_depth=max_depth,\n",
    "    leaf_model_type=leaf_model,\n",
    "    leaf_model_scale=leaf_prior_scale,\n",
    "    cutpoint_grid_size=cutpoint_grid_size,\n",
    ")\n",
    "forest_sampler = ForestSampler(\n",
    "    dataset, global_model_config, forest_model_config\n",
    ")\n",
    "cpp_rng = RNG(random_seed)\n",
    "global_var_model = GlobalVarianceModel()\n",
    "leaf_var_model = LeafVarianceModel()\n",
    "\n",
    "# Initialize the leaves of each tree in the mean forest\n",
    "if leaf_regression:\n",
    "    forest_init_val = np.repeat(0.0, W.shape[1])\n",
    "else:\n",
    "    forest_init_val = np.array([0.0])\n",
    "forest_sampler.prepare_for_sampler(\n",
    "    dataset,\n",
    "    residual,\n",
    "    active_forest,\n",
    "    leaf_model,\n",
    "    forest_init_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to run the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmstart = 10\n",
    "num_mcmc = 100\n",
    "num_samples = num_warmstart + num_mcmc\n",
    "global_var_samples = np.concatenate(\n",
    "    (np.array([global_variance_init]), np.repeat(0, num_samples))\n",
    ")\n",
    "leaf_scale_samples = np.concatenate((np.array([tau_init]), np.repeat(0, num_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the \"grow-from-root\" (XBART) sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_warmstart):\n",
    "    forest_sampler.sample_one_iteration(\n",
    "        forest_container,\n",
    "        active_forest,\n",
    "        dataset,\n",
    "        residual,\n",
    "        cpp_rng,\n",
    "        global_model_config, \n",
    "        forest_model_config,\n",
    "        True,\n",
    "        True,\n",
    "    )\n",
    "    global_var_samples[i + 1] = global_var_model.sample_one_iteration(\n",
    "        residual, cpp_rng, a_global, b_global\n",
    "    )\n",
    "    leaf_scale_samples[i + 1] = leaf_var_model.sample_one_iteration(\n",
    "        active_forest, cpp_rng, a_leaf, b_leaf\n",
    "    )\n",
    "    leaf_prior_scale[0, 0] = leaf_scale_samples[i + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the MCMC (BART) sampler, initialized at the last XBART sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_warmstart, num_samples):\n",
    "    forest_sampler.sample_one_iteration(\n",
    "        forest_container,\n",
    "        active_forest,\n",
    "        dataset,\n",
    "        residual,\n",
    "        cpp_rng,\n",
    "        global_model_config, \n",
    "        forest_model_config,\n",
    "        True,\n",
    "        False,\n",
    "    )\n",
    "    global_var_samples[i + 1] = global_var_model.sample_one_iteration(\n",
    "        residual, cpp_rng, a_global, b_global\n",
    "    )\n",
    "    leaf_scale_samples[i + 1] = leaf_var_model.sample_one_iteration(\n",
    "        active_forest, cpp_rng, a_leaf, b_leaf\n",
    "    )\n",
    "    leaf_prior_scale[0, 0] = leaf_scale_samples[i + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract mean function and error variance posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest predictions\n",
    "forest_preds = forest_container.predict(dataset) * y_std + y_bar\n",
    "forest_preds_gfr = forest_preds[:, :num_warmstart]\n",
    "forest_preds_mcmc = forest_preds[:, num_warmstart:num_samples]\n",
    "\n",
    "# Global error variance\n",
    "sigma2_samples = global_var_samples * y_std * y_std\n",
    "sigma2_samples_gfr = sigma2_samples[:num_warmstart]\n",
    "sigma2_samples_mcmc = sigma2_samples[num_warmstart:num_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the GFR (XBART) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_gfr = forest_preds_gfr.mean(axis=1, keepdims=True)\n",
    "forest_pred_df_gfr = pd.DataFrame(\n",
    "    np.concatenate((np.expand_dims(y, axis=1), forest_pred_avg_gfr), axis=1),\n",
    "    columns=[\"True y\", \"Average predicted y\"],\n",
    ")\n",
    "sns.scatterplot(data=forest_pred_df_gfr, x=\"True y\", y=\"Average predicted y\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_df_gfr = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            np.expand_dims(np.arange(num_warmstart), axis=1),\n",
    "            np.expand_dims(sigma2_samples_gfr, axis=1),\n",
    "        ),\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Sample\", \"Sigma^2\"],\n",
    ")\n",
    "sns.scatterplot(data=sigma_df_gfr, x=\"Sample\", y=\"Sigma^2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the MCMC (BART) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_mcmc = forest_preds_mcmc.mean(axis=1, keepdims=True)\n",
    "forest_pred_df_mcmc = pd.DataFrame(\n",
    "    np.concatenate((np.expand_dims(y, axis=1), forest_pred_avg_mcmc), axis=1),\n",
    "    columns=[\"True y\", \"Average predicted y\"],\n",
    ")\n",
    "sns.scatterplot(data=forest_pred_df_mcmc, x=\"True y\", y=\"Average predicted y\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_df_mcmc = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            np.expand_dims(np.arange(num_samples - num_warmstart), axis=1),\n",
    "            np.expand_dims(sigma2_samples_mcmc, axis=1),\n",
    "        ),\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Sample\", \"Sigma^2\"],\n",
    ")\n",
    "sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Causal Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNG\n",
    "random_seed = 101\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "# Generate covariates and basis\n",
    "n = 500\n",
    "p_X = 5\n",
    "X = rng.uniform(0, 1, (n, p_X))\n",
    "pi_X = 0.35 + 0.3 * X[:, 0]\n",
    "Z = rng.binomial(1, pi_X, n).astype(float)\n",
    "\n",
    "# Define the outcome mean functions (prognostic and treatment effects)\n",
    "mu_X = (pi_X - 0.5) * 30\n",
    "# tau_X = np.sin(X[:,1]*2*np.pi)\n",
    "tau_X = X[:, 1] * 2\n",
    "\n",
    "# Generate outcome\n",
    "epsilon = rng.normal(0, 1, n)\n",
    "y = mu_X + tau_X * Z + epsilon\n",
    "\n",
    "# Standardize outcome\n",
    "y_bar = np.mean(y)\n",
    "y_std = np.std(y)\n",
    "resid = (y - y_bar) / y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some sampling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prognostic forest parameters\n",
    "alpha_mu = 0.95\n",
    "beta_mu = 2.0\n",
    "min_samples_leaf_mu = 1\n",
    "max_depth_mu = -1\n",
    "num_trees_mu = 200\n",
    "cutpoint_grid_size_mu = 100\n",
    "tau_init_mu = 1 / num_trees_mu\n",
    "leaf_prior_scale_mu = np.array([[tau_init_mu]], order=\"C\")\n",
    "a_leaf_mu = 3.0\n",
    "b_leaf_mu = 1 / num_trees_mu\n",
    "leaf_regression_mu = False\n",
    "feature_types_mu = np.repeat(0, p_X + 1).astype(int)  # 0 = numeric\n",
    "var_weights_mu = np.repeat(1 / (p_X + 1), p_X + 1)\n",
    "leaf_model_mu = 0\n",
    "leaf_dimension_mu = 1\n",
    "\n",
    "# Treatment forest parameters\n",
    "alpha_tau = 0.75\n",
    "beta_tau = 3.0\n",
    "min_samples_leaf_tau = 1\n",
    "max_depth_tau = -1\n",
    "num_trees_tau = 100\n",
    "cutpoint_grid_size_tau = 100\n",
    "tau_init_tau = 1 / num_trees_tau\n",
    "leaf_prior_scale_tau = np.array([[tau_init_tau]], order=\"C\")\n",
    "a_leaf_tau = 3.0\n",
    "b_leaf_tau = 1 / num_trees_tau\n",
    "leaf_regression_tau = True\n",
    "feature_types_tau = np.repeat(0, p_X).astype(int)  # 0 = numeric\n",
    "var_weights_tau = np.repeat(1 / p_X, p_X)\n",
    "leaf_model_tau = 1\n",
    "leaf_dimension_tau = 1\n",
    "\n",
    "# Global parameters\n",
    "a_global = 2.0\n",
    "b_global = 1.0\n",
    "global_variance_init = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data from numpy to `StochTree` representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prognostic Forest Dataset (covariates)\n",
    "dataset_mu = Dataset()\n",
    "dataset_mu.add_covariates(np.c_[X, pi_X])\n",
    "\n",
    "# Treatment Forest Dataset (covariates and treatment variable)\n",
    "dataset_tau = Dataset()\n",
    "dataset_tau.add_covariates(X)\n",
    "dataset_tau.add_basis(Z)\n",
    "\n",
    "# Residual\n",
    "residual = Residual(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize tracking and sampling classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global classes\n",
    "global_model_config = GlobalModelConfig(global_error_variance=global_variance_init)\n",
    "cpp_rng = RNG(random_seed)\n",
    "global_var_model = GlobalVarianceModel()\n",
    "\n",
    "# Prognostic forest sampling classes\n",
    "forest_container_mu = ForestContainer(num_trees_mu, 1, True, False)\n",
    "active_forest_mu = Forest(num_trees_mu, 1, True, False)\n",
    "forest_model_config_mu = ForestModelConfig(\n",
    "    num_trees=num_trees_mu,\n",
    "    num_features=p_X + 1,\n",
    "    num_observations=n,\n",
    "    feature_types=feature_types_mu,\n",
    "    variable_weights=var_weights_mu,\n",
    "    leaf_dimension=leaf_dimension_mu,\n",
    "    alpha=alpha_mu,\n",
    "    beta=beta_mu,\n",
    "    min_samples_leaf=min_samples_leaf_mu,\n",
    "    max_depth=max_depth_mu,\n",
    "    leaf_model_type=leaf_model_mu,\n",
    "    leaf_model_scale=leaf_prior_scale_mu,\n",
    "    cutpoint_grid_size=cutpoint_grid_size_mu,\n",
    ")\n",
    "forest_sampler_mu = ForestSampler(\n",
    "    dataset_mu,\n",
    "    global_model_config, \n",
    "    forest_model_config_mu\n",
    ")\n",
    "leaf_var_model_mu = LeafVarianceModel()\n",
    "\n",
    "# Treatment forest sampling classes\n",
    "forest_container_tau = ForestContainer(\n",
    "    num_trees_tau, 1 if np.ndim(Z) == 1 else Z.shape[1], False, False\n",
    ")\n",
    "active_forest_tau = Forest(\n",
    "    num_trees_tau, 1 if np.ndim(Z) == 1 else Z.shape[1], False, False\n",
    ")\n",
    "forest_model_config_tau = ForestModelConfig(\n",
    "    num_trees=num_trees_tau,\n",
    "    num_features=p_X,\n",
    "    num_observations=n,\n",
    "    feature_types=feature_types_tau,\n",
    "    variable_weights=var_weights_tau,\n",
    "    leaf_dimension=leaf_dimension_tau,\n",
    "    alpha=alpha_tau,\n",
    "    beta=beta_tau,\n",
    "    min_samples_leaf=min_samples_leaf_tau,\n",
    "    max_depth=max_depth_tau,\n",
    "    leaf_model_type=leaf_model_tau,\n",
    "    leaf_model_scale=leaf_prior_scale_tau,\n",
    "    cutpoint_grid_size=cutpoint_grid_size_tau,\n",
    ")\n",
    "forest_sampler_tau = ForestSampler(\n",
    "    dataset_tau,\n",
    "    global_model_config, \n",
    "    forest_model_config_tau\n",
    ")\n",
    "leaf_var_model_tau = LeafVarianceModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the leaves of the prognostic and treatment forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mu = np.array([np.squeeze(np.mean(resid))])\n",
    "forest_sampler_mu.prepare_for_sampler(\n",
    "    dataset_mu,\n",
    "    residual,\n",
    "    active_forest_mu,\n",
    "    leaf_model_mu,\n",
    "    init_mu,\n",
    ")\n",
    "\n",
    "init_tau = np.array([0.0])\n",
    "forest_sampler_tau.prepare_for_sampler(\n",
    "    dataset_tau,\n",
    "    residual,\n",
    "    active_forest_tau,\n",
    "    leaf_model_tau,\n",
    "    init_tau,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to run the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmstart = 10\n",
    "num_mcmc = 100\n",
    "num_samples = num_warmstart + num_mcmc\n",
    "global_var_samples = np.empty(num_samples)\n",
    "leaf_scale_samples_mu = np.empty(num_samples)\n",
    "leaf_scale_samples_tau = np.empty(num_samples)\n",
    "leaf_prior_scale_mu = np.array([[tau_init_mu]])\n",
    "leaf_prior_scale_tau = np.array([[tau_init_tau]])\n",
    "current_b0 = -0.5\n",
    "current_b1 = 0.5\n",
    "b_0_samples = np.empty(num_samples)\n",
    "b_1_samples = np.empty(num_samples)\n",
    "tau_basis = (1 - Z) * current_b0 + Z * current_b1\n",
    "dataset_tau.update_basis(tau_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the \"grow-from-root\" (XBART) sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_warmstart):\n",
    "    # Sample the prognostic forest\n",
    "    forest_sampler_mu.sample_one_iteration(\n",
    "        forest_container_mu,\n",
    "        active_forest_mu,\n",
    "        dataset_mu,\n",
    "        residual,\n",
    "        cpp_rng,\n",
    "        global_model_config, \n",
    "        forest_model_config_mu,\n",
    "        True,\n",
    "        True,\n",
    "    )\n",
    "    # Sample global variance\n",
    "    current_sigma2 = global_var_model.sample_one_iteration(\n",
    "        residual, cpp_rng, a_global, b_global\n",
    "    )\n",
    "    global_model_config.update_global_error_variance(current_sigma2)\n",
    "    # Sample prognostic forest leaf scale\n",
    "    leaf_prior_scale_mu[0, 0] = leaf_var_model_mu.sample_one_iteration(\n",
    "        active_forest_mu, cpp_rng, a_leaf_mu, b_leaf_mu\n",
    "    )\n",
    "    leaf_scale_samples_mu[i] = leaf_prior_scale_mu[0, 0]\n",
    "    forest_model_config_mu.update_leaf_model_scale(\n",
    "        leaf_prior_scale_mu\n",
    "    )\n",
    "\n",
    "    # Sample the treatment effect forest\n",
    "    forest_sampler_tau.sample_one_iteration(\n",
    "        forest_container_tau,\n",
    "        active_forest_tau,\n",
    "        dataset_tau,\n",
    "        residual,\n",
    "        cpp_rng,\n",
    "        global_model_config, \n",
    "        forest_model_config_tau,\n",
    "        True,\n",
    "        True,\n",
    "    )\n",
    "    \n",
    "    # Sample adaptive coding parameters\n",
    "    mu_x = active_forest_mu.predict_raw(dataset_mu)\n",
    "    tau_x = np.squeeze(active_forest_tau.predict_raw(dataset_tau))\n",
    "    s_tt0 = np.sum(tau_x * tau_x * (Z == 0))\n",
    "    s_tt1 = np.sum(tau_x * tau_x * (Z == 1))\n",
    "    partial_resid_mu = resid - np.squeeze(mu_x)\n",
    "    s_ty0 = np.sum(tau_x * partial_resid_mu * (Z == 0))\n",
    "    s_ty1 = np.sum(tau_x * partial_resid_mu * (Z == 1))\n",
    "    current_b0 = rng.normal(\n",
    "        loc=(s_ty0 / (s_tt0 + 2 * current_sigma2)),\n",
    "        scale=np.sqrt(current_sigma2 / (s_tt0 + 2 * current_sigma2)),\n",
    "        size=1,\n",
    "    )[0]\n",
    "    current_b1 = rng.normal(\n",
    "        loc=(s_ty1 / (s_tt1 + 2 * current_sigma2)),\n",
    "        scale=np.sqrt(current_sigma2 / (s_tt1 + 2 * current_sigma2)),\n",
    "        size=1,\n",
    "    )[0]\n",
    "    tau_basis = (1 - Z) * current_b0 + Z * current_b1\n",
    "    dataset_tau.update_basis(tau_basis)\n",
    "    forest_sampler_tau.propagate_basis_update(dataset_tau, residual, active_forest_tau)\n",
    "    b_0_samples[i] = current_b0\n",
    "    b_1_samples[i] = current_b1\n",
    "\n",
    "    # Sample global variance\n",
    "    current_sigma2 = global_var_model.sample_one_iteration(\n",
    "        residual, cpp_rng, a_global, b_global\n",
    "    )\n",
    "    global_model_config.update_global_error_variance(current_sigma2)\n",
    "    global_var_samples[i] = current_sigma2\n",
    "    # Sample treatment forest leaf scale\n",
    "    leaf_prior_scale_tau[0, 0] = leaf_var_model_tau.sample_one_iteration(\n",
    "        active_forest_tau, cpp_rng, a_leaf_tau, b_leaf_tau\n",
    "    )\n",
    "    leaf_scale_samples_tau[i] = leaf_prior_scale_tau[0, 0]\n",
    "    forest_model_config_tau.update_leaf_model_scale(\n",
    "        leaf_prior_scale_tau\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the MCMC (BART) sampler, initialized at the last XBART sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_warmstart, num_samples):\n",
    "    # Sample the prognostic forest\n",
    "    forest_sampler_mu.sample_one_iteration(\n",
    "        forest_container_mu,\n",
    "        active_forest_mu,\n",
    "        dataset_mu,\n",
    "        residual,\n",
    "        cpp_rng,\n",
    "        global_model_config, \n",
    "        forest_model_config_mu,\n",
    "        True,\n",
    "        False,\n",
    "    )\n",
    "    # Sample global variance\n",
    "    current_sigma2 = global_var_model.sample_one_iteration(\n",
    "        residual, cpp_rng, a_global, b_global\n",
    "    )\n",
    "    global_model_config.update_global_error_variance(current_sigma2)\n",
    "    # Sample prognostic forest leaf scale\n",
    "    leaf_prior_scale_mu[0, 0] = leaf_var_model_mu.sample_one_iteration(\n",
    "        active_forest_mu, cpp_rng, a_leaf_mu, b_leaf_mu\n",
    "    )\n",
    "    leaf_scale_samples_mu[i] = leaf_prior_scale_mu[0, 0]\n",
    "    forest_model_config_mu.update_leaf_model_scale(\n",
    "        leaf_prior_scale_mu\n",
    "    )\n",
    "\n",
    "    # Sample the treatment effect forest\n",
    "    forest_sampler_tau.sample_one_iteration(\n",
    "        forest_container_tau,\n",
    "        active_forest_tau,\n",
    "        dataset_tau,\n",
    "        residual,\n",
    "        cpp_rng,\n",
    "        global_model_config, \n",
    "        forest_model_config_tau,\n",
    "        True,\n",
    "        False,\n",
    "    )\n",
    "    \n",
    "    # Sample adaptive coding parameters\n",
    "    mu_x = active_forest_mu.predict_raw(dataset_mu)\n",
    "    tau_x = np.squeeze(active_forest_tau.predict_raw(dataset_tau))\n",
    "    s_tt0 = np.sum(tau_x * tau_x * (Z == 0))\n",
    "    s_tt1 = np.sum(tau_x * tau_x * (Z == 1))\n",
    "    partial_resid_mu = resid - np.squeeze(mu_x)\n",
    "    s_ty0 = np.sum(tau_x * partial_resid_mu * (Z == 0))\n",
    "    s_ty1 = np.sum(tau_x * partial_resid_mu * (Z == 1))\n",
    "    current_b0 = rng.normal(\n",
    "        loc=(s_ty0 / (s_tt0 + 2 * current_sigma2)),\n",
    "        scale=np.sqrt(current_sigma2 / (s_tt0 + 2 * current_sigma2)),\n",
    "        size=1,\n",
    "    )[0]\n",
    "    current_b1 = rng.normal(\n",
    "        loc=(s_ty1 / (s_tt1 + 2 * current_sigma2)),\n",
    "        scale=np.sqrt(current_sigma2 / (s_tt1 + 2 * current_sigma2)),\n",
    "        size=1,\n",
    "    )[0]\n",
    "    tau_basis = (1 - Z) * current_b0 + Z * current_b1\n",
    "    dataset_tau.update_basis(tau_basis)\n",
    "    forest_sampler_tau.propagate_basis_update(dataset_tau, residual, active_forest_tau)\n",
    "    b_0_samples[i] = current_b0\n",
    "    b_1_samples[i] = current_b1\n",
    "\n",
    "    # Sample global variance\n",
    "    current_sigma2 = global_var_model.sample_one_iteration(\n",
    "        residual, cpp_rng, a_global, b_global\n",
    "    )\n",
    "    global_model_config.update_global_error_variance(current_sigma2)\n",
    "    global_var_samples[i] = current_sigma2\n",
    "    # Sample treatment forest leaf scale\n",
    "    leaf_prior_scale_tau[0, 0] = leaf_var_model_tau.sample_one_iteration(\n",
    "        active_forest_tau, cpp_rng, a_leaf_tau, b_leaf_tau\n",
    "    )\n",
    "    leaf_scale_samples_tau[i] = leaf_prior_scale_tau[0, 0]\n",
    "    forest_model_config_tau.update_leaf_model_scale(\n",
    "        leaf_prior_scale_tau\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract mean function and error variance posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest predictions\n",
    "forest_preds_mu = forest_container_mu.predict(dataset_mu) * y_std + y_bar\n",
    "forest_preds_mu_gfr = forest_preds_mu[:, :num_warmstart]\n",
    "forest_preds_mu_mcmc = forest_preds_mu[:, num_warmstart:num_samples]\n",
    "treatment_coding_samples = b_1_samples - b_0_samples\n",
    "forest_preds_tau = (\n",
    "    forest_container_tau.predict_raw(dataset_tau)\n",
    "    * y_std\n",
    "    * np.expand_dims(treatment_coding_samples, axis=(0, 2))\n",
    ")\n",
    "forest_preds_tau_gfr = forest_preds_tau[:, :num_warmstart]\n",
    "forest_preds_tau_mcmc = forest_preds_tau[:, num_warmstart:num_samples]\n",
    "\n",
    "# Global error variance\n",
    "sigma2_samples = global_var_samples * y_std * y_std\n",
    "sigma2_samples_gfr = sigma2_samples[:num_warmstart]\n",
    "sigma2_samples_mcmc = sigma2_samples[num_warmstart:num_samples]\n",
    "\n",
    "# Adaptive coding parameters\n",
    "b_1_samples_gfr = b_1_samples[:num_warmstart] * y_std\n",
    "b_0_samples_gfr = b_0_samples[:num_warmstart] * y_std\n",
    "b_1_samples_mcmc = b_1_samples[num_warmstart:] * y_std\n",
    "b_0_samples_mcmc = b_0_samples[num_warmstart:] * y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the GFR (XBART) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_preds_tau_avg_gfr = np.squeeze(forest_preds_tau_gfr).mean(axis=1, keepdims=True)\n",
    "forest_pred_tau_df_gfr = pd.DataFrame(\n",
    "    np.concatenate((np.expand_dims(tau_X, 1), forest_preds_tau_avg_gfr), axis=1),\n",
    "    columns=[\"True tau\", \"Average estimated tau\"],\n",
    ")\n",
    "sns.scatterplot(data=forest_pred_tau_df_gfr, x=\"True tau\", y=\"Average estimated tau\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_gfr = np.squeeze(forest_preds_mu_gfr).mean(axis=1, keepdims=True)\n",
    "forest_pred_df_gfr = pd.DataFrame(\n",
    "    np.concatenate((np.expand_dims(mu_X, 1), forest_pred_avg_gfr), axis=1),\n",
    "    columns=[\"True mu\", \"Average estimated mu\"],\n",
    ")\n",
    "sns.scatterplot(data=forest_pred_df_gfr, x=\"True mu\", y=\"Average estimated mu\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_df_gfr = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            np.expand_dims(np.arange(num_warmstart), axis=1),\n",
    "            np.expand_dims(sigma2_samples_gfr, axis=1),\n",
    "        ),\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Sample\", \"Sigma^2\"],\n",
    ")\n",
    "sns.scatterplot(data=sigma_df_gfr, x=\"Sample\", y=\"Sigma^2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df_gfr = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            np.expand_dims(np.arange(num_warmstart), axis=1),\n",
    "            np.expand_dims(b_0_samples_gfr, axis=1),\n",
    "            np.expand_dims(b_1_samples_gfr, axis=1),\n",
    "        ),\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Sample\", \"Beta_0\", \"Beta_1\"],\n",
    ")\n",
    "sns.scatterplot(data=b_df_gfr, x=\"Sample\", y=\"Beta_0\")\n",
    "sns.scatterplot(data=b_df_gfr, x=\"Sample\", y=\"Beta_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the MCMC (BART) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis=1, keepdims=True)\n",
    "forest_pred_df_mcmc = pd.DataFrame(\n",
    "    np.concatenate((np.expand_dims(tau_X, 1), forest_pred_avg_mcmc), axis=1),\n",
    "    columns=[\"True tau\", \"Average estimated tau\"],\n",
    ")\n",
    "sns.scatterplot(data=forest_pred_df_mcmc, x=\"True tau\", y=\"Average estimated tau\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis=1, keepdims=True)\n",
    "forest_pred_df_mcmc = pd.DataFrame(\n",
    "    np.concatenate((np.expand_dims(mu_X, 1), forest_pred_avg_mcmc), axis=1),\n",
    "    columns=[\"True mu\", \"Average estimated mu\"],\n",
    ")\n",
    "sns.scatterplot(data=forest_pred_df_mcmc, x=\"True mu\", y=\"Average estimated mu\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_df_mcmc = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            np.expand_dims(np.arange(num_samples - num_warmstart), axis=1),\n",
    "            np.expand_dims(sigma2_samples_mcmc, axis=1),\n",
    "        ),\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Sample\", \"Sigma^2\"],\n",
    ")\n",
    "sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df_mcmc = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            np.expand_dims(np.arange(num_samples - num_warmstart), axis=1),\n",
    "            np.expand_dims(b_0_samples_mcmc, axis=1),\n",
    "            np.expand_dims(b_1_samples_mcmc, axis=1),\n",
    "        ),\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"Sample\", \"Beta_0\", \"Beta_1\"],\n",
    ")\n",
    "sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_0\")\n",
    "sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Murray, Jared S. \"Log-linear Bayesian additive regression trees for multinomial logistic and count regression models.\" Journal of the American Statistical Association 116, no. 534 (2021): 756-769.\n",
    "\n",
    "Hahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. \"Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion).\" Bayesian Analysis 15, no. 3 (2020): 965-1056."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
