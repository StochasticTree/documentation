var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"StochTree","text":"<p><code>stochtree</code> (short for \"stochastic trees\") unlocks flexible decision tree modeling in R or Python. </p>"},{"location":"index.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started: Details on how to install and use <code>stochtree</code></li> <li>About: Overview of the models supported by stochtree and pointers to further reading</li> <li>R Package: Complete documentation of the R package</li> <li>Python Package: Complete documentation of the Python package</li> <li>C++ Core API and Architecture: Overview and documentation of the C++ codebase that supports stochtree</li> <li>Advanced Vignettes: In-depth tutorials on new methods implemented using stochtree</li> <li>Development: Roadmap and how to contribute</li> </ul>"},{"location":"index.html#what-does-the-software-do","title":"What does the software do?","text":"<p>Boosted decision tree models (like xgboost,  LightGBM, or  scikit-learn's HistGradientBoostingRegressor)  are great, but often require time-consuming hyperparameter tuning.  <code>stochtree</code> can help you avoid this, by running a fast Bayesian analog of gradient boosting (called BART -- Bayesian Additive Regression Trees).</p> <p><code>stochtree</code> has two primary interfaces:</p> <ol> <li>\"High-level\": robust implementations of many popular stochastic tree algorithms (BART, XBART, BCF, XBCF), with support for serialization and parallelism.</li> <li>\"Low-level\": access to the \"inner loop\" of a forest sampler, allowing custom tree algorithm development in &lt;50 lines of code.</li> </ol> <p>The \"core\" of the software is written in C++, but it provides R and Python APIs.  The R package is available on CRAN and the python package will soon be on PyPI.</p>"},{"location":"index.html#why-stochastic-trees","title":"Why \"stochastic\" trees?","text":"<p>\"Stochastic\" loosely means the same thing as \"random.\" This naturally raises the question: how is <code>stochtree</code> different from a random forest library? At a superficial level, both are decision tree ensembles that use randomness in training.</p> <p>The difference lies in how that \"randomness\" is deployed.  Random forests take random subsets of a training dataset, and then run a deterministic decision tree fitting algorithm (recursive partitioning). Stochastic tree algorithms use randomness to construct decision tree ensembles from a fixed training dataset.</p> <p>The original stochastic tree model, Bayesian Additive Regression Trees (BART), used Markov Chain Monte Carlo (MCMC) to sample forests from their posterior distribution. </p> <p>So why not call our project <code>bayesiantree</code>?</p> <p>Some algorithms implemented in <code>stochtree</code> are \"quasi-Bayesian\" in that they are inspired by a Bayesian model, but are sampled with fast algorithms that do not provide a valid Bayesian posterior distribution.</p> <p>Moreover, we think of stochastic forests as general-purpose modeling tools.  What makes them useful is their strong empirical performance -- especially on small or noisy datasets -- not their adherence to any statistical framework.</p> <p>So why not just call our project <code>decisiontree</code>?</p> <p>Put simply, the sampling approach is part of what makes BART and other <code>stochtree</code> algorithms work so well -- we know because we have tested out versions that did not do stochastic sampling of the tree fits. </p> <p>So we settled on the term \"stochastic trees\", or \"stochtree\" for short (pronounced \"stoke-tree\").</p>"},{"location":"about.html","title":"Overview of Stochastic Tree Models","text":"<p>Stochastic tree models are a powerful addition to your modeling toolkit. As with many machine learning methods, understanding these models in depth is an involved task.</p> <p>There are many excellent published papers on stochastic tree models  (to name a few, the original BART paper,  the XBART paper,  and the BCF paper).  Here, we aim to build up an abbreviated intuition for these models from their conceptually-simple building blocks.</p>"},{"location":"about.html#notation","title":"Notation","text":"<p>We're going to introduce some notation to make these concepts precise. In a traditional supervised learning setting, we hope to predict some outcome from features in a training dataset. We'll call the outcome \\(y\\) and the features \\(X\\). Our goal is to come up with a function \\(f\\) that predicts the outcome \\(y\\) as well as possible from \\(X\\) alone. </p>"},{"location":"about.html#decision-trees","title":"Decision Trees","text":"<p>Decision tree learning is a simple machine learning method that  constructs a function \\(f\\) from a series of conditional statements. Consider the tree below.</p> <pre><code>stateDiagram-v2\n    state split_one &lt;&lt;choice&gt;&gt;\n    state split_two &lt;&lt;choice&gt;&gt;\n    split_one --&gt; split_two: if x1 &lt;= 1\n    split_one --&gt; c : if x1 &gt; 1\n    split_two --&gt; a: if x2 &lt;= -2\n    split_two --&gt; b : if x2 &gt; -2</code></pre> <p>We evaluate two conditional statments (<code>X[,1] &gt; 1</code> and <code>X[,2] &gt; -2</code>), arranged in a tree-like sequence of branches,  which determine whether the model predicts <code>a</code>, <code>b</code>, or <code>c</code>. We could similarly express this tree in math notation as </p> \\[\\begin{equation*} f(X_i) = \\begin{cases} a &amp; ; \\;\\;\\; X_{i,1} \\leq 1, \\;\\; X_{i,2} \\leq -2\\\\ b &amp; ; \\;\\;\\; X_{i,1} \\leq 1, \\;\\; X_{i,2} &gt; -2\\\\ c &amp; ; \\;\\;\\; X_{i,1} &gt; 1 \\end{cases} \\end{equation*}\\] <p>We won't belabor the discussion of trees as there are many good textbooks and online articles on the topic,  but we'll close by noting that training decision trees introduces a delicate balance between  overfitting and underfitting.  Simple trees like the one above do not capture much complexity in a dataset and may potentially be underfit  while deep, complex trees are vulnerable to overfitting and tend to have high variance.</p>"},{"location":"about.html#boosted-decision-tree-ensembles","title":"Boosted Decision Tree Ensembles","text":"<p>One way to address the overfitting-underfitting tradeoff of decision trees is to build an \"ensemble\" of decision  trees, so that the function \\(f\\) is defined by a sum of \\(k\\) individual decision trees \\(g_i\\)</p> \\[\\begin{equation*} f(X_i) = g_1(X_i) + \\dots + g_k(X_i) \\end{equation*}\\] <p>There are several ways to train an ensemble of decision trees (sometimes called \"forests\"), the most popular of which are random forests and  gradient boosting. Their main difference is that random forests train  all \\(m\\) trees independently of one another, while boosting trains tree sequentially, so that tree \\(j\\) depends on the result of training trees 1 through \\(j-1\\). Libraries like xgboost and LightGBM are popular examples of boosted tree ensembles.</p> <p>Tree ensembles often outperform neural networks and other machine learning methods on tabular datasets,  but classic tree ensemble methods return a single estimated function \\(f\\), without expressing uncertainty around its estimates.</p>"},{"location":"about.html#stochastic-tree-ensembles","title":"Stochastic Tree Ensembles","text":"<p>Stochastic tree ensembles differ from their classical counterparts in their use of randomness in learning a function.  Rather than returning a single \"best\" tree ensemble, stochastic tree ensembles return a range of tree ensembles that fit the data well. Mechanically, it's useful to think of \"sampling\" -- rather than \"fitting\" -- a stochastic tree ensemble model.</p> <p>Why is this useful? Suppose we've sampled \\(m\\) forests. For each observation \\(i\\), we obtain \\(m\\) predictions: \\([f_1(X_i), \\dots, f_m(X_i)]\\).  From this \"dataset\" of predictions, we can compute summary statistics, where a mean or median would give something akin to the predictions of an xgboost or lightgbm model,  and the \\(\\alpha\\) and \\(1-\\alpha\\) quantiles give a credible interval.</p> <p>Rather than explain each of the models that <code>stochtree</code> supports in depth here, we provide a high-level overview, with pointers to the relevant literature.</p>"},{"location":"about.html#supervised-learning","title":"Supervised Learning","text":"<p>The <code>bart</code> R function and the <code>BARTModel</code> python class are the primary interface for supervised  prediction tasks in <code>stochtree</code>. The primary references for these models are  BART (Chipman, George, McCulloch 2010) and  XBART (He and Hahn 2021).</p> <p>In addition to the standard BART / XBART models, in which each tree's leaves return a constant prediction, <code>stochtree</code> also supports  arbitrary leaf regression on a user-provided basis (i.e. an expanded version of Chipman et al 2002 and Gramacy and Lee 2012).</p>"},{"location":"about.html#causal-inference","title":"Causal Inference","text":"<p>The <code>bcf</code> R function and the <code>BCFModel</code> python class are the primary interface for causal effect  estimation in <code>stochtree</code>. The primary references for these models are  BCF (Hahn, Murray, Carvalho 2021) and  XBCF (Krantsevich, He, Hahn 2022).</p>"},{"location":"about.html#additional-modeling-features","title":"Additional Modeling Features","text":"<p>Both the BART and BCF interfaces in <code>stochtree</code> support the following extensions:</p> <ul> <li>Accelerated / \"warm-start\" sampling of forests (i.e. He and Hahn 2021)</li> <li>Forest-based heteroskedasticity (i.e. Murray 2021)</li> <li>Additive random effects (i.e. Gelman et al 2008)</li> </ul>"},{"location":"getting-started.html","title":"Getting Started","text":"<p><code>stochtree</code> is composed of a C++ \"core\" and R / Python interfaces to that core. Below, we detail how to install the R / Python packages, or work directly with the C++ codebase.</p>"},{"location":"getting-started.html#r-package","title":"R Package","text":""},{"location":"getting-started.html#cran","title":"CRAN","text":"<p>The R package can be installed from CRAN via</p> <pre><code>install.packages(\"stochtree\")\n</code></pre>"},{"location":"getting-started.html#development-version-local-build","title":"Development Version (Local Build)","text":"<p>The development version of <code>stochtree</code> can be installed from Github via</p> <pre><code>remotes::install_github(\"StochasticTree/stochtree\", ref=\"r-dev\")\n</code></pre>"},{"location":"getting-started.html#python-package","title":"Python Package","text":""},{"location":"getting-started.html#pypi","title":"PyPI","text":"<p><code>stochtree</code>'s Python package can be installed from PyPI via</p> <pre><code>pip install stochtree\n</code></pre>"},{"location":"getting-started.html#development-version-local-build_1","title":"Development Version (Local Build)","text":"<p>The development version of <code>stochtree</code> can be installed from source using pip's git interface.  To proceed, you will need a working version of git and python 3.8 or greater (available from several sources, one of the most  straightforward being the anaconda suite).</p>"},{"location":"getting-started.html#quick-start","title":"Quick start","text":"<p>Without worrying about virtual environments (detailed further below), <code>stochtree</code> can be installed from the command line</p> <pre><code>pip install numpy scipy pytest pandas scikit-learn pybind11\npip install git+https://github.com/StochasticTree/stochtree.git\n</code></pre>"},{"location":"getting-started.html#virtual-environment-installation","title":"Virtual environment installation","text":"<p>Often, users prefer to manage different projects (with different package / python version requirements) in virtual environments. </p>"},{"location":"getting-started.html#conda","title":"Conda","text":"<p>Conda provides a straightforward experience in managing python dependencies, avoiding version conflicts / ABI issues / etc.</p> <p>To build stochtree using a <code>conda</code> based workflow, first create and activate a conda environment with the requisite dependencies</p> <pre><code>conda create -n stochtree-dev -c conda-forge python=3.10 numpy scipy pytest pandas pybind11 scikit-learn\nconda activate stochtree-dev\n</code></pre> <p>Then install the package from github via pip</p> <pre><code>pip install git+https://github.com/StochasticTree/stochtree.git\n</code></pre> <p>(Note: if you'd like to run <code>stochtree</code>'s notebook examples, you will also need <code>jupyterlab</code>, <code>seaborn</code>, and <code>matplotlib</code>)</p> <pre><code>conda install matplotlib seaborn\npip install jupyterlab\n</code></pre> <p>With these dependencies installed, you can clone the repo and run the <code>demo/</code> examples.</p>"},{"location":"getting-started.html#venv","title":"Venv","text":"<p>You could also use venv for environment management. First, navigate to the folder in which you usually store virtual environments  (i.e. <code>cd /path/to/envs</code>) and create and activate a virtual environment:</p> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Install all of the package (and demo notebook) dependencies</p> <pre><code>pip install numpy scipy pytest pandas scikit-learn pybind11\n</code></pre> <p>Then install stochtree via</p> <pre><code>pip install git+https://github.com/StochasticTree/stochtree.git\n</code></pre> <p>As above, if you'd like to run the notebook examples in the <code>demo/</code> subfolder, you will also need <code>jupyterlab</code>, <code>seaborn</code>, and <code>matplotlib</code> and you will have to clone the repo</p> <pre><code>pip install matplotlib seaborn jupyterlab\n</code></pre>"},{"location":"getting-started.html#c-core","title":"C++ Core","text":"<p>While the C++ core links to both R and Python for a performant, high-level interface,  the C++ code can be compiled and unit-tested and compiled into a standalone  debug program.</p>"},{"location":"getting-started.html#compilation","title":"Compilation","text":""},{"location":"getting-started.html#cloning-the-repository","title":"Cloning the Repository","text":"<p>To clone the repository, you must have git installed, which you can do following these instructions. </p> <p>Once git is available at the command line, navigate to the folder that will store this project (in bash / zsh, this is done by running <code>cd</code> followed by the path to the directory).  Then, clone the <code>stochtree</code> repo as a subfolder by running <pre><code>git clone --recursive https://github.com/StochasticTree/stochtree.git\n</code></pre></p> <p>NOTE: this project incorporates several dependencies as git submodules,  which is why the <code>--recursive</code> flag is necessary (some systems may perform a recursive clone without this flag, but  <code>--recursive</code> ensures this behavior on all platforms). If you have already cloned the repo without the <code>--recursive</code> flag,  you can retrieve the submodules recursively by running <code>git submodule update --init --recursive</code> in the main repo directory.</p>"},{"location":"getting-started.html#cmake-build","title":"CMake Build","text":"<p>The C++ project can be built independently from the R / Python packages using <code>cmake</code>.  See here for details on installing cmake (alternatively,  on MacOS, <code>cmake</code> can be installed using homebrew). Once <code>cmake</code> is installed, you can build the CLI by navigating to the main  project directory at your command line (i.e. <code>cd /path/to/stochtree</code>) and  running the following code </p> <pre><code>rm -rf build\nmkdir build\ncmake -S . -B build\ncmake --build build\n</code></pre> <p>The CMake build has two primary targets, which are detailed below</p>"},{"location":"getting-started.html#debug-program","title":"Debug Program","text":"<p><code>debug/api_debug.cpp</code> defines a standalone target that can be straightforwardly run with a debugger (i.e. <code>lldb</code>, <code>gdb</code>)  while making non-trivial changes to the C++ code. This debugging program is compiled as part of the CMake build if the <code>BUILD_DEBUG_TARGETS</code> option in <code>CMakeLists.txt</code> is set to <code>ON</code>.</p> <p>Once the program has been built, it can be run from the command line via <code>./build/debugstochtree</code> or attached to a debugger  via <code>lldb ./build/debugstochtree</code> (clang) or <code>gdb ./build/debugstochtree</code> (gcc).</p>"},{"location":"getting-started.html#unit-tests","title":"Unit Tests","text":"<p>We test <code>stochtree</code> using the GoogleTest framework. Unit tests are compiled into a single target as part of the CMake build if the <code>BUILD_TEST</code> option is set to <code>ON</code>  and the test suite can be run after compilation via <code>./build/teststochtree</code></p>"},{"location":"getting-started.html#xcode","title":"Xcode","text":"<p>While using <code>gdb</code> or <code>lldb</code> on <code>debugstochtree</code> at the command line is very helpful, users may prefer debugging in a full-fledged IDE like xcode. This project's C++ core can be converted to an xcode project from <code>CMakeLists.txt</code>, but first you must turn off sanitizers (xcode seems to have its own way of setting this at build time for different configurations, and having injected  <code>-fsanitize=address</code> statically into compiler arguments will cause xcode errors). To do this, modify the <code>USE_SANITIZER</code> line in <code>CMakeLists.txt</code>:</p> <pre><code>option(USE_SANITIZER \"Use santizer flags\" OFF)\n</code></pre> <p>or via command-line argument to <code>cmake -G Xcode</code> as shown below. To generate an XCode project based on the build targets and specifications defined in a <code>CMakeLists.txt</code> file (and ensure that debug and santizer flags are switched off), navigate to the main project folder (i.e. <code>cd /path/to/project</code>) and run the following commands:</p> <pre><code>rm -rf xcode/\nmkdir xcode\ncd xcode\ncmake -G Xcode .. -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=c++ -DUSE_SANITIZER=OFF -DUSE_DEBUG=OFF\ncd ..\n</code></pre> <p>Now, if you navigate to the xcode subfolder (in Finder), you should be able to click on a <code>.xcodeproj</code> file and the project will open in XCode.</p>"},{"location":"R_docs/index.html","title":"StochTree R API Reference","text":"<p>Overview of the <code>stochtree</code> R library's key classes and functions, built as a self-contained doc site using the pkgdown format. The <code>stochtree</code> interface is divided into two \"levels\":</p> <ol> <li>\"High level\": end-to-end implementations of stochastic tree ensembles for supervised learning (BART / XBART) and causal inference (BCF / XBCF). <ol> <li>The BART (supervised learning) interface is documented here.</li> <li>The BCF (causal inference) interface is documented here.</li> </ol> </li> <li>\"Low level\": we provide access to most of the C++ sampling objects and functionality via R, which allow for custom sampling algorithms and integration of other model terms. This interface consists broadly of the following components:<ol> <li>Data API: loading and storing in-memory data needed to train <code>stochtree</code> models.</li> <li>Forest API: creating, storing, modifying, and sampling ensembles of decision trees that underlie all <code>stochtree</code> models.</li> <li>Serialization API: serializing models to JSON (files or in-memory strings).</li> <li>Random Effects API: sampling from additive random effects models.</li> </ol> </li> </ol>"},{"location":"cpp_docs/index.html","title":"StochTree C++ API","text":"<p>This page documents the data structures and interfaces that constitute the <code>stochtree</code> C++ core.  It may be useful to researchers building novel tree algorithms or users seeking a deeper understanding of the algorithms implemented in <code>stochtree</code>. This resource is split into:</p> <ol> <li>Technical documentation of the design / computational aspects of the C++ core (in progress!)<ol> <li>Tracker API: temporary data structures that synchronize a training dataset and the current state of a decision tree ensemble for faster sampling </li> </ol> </li> <li>Doxygen site with auto-generated documentation of C++ classes and functions</li> </ol>"},{"location":"cpp_docs/tracking.html","title":"Forest Sampling Tracker","text":"<p>A truly minimalist tree ensemble library only needs </p> <ul> <li>A representation of a decision tree</li> <li>A container for grouping / storing ensembles of trees</li> <li>In-memory access to / representation of training data</li> <li>Routines / functions to construct the trees</li> </ul> <p>Most algorithms for optimizing or sampling tree ensembles frequently perform the following operations</p> <ul> <li>Determine which leaf a training observation falls into for a decision tree (to compute its prediction and update the residual / outcome)</li> <li>Evaluate potential split candidates for a leaf of a decision</li> </ul> <p>With only the \"minimalist\" tools above, these two tasks proceed largely as follows</p> <ul> <li>For every observation in the dataset, traverse the tree (runtime depends on the tree topology but in a fully balanced tree with \\(k\\) leaf nodes, this has time complexity \\(O(N \\log (k))\\)).</li> <li>For a given node, determine which observations in the training set fall into this node. This requires \\(O(N)\\) boolean operations.</li> </ul> <p>These operations both perform unnecessary computation which can be avoided with some additional real-time tracking. Essentially, we want </p> <ol> <li>A mapping from dataset row index to leaf node id for every tree in an ensemble (so that we can skip the tree traversal during prediction)</li> <li>A mapping from leaf node id to dataset row indices every tree in an ensemble (so that we can skip the full pass through the training data at split evaluation)</li> </ol>"},{"location":"development/index.html","title":"Development","text":"<p><code>stochtree</code> is in active development. Here, we detail some aspects of the development process</p> <ul> <li>Contributing: how to get involved with stochtree, by contributing code, documentation, or helpful feedback</li> <li>Roadmap: timelines for new feature development and releases</li> </ul>"},{"location":"development/contributing.html","title":"Contributing","text":"<p><code>stochtree</code> is hosted on Github.  Any feedback, requests, or bug reports can be submitted as issues.  Moreover, if you have ideas for how to improve stochtree, we welcome pull requests.</p>"},{"location":"development/contributing.html#building-stochtree","title":"Building StochTree","text":"<p>Any local stochtree development will require cloning the repository from Github.  If you don't have git installed, you can do so following these instructions. </p> <p>Once git is available at the command line, navigate to the folder that will store this project (in bash / zsh, this is done by running <code>cd</code> followed by the path to the directory).  Then, clone the <code>stochtree</code> repo as a subfolder by running <pre><code>git clone --recursive https://github.com/StochasticTree/stochtree.git\n</code></pre></p> <p>NOTE: this project incorporates several C++ dependencies as git submodules,  which is why the <code>--recursive</code> flag is necessary. If you have already cloned the repo without the <code>--recursive</code> flag,  you can retrieve the submodules recursively by running <code>git submodule update --init --recursive</code> in the main repo directory.</p>"},{"location":"development/contributing.html#r","title":"R","text":"<p>This section will detail how to use RStudio to build and make changes to stochtree. There are other tools that are useful for R  package development (for example, Positron, VS Code,  and ESS), but we will focus on RStudio in this walkthrough.</p> <p>Once you've cloned the stochtree repository, follow these steps to build stochtree:</p> <ol> <li>Create an RStudio project in the stochtree directory</li> <li>Build the package in RStudio</li> </ol> <p>Note that due to the complicated folder structure of the stochtree repo, step 2 might not work out of the box on all platforms.  If stochtree fails to build, you can use the script that we use to create a CRAN-friendly stochtree R package directory, which  creates a <code>stochtree_cran</code> subdirectory of the stochtree folder and copies the relevant R package files into this subfolder. You can run this script by entering <code>Rscript cran-bootstrap.R 1 1 1</code> in the terminal in RStudio.  Once you have a <code>stochtree_cran</code> subfolder, you can build stochtree using</p> <pre><code>devtools::install_local(\"stochtree_cran\")\n</code></pre> <p>Since this is a temporary folder, you can clean it up by running <code>Rscript cran-cleanup.R</code> in the terminal in RStudio.</p>"},{"location":"development/contributing.html#python","title":"Python","text":"<p>Building and making changes to the python library is best done in an isolated virtual environment. There are many different ways of  managing virtual environments in Python, but here we focus on <code>conda</code> and <code>venv</code>.</p>"},{"location":"development/contributing.html#conda","title":"Conda","text":"<p>Conda provides a straightforward experience in managing python dependencies, avoiding version conflicts / ABI issues / etc.</p> <p>To build stochtree using a <code>conda</code> based workflow, first create and activate a conda environment with the requisite dependencies</p> <pre><code>conda create -n stochtree-dev -c conda-forge python=3.10 numpy scipy pytest pandas pybind11 scikit-learn matplotlib seaborn\nconda activate stochtree-dev\npip install jupyterlab\n</code></pre> <p>Then install the package by navigating to the stochtree directory and running</p> <pre><code>pip install .\n</code></pre> <p>Note that if you are making changes and finding that they aren't reflected after a reinstall of stochtree, you can  clear all of the python package build artifacts with </p> <pre><code>rm -rf stochtree.egg-info; rm -rf .pytest_cache; rm -rf build\n</code></pre> <p>and then rerun <code>pip install .</code></p>"},{"location":"development/contributing.html#venv","title":"Venv","text":"<p>You could also use venv for environment management. First, navigate to the folder in which you usually store virtual environments  (i.e. <code>cd /path/to/envs</code>) and create and activate a virtual environment:</p> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Install all of the package (and demo notebook) dependencies</p> <pre><code>pip install numpy scipy pytest pandas scikit-learn pybind11 matplotlib seaborn jupyterlab\n</code></pre> <p>Then install the package by navigating to the stochtree directory and running</p> <pre><code>pip install .\n</code></pre> <p>Note that if you are making changes and finding that they aren't reflected after a reinstall of stochtree, you can  clear all of the python package development artifacts with </p> <pre><code>rm -rf stochtree.egg-info; rm -rf .pytest_cache; rm -rf build\n</code></pre> <p>and then rerun <code>pip install .</code></p>"},{"location":"development/contributing.html#c","title":"C++","text":""},{"location":"development/contributing.html#cmake","title":"CMake","text":"<p>The C++ project can be built independently from the R / Python packages using <code>cmake</code>.  See here for details on installing cmake (alternatively,  on MacOS, <code>cmake</code> can be installed using homebrew). Once <code>cmake</code> is installed, you can build the CLI by navigating to the main  project directory at your command line (i.e. <code>cd /path/to/stochtree</code>) and  running the following code </p> <pre><code>rm -rf build\nmkdir build\ncmake -S . -B build\ncmake --build build\n</code></pre> <p>The CMake build has two primary targets, which are detailed below</p>"},{"location":"development/contributing.html#debug-program","title":"Debug Program","text":"<p><code>debug/api_debug.cpp</code> defines a standalone target that can be straightforwardly run with a debugger (i.e. <code>lldb</code>, <code>gdb</code>)  while making non-trivial changes to the C++ code. This debugging program is compiled as part of the CMake build if the <code>BUILD_DEBUG_TARGETS</code> option in <code>CMakeLists.txt</code> is set to <code>ON</code>.</p> <p>Once the program has been built, it can be run from the command line via <code>./build/debugstochtree</code> or attached to a debugger  via <code>lldb ./build/debugstochtree</code> (clang) or <code>gdb ./build/debugstochtree</code> (gcc).</p>"},{"location":"development/contributing.html#unit-tests","title":"Unit Tests","text":"<p>We test <code>stochtree</code> using the GoogleTest framework. Unit tests are compiled into a single target as part of the CMake build if the <code>BUILD_TEST</code> option is set to <code>ON</code>  and the test suite can be run after compilation via <code>./build/teststochtree</code></p>"},{"location":"development/contributing.html#debugging","title":"Debugging","text":"<p>Debugging stochtree invariably leads to the \"core\" C++ codebase, which requires care to debug correctly. Below we detail how to debug stochtree's C++ core through each of the three interfaces (C++, R and Python).</p>"},{"location":"development/contributing.html#c-program","title":"C++ Program","text":"<p>The <code>debugstochtree</code> cmake target exists precisely to quickly debug the C++ core of stochtree.</p> <p>First, you must build the program using debug symbols, which you can do by enabling the <code>USE_DEBUG</code> optoon  and ensuring that <code>BUILD_DEBUG_TARGETS</code> is also switched on, as below</p> <pre><code>rm -rf build\nmkdir build\ncmake -S . -B build -DBUILD_DEBUG_TARGETS=ON -DUSE_DEBUG=ON\ncmake --build build\n</code></pre> <p>From here, you can debug at the command line using lldb on MacOS on gdb on linux by running either <code>lldb ./build/debugstochtree</code> or <code>gdb ./build/debugstochtree</code> and using the appropriate lldb / gdb shortcuts to debug your program.</p>"},{"location":"development/contributing.html#xcode","title":"xcode","text":"<p>While using <code>gdb</code> or <code>lldb</code> on <code>debugstochtree</code> at the command line is very helpful, users may prefer debugging in a full-fledged IDE like xcode (if working in MacOS).  This project's C++ core can be converted to an xcode project from <code>CMakeLists.txt</code>, but first you must turn off sanitizers  (xcode seems to have its own way of setting this at build time for different configurations, and having injected  <code>-fsanitize=address</code> statically into compiler arguments will cause xcode errors). To do this, modify the <code>USE_SANITIZER</code> line in <code>CMakeLists.txt</code>:</p> <pre><code>option(USE_SANITIZER \"Use santizer flags\" OFF)\n</code></pre> <p>To generate an XCode project based on the build targets and specifications defined in a <code>CMakeLists.txt</code>, navigate to the main project folder (i.e. <code>cd /path/to/project</code>) and run the following commands:</p> <pre><code>rm -rf xcode/\nmkdir xcode\ncd xcode\ncmake -G Xcode .. -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=c++ -DUSE_SANITIZER=OFF -DUSE_DEBUG=OFF\ncd ..\n</code></pre> <p>Now, if you navigate to the xcode subfolder (in Finder), you should be able to click on a <code>.xcodeproj</code> file and the project will open in XCode.</p>"},{"location":"development/contributing.html#r-package","title":"R Package","text":"<p>Debugging stochtree R code requires building the R package with debug symbols. The simplest way to do this is to open your R installation's <code>Makevars</code> file  by running <code>usethis::edit_r_makevars()</code> in RStudio which will open <code>Makevars</code>  in a code editor.</p> <p>If your <code>Makevars</code> file already has a line that begins with <code>CXX17FLAGS = ...</code>,  look for a <code>-g -O2</code> compiler flag and change this to <code>-g -O0</code>. If this flag isn't  set in the <code>CXX17FLAGS =</code> line, then simply add <code>-g -O0</code> to this line after the <code>=</code>. If your <code>Makevars</code> file already does not have a line that begins with <code>CXX17FLAGS = ...</code>,  add <code>CXX17FLAGS = -g -O0</code>.</p> <p>Now, rebuild the R package as above. Save the R code you'd like to debug to an R script.  Suppose for the sake of illustration that the code you want to debug is saved in  <code>path/to/debug_script.R</code>.</p> <p>At the command line (either the terminal in RStudio or your local terminal program),  run <code>R -d lldb</code> if you are using MacOS (or <code>R -d gdb</code> if you are using Linux).</p> <p>Now, you'll see an lldb prompt which should look like below with a blinking cursor after it</p> <pre><code>(lldb) \n</code></pre> <p>From there, you can set breakpoints, either to specific lines of specific files like <code>b src/tree.cpp:2117</code> or to break whenever there is an error using <code>breakpoint set -E c++</code>. (Note: in gdb, the breakpoint and control flow commands are slightly different, see here for more detail on debugging R through <code>gdb</code>.) Now, you can run R through the debugger by typing</p> <pre><code>r\n</code></pre> <p>This should load an R console, from which you can execute a script you've set up to run your code using </p> <pre><code>source(\"path/to/debug_script.R\")\n</code></pre> <p>The code will either stop when it hits your first line-based breakpoint or when it runs into an error if you set the error-based breakpoint. From there, you can navigate using <code>lldb</code> (or <code>gdb</code>) commands.</p> <p>Note: once you've loaded the R console, you can also simply interactively run commands that call stochtree's C++ code (i.e. running the <code>bart()</code> or <code>bcf()</code> functions). If you're debugging at this level, you probably have a specific problem in mind, and using a repeatable script will be worth your while, but it is not strictly necessary.</p>"},{"location":"development/contributing.html#python-package","title":"Python Package","text":"<p>First, you need to build stochtree's C++ extension with debug symbols.  As always, start by navigating to the stochtree directory (i.e. <code>cd /path/to/stochtree/</code>)  and activating your development virtual environment (i.e. <code>conda activate [env_name]</code> or <code>source venv/bin/activate</code>).</p> <p>Since stochtree builds its C++ extension via cmake following this example,  you'll need to ensure that the <code>self.debug</code> field in the <code>CMakeBuild</code> class is set to <code>True</code>.  You can do this by setting an environment variable of <code>DEBUG</code> equal to 1. In bash, you can do this with <code>export DEBUG=1</code> at the command line. </p> <p>Once this is done, build the python library using </p> <pre><code>pip install .\n</code></pre> <p>Suppose you'd like to debug stochtree through a script called <code>/path/to/script.py</code>.</p> <p>First, target a python process with <code>lldb</code> (or, alternatively, replace with <code>gdb</code> below if you use <code>gcc</code> as your compiler) via</p> <pre><code>lldb python\n</code></pre> <p>Now, you'll see an lldb (or gdb) prompt which should look like below with a blinking cursor after it</p> <pre><code>(lldb) \n</code></pre> <p>From there, you can set breakpoints, either to specific lines of specific files like <code>b src/tree.cpp:2117</code> or to break whenever there is an error using <code>breakpoint set -E c++</code>. (If you're using <code>gdb</code>, see here for a comparison between lldb commands and gdb commands for setting breakpoints and navigating your program.) Now you can run your python script through the debugger by typing</p> <pre><code>r /path/to/script.py\n</code></pre> <p>The program will run until the first breakpoint is hit, and at this point you can navigate using lldb (or gdb) commands.</p> <p>Note: rather than running a script like <code>/path/to/script.py</code> above, you can also simply load the python console by typing <code>r</code> at the <code>(lldb)</code> terminal and then interactively run commands that call stochtree's C++ code (i.e. sampling <code>BARTModel</code> or <code>BCFModel</code> objects). If you're debugging at this level, you probably have a specific problem in mind, and using a repeatable script will be worth your while, but it is not strictly necessary.</p>"},{"location":"development/roadmap.html","title":"Development Roadmap","text":"<p>We are working hard to make <code>stochtree</code> faster, easier to use, and more flexible! Below is a snapshot of our development roadmap. We categorize new product enhancements into four categories:</p> <ol> <li>User Interface: the way that a user can build, store, and use models</li> <li>Performance: program runtime and memory usage of various models</li> <li>Modeling Features: scope of modeling tools provided</li> <li>Interoperability: compatibility with other computing and data libraries</li> </ol> <p>Our development goals are prioritized along three broad timelines</p> <ol> <li>Now: development is currently underway or planned for a near-term release</li> <li>Next: design / research needed; development hinges on feasibility and time demands</li> <li>Later: long-term goal; exploratory</li> </ol> Category Now Next Later User Interface Performance Hardware acceleration (Apple Silicon GPU)Hardware acceleration (NVIDIA GPU)Out-of-memory sampler Modeling Features Quantile cutpoint samplingProbit BART and BCF Monotonicity constraintsMulticlass classification Interoperability PyMC (Python)Stan (R / Python)Apache Arrow (R / Python)Polars (Python)"},{"location":"python_docs/index.html","title":"StochTree Python Library","text":"<p>Our documentation of the <code>stochtree</code> python library has two components:</p> <ol> <li>API Documentation: in-depth documentation of the classes and functions and govern the <code>stochtree</code> python API</li> <li>Demos: notebook-style vignettes that showcase the functionality, output, and use cases of <code>stochtree</code> in python</li> </ol>"},{"location":"python_docs/api/index.html","title":"StochTree Python API Reference","text":"<p>The <code>stochtree</code> interface is divided into two \"levels\":</p> <ol> <li>\"High level\": end-to-end implementations of stochastic tree ensembles for supervised learning (BART / XBART) and causal inference (BCF / XBCF). Both interfaces are designed to mirror the scikit-learn estimator style, with the <code>.fit()</code> method replaced by a <code>.sample()</code> method.<ol> <li>The BART (supervised learning) interface is documented here.</li> <li>The BCF (causal inference) interface is documented here.</li> </ol> </li> <li>\"Low level\": we provide access to most of the C++ sampling objects and functionality via Python, which allow for custom sampling algorithms and integration of other model terms. This interface is documented here and consists broadly of the following components:<ol> <li>Data API: loading and storing in-memory data needed to train <code>stochtree</code> models.</li> <li>Forest API: creating, storing, and modifying ensembles of decision trees that underlie all <code>stochtree</code> models.</li> <li>Sampler API: sampling from stochastic tree ensemble models as well as several supported parametric models.</li> <li>Utilities API: seeding a C++ random number generator, preprocessing data, and serializing models to JSON (files or in-memory strings).</li> </ol> </li> </ol>"},{"location":"python_docs/api/bart.html","title":"BART","text":""},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel","title":"<code>stochtree.bart.BARTModel</code>","text":"<p>Class that handles sampling, storage, and serialization of stochastic forest models for supervised learning.  The class takes its name from Bayesian Additive Regression Trees, an MCMC sampler originally developed in  Chipman, George, McCulloch (2010), but supports several sampling algorithms:</p> <ul> <li>MCMC: The \"classic\" sampler defined in Chipman, George, McCulloch (2010). In order to run the MCMC sampler, set <code>num_gfr = 0</code> (explained below) and then define a sampler according to several parameters:<ul> <li><code>num_burnin</code>: the number of iterations to run before \"retaining\" samples for further analysis. These \"burned in\" samples are helpful for allowing a sampler to converge before retaining samples.</li> <li><code>num_chains</code>: the number of independent sequences of MCMC samples to generate (typically referred to in the literature as \"chains\")</li> <li><code>num_mcmc</code>: the number of \"retained\" samples of the posterior distribution</li> <li><code>keep_every</code>: after a sampler has \"burned in\", we will run the sampler for <code>keep_every</code> * <code>num_mcmc</code> iterations, retaining one of each <code>keep_every</code> iteration in a chain.</li> </ul> </li> <li>GFR (Grow-From-Root): A fast, greedy approximation of the BART MCMC sampling algorithm introduced in He and Hahn (2021). GFR sampler iterations are governed by the <code>num_gfr</code> parameter, and there are two primary ways to use this sampler:<ul> <li>Standalone: setting <code>num_gfr &gt; 0</code> and both <code>num_burnin = 0</code> and <code>num_mcmc = 0</code> will only run and retain GFR samples of the posterior. This is typically referred to as \"XBART\" (accelerated BART).</li> <li>Initializer for MCMC: setting <code>num_gfr &gt; 0</code> and <code>num_mcmc &gt; 0</code> will use ensembles from the GFR algorithm to initialize <code>num_chains</code> independent MCMC BART samplers, which are run for <code>num_mcmc</code> iterations. This is typically referred to as \"warm start BART\".</li> </ul> </li> </ul> <p>In addition to enabling multiple samplers, we support a broad set of models. First, note that the original BART model of Chipman, George, McCulloch (2010) is</p> \\[\\begin{equation*} \\begin{aligned} y &amp;= f(X) + \\epsilon\\\\ f(X) &amp;\\sim \\text{BART}(\\cdot)\\\\ \\epsilon &amp;\\sim N(0, \\sigma^2)\\\\ \\sigma^2 &amp;\\sim IG(\\nu, \\nu\\lambda) \\end{aligned} \\end{equation*}\\] <p>In words, there is a nonparametric mean function governed by a tree ensemble with a BART prior and an additive (mean-zero) Gaussian error  term, whose variance is parameterized with an inverse gamma prior.</p> <p>The <code>BARTModel</code> class supports the following extensions of this model:</p> <ul> <li>Leaf Regression: Rather than letting <code>f(X)</code> define a standard decision tree ensemble, in which each tree uses <code>X</code> to partition the data and then serve up constant predictions, we allow for models <code>f(X,Z)</code> in which <code>X</code> and <code>Z</code> together define a partitioned linear model (<code>X</code> partitions the data and <code>Z</code> serves as the basis for regression models). This model can be run by specifying <code>leaf_basis_train</code> in the <code>sample</code> method.</li> <li>Heteroskedasticity: Rather than define \\(\\epsilon\\) parameterically, we can let a forest \\(\\sigma^2(X)\\) model a conditional error variance function. This can be done by setting <code>num_trees_variance &gt; 0</code> in the <code>params</code> dictionary passed to the <code>sample</code> method.</li> </ul>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.sample","title":"<code>sample(X_train, y_train, leaf_basis_train=None, rfx_group_ids_train=None, rfx_basis_train=None, X_test=None, leaf_basis_test=None, rfx_group_ids_test=None, rfx_basis_test=None, num_gfr=5, num_burnin=0, num_mcmc=100, general_params=None, mean_forest_params=None, variance_forest_params=None, previous_model_json=None, previous_model_warmstart_sample_num=None)</code>","text":"<p>Runs a BART sampler on provided training set. Predictions will be cached for the training set and (if provided) the test set. Does not require a leaf regression basis.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>array</code> <p>Training set covariates on which trees may be partitioned.</p> required <code>y_train</code> <code>array</code> <p>Training set outcome.</p> required <code>leaf_basis_train</code> <code>array</code> <p>Optional training set basis vector used to define a regression to be run in the leaves of each tree.</p> <code>None</code> <code>rfx_group_ids_train</code> <code>array</code> <p>Optional group labels used for an additive random effects model.</p> <code>None</code> <code>rfx_basis_train</code> <code>array</code> <p>Optional basis for \"random-slope\" regression in an additive random effects model.</p> <code>None</code> <code>X_test</code> <code>array</code> <p>Optional test set covariates.</p> <code>None</code> <code>leaf_basis_test</code> <code>array</code> <p>Optional test set basis vector used to define a regression to be run in the leaves of each tree. Must be included / omitted consistently (i.e. if leaf_basis_train is provided, then leaf_basis_test must be provided alongside X_test).</p> <code>None</code> <code>rfx_group_ids_test</code> <code>array</code> <p>Optional test set group labels used for an additive random effects model. We do not currently support (but plan to in the near future), test set evaluation for group labels that were not in the training set.</p> <code>None</code> <code>rfx_basis_test</code> <code>array</code> <p>Optional test set basis for \"random-slope\" regression in additive random effects model.</p> <code>None</code> <code>num_gfr</code> <code>int</code> <p>Number of \"warm-start\" iterations run using the grow-from-root algorithm (He and Hahn, 2021). Defaults to <code>5</code>.</p> <code>5</code> <code>num_burnin</code> <code>int</code> <p>Number of \"burn-in\" iterations of the MCMC sampler. Defaults to <code>0</code>. Ignored if <code>num_gfr &gt; 0</code>.</p> <code>0</code> <code>num_mcmc</code> <code>int</code> <p>Number of \"retained\" iterations of the MCMC sampler. Defaults to <code>100</code>. If this is set to 0, GFR (XBART) samples will be retained.</p> <code>100</code> <code>general_params</code> <code>dict</code> <p>Dictionary of general model parameters, each of which has a default value processed internally, so this argument is optional.</p> <ul> <li><code>cutpoint_grid_size</code> (<code>int</code>): Maximum number of cutpoints to consider for each feature. Defaults to <code>100</code>.</li> <li><code>standardize</code> (<code>bool</code>): Whether or not to standardize the outcome (and store the offset / scale in the model object). Defaults to <code>True</code>.</li> <li><code>sample_sigma2_global</code> (<code>bool</code>): Whether or not to update the <code>sigma^2</code> global error variance parameter based on <code>IG(sigma2_global_shape, sigma2_global_scale)</code>. Defaults to <code>True</code>.</li> <li><code>sigma2_init</code> (<code>float</code>): Starting value of global variance parameter. Set internally to the outcome variance (standardized if <code>standardize = True</code>) if not set here.</li> <li><code>sigma2_global_shape</code> (<code>float</code>): Shape parameter in the <code>IG(sigma2_global_shape, b_glsigma2_global_scaleobal)</code> global error variance model. Defaults to <code>0</code>.</li> <li><code>sigma2_global_scale</code> (<code>float</code>): Scale parameter in the <code>IG(sigma2_global_shape, b_glsigma2_global_scaleobal)</code> global error variance model. Defaults to <code>0</code>.</li> <li><code>variable_weights</code> (<code>np.array</code>): Numeric weights reflecting the relative probability of splitting on each variable. Does not need to sum to 1 but cannot be negative. Defaults to uniform over the columns of <code>X_train</code> if not provided.</li> <li><code>random_seed</code> (<code>int</code>): Integer parameterizing the C++ random number generator. If not specified, the C++ random number generator is seeded according to <code>std::random_device</code>.</li> <li><code>keep_burnin</code> (<code>bool</code>): Whether or not \"burnin\" samples should be included in predictions. Defaults to <code>False</code>. Ignored if <code>num_mcmc == 0</code>.</li> <li><code>keep_gfr</code> (<code>bool</code>): Whether or not \"warm-start\" / grow-from-root samples should be included in predictions. Defaults to <code>False</code>. Ignored if <code>num_mcmc == 0</code>.</li> <li><code>keep_every</code> (<code>int</code>): How many iterations of the burned-in MCMC sampler should be run before forests and parameters are retained. Defaults to <code>1</code>. Setting <code>keep_every = k</code> for some <code>k &gt; 1</code> will \"thin\" the MCMC samples by retaining every <code>k</code>-th sample, rather than simply every sample. This can reduce the autocorrelation of the MCMC samples.</li> <li><code>num_chains</code> (<code>int</code>): How many independent MCMC chains should be sampled. If <code>num_mcmc = 0</code>, this is ignored. If <code>num_gfr = 0</code>, then each chain is run from root for <code>num_mcmc * keep_every + num_burnin</code> iterations, with <code>num_mcmc</code> samples retained. If <code>num_gfr &gt; 0</code>, each MCMC chain will be initialized from a separate GFR ensemble, with the requirement that <code>num_gfr &gt;= num_chains</code>. Defaults to <code>1</code>.</li> </ul> <code>None</code> <code>mean_forest_params</code> <code>dict</code> <p>Dictionary of mean forest model parameters, each of which has a default value processed internally, so this argument is optional.</p> <ul> <li><code>num_trees</code> (<code>int</code>): Number of trees in the conditional mean model. Defaults to <code>200</code>. If <code>num_trees = 0</code>, the conditional mean will not be modeled using a forest and sampling will only proceed if <code>num_trees &gt; 0</code> for the variance forest.</li> <li><code>alpha</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 in the conditional mean model. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>0.95</code>.</li> <li><code>beta</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 in the conditional mean model. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>2</code>.</li> <li><code>min_samples_leaf</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples, in the conditional mean model. Defaults to <code>5</code>.</li> <li><code>max_depth</code> (<code>int</code>): Maximum depth of any tree in the ensemble in the conditional mean model. Defaults to <code>10</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>sample_sigma2_leaf</code> (<code>bool</code>): Whether or not to update the <code>tau</code> leaf scale variance parameter based on <code>IG(sigma2_leaf_shape, sigma2_leaf_scale)</code>. Cannot (currently) be set to true if <code>leaf_basis_train</code> has more than one column. Defaults to <code>False</code>.</li> <li><code>sigma2_leaf_init</code> (<code>float</code>): Starting value of leaf node scale parameter. Calibrated internally as <code>1/num_trees</code> if not set here.</li> <li><code>sigma2_leaf_shape</code> (<code>float</code>): Shape parameter in the <code>IG(sigma2_leaf_shape, sigma2_leaf_scale)</code> leaf node parameter variance model. Defaults to <code>3</code>.</li> <li><code>sigma2_leaf_scale</code> (<code>float</code>): Scale parameter in the <code>IG(sigma2_leaf_shape, sigma2_leaf_scale)</code> leaf node parameter variance model. Calibrated internally as <code>0.5/num_trees</code> if not set here.</li> <li><code>keep_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be included in the mean forest. Defaults to <code>None</code>.</li> <li><code>drop_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be excluded from the mean forest. Defaults to <code>None</code>. If both <code>drop_vars</code> and <code>keep_vars</code> are set, <code>drop_vars</code> will be ignored.</li> </ul> <code>None</code> <code>variance_forest_params</code> <code>dict</code> <p>Dictionary of variance forest model  parameters, each of which has a default value processed internally, so this argument is optional.</p> <ul> <li><code>num_trees</code> (<code>int</code>): Number of trees in the conditional variance model. Defaults to <code>0</code>. Variance is only modeled using a tree / forest if <code>num_trees &gt; 0</code>.</li> <li><code>alpha</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 in the conditional variance model. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>0.95</code>.</li> <li><code>beta</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 in the conditional variance model. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>2</code>.</li> <li><code>min_samples_leaf</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples, in the conditional variance model. Defaults to <code>5</code>.</li> <li><code>max_depth</code> (<code>int</code>): Maximum depth of any tree in the ensemble in the conditional variance model. Defaults to <code>10</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>leaf_prior_calibration_param</code> (<code>float</code>): Hyperparameter used to calibrate the [optional] <code>IG(var_forest_prior_shape, var_forest_prior_scale)</code> conditional error variance model. If <code>var_forest_prior_shape</code> and <code>var_forest_prior_scale</code> are not set below, this calibration parameter is used to set these values to <code>num_trees / leaf_prior_calibration_param^2 + 0.5</code> and <code>num_trees / leaf_prior_calibration_param^2</code>, respectively. Defaults to <code>1.5</code>.</li> <li><code>var_forest_leaf_init</code> (<code>float</code>): Starting value of root forest prediction in conditional (heteroskedastic) error variance model. Calibrated internally as <code>np.log(0.6*np.var(y_train))/num_trees_variance</code>, where <code>y_train</code> is the possibly standardized outcome, if not set.</li> <li><code>var_forest_prior_shape</code> (<code>float</code>): Shape parameter in the [optional] <code>IG(var_forest_prior_shape, var_forest_prior_scale)</code> conditional error variance forest (which is only sampled if <code>num_trees &gt; 0</code>). Calibrated internally as <code>num_trees / leaf_prior_calibration_param^2 + 0.5</code> if not set here.</li> <li><code>var_forest_prior_scale</code> (<code>float</code>): Scale parameter in the [optional] <code>IG(var_forest_prior_shape, var_forest_prior_scale)</code> conditional error variance forest (which is only sampled if <code>num_trees &gt; 0</code>). Calibrated internally as <code>num_trees / leaf_prior_calibration_param^2</code> if not set here.</li> <li><code>keep_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be included in the variance forest. Defaults to <code>None</code>.</li> <li><code>drop_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be excluded from the variance forest. Defaults to <code>None</code>. If both <code>drop_vars</code> and <code>keep_vars</code> are set, <code>drop_vars</code> will be ignored.</li> </ul> <code>None</code> <code>previous_model_json</code> <code>str</code> <p>JSON string containing a previous BART model. This can be used to \"continue\" a sampler interactively after inspecting the samples or to run parallel chains \"warm-started\" from existing forest samples. Defaults to <code>None</code>.</p> <code>None</code> <code>previous_model_warmstart_sample_num</code> <code>int</code> <p>Sample number from <code>previous_model_json</code> that will be used to warmstart this BART sampler. Zero-indexed (so that the first sample is used for warm-start by setting <code>previous_model_warmstart_sample_num = 0</code>). Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>BARTModel</code> <p>Sampled BART Model.</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.predict","title":"<code>predict(covariates, basis=None, rfx_group_ids=None, rfx_basis=None)</code>","text":"<p>Return predictions from every forest sampled (either / both of mean and variance). Return type is either a single array of predictions, if a BART model only includes a mean or variance term, or a tuple of prediction arrays, if a BART model includes both.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array</code> <p>Test set covariates.</p> required <code>basis</code> <code>array</code> <p>Optional test set basis vector, must be provided if the model was trained with a leaf regression basis.</p> <code>None</code> <code>rfx_group_ids</code> <code>array</code> <p>Optional group labels used for an additive random effects model.</p> <code>None</code> <code>rfx_basis</code> <code>array</code> <p>Optional basis for \"random-slope\" regression in an additive random effects model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mu_x</code> <code>(array, optional)</code> <p>Mean forest and / or random effects predictions.</p> <code>sigma2_x</code> <code>(array, optional)</code> <p>Variance forest predictions.</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.predict_mean","title":"<code>predict_mean(covariates, basis=None, rfx_group_ids=None, rfx_basis=None)</code>","text":"<p>Predict expected conditional outcome from a BART model.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array</code> <p>Test set covariates.</p> required <code>basis</code> <code>array</code> <p>Optional test set basis vector, must be provided if the model was trained with a leaf regression basis.</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>Mean forest predictions.</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.predict_variance","title":"<code>predict_variance(covariates)</code>","text":"<p>Predict expected conditional variance from a BART model.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array</code> <p>Test set covariates.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Variance forest predictions.</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.to_json","title":"<code>to_json()</code>","text":"<p>Converts a sampled BART model to JSON string representation (which can then be saved to a file or processed using the <code>json</code> library)</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.from_json","title":"<code>from_json(json_string)</code>","text":"<p>Converts a JSON string to an in-memory BART model.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p> required"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.from_json_string_list","title":"<code>from_json_string_list(json_string_list)</code>","text":"<p>Convert a list of (in-memory) JSON strings that represent BART models to a single combined BART model object which can be used for prediction, etc...</p> <p>Parameters:</p> Name Type Description Default <code>json_string_list</code> <code>list of str</code> <p>List of JSON strings which can be parsed to objects of type <code>JSONSerializer</code> containing Json representation of a BART model</p> required"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.is_sampled","title":"<code>is_sampled()</code>","text":"<p>Whether or not a BART model has been sampled.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if a BART model has been sampled, <code>False</code> otherwise</p>"},{"location":"python_docs/api/bcf.html","title":"BCF","text":""},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel","title":"<code>stochtree.bcf.BCFModel</code>","text":"<p>Class that handles sampling, storage, and serialization of stochastic forest models for causal effect estimation.  The class takes its name from Bayesian Causal Forests, an MCMC sampler originally developed in  Hahn, Murray, Carvalho (2020), but supports several sampling algorithms:</p> <ul> <li>MCMC: The \"classic\" sampler defined in Hahn, Murray, Carvalho (2020). In order to run the MCMC sampler,  set <code>num_gfr = 0</code> (explained below) and then define a sampler according to several parameters:<ul> <li><code>num_burnin</code>: the number of iterations to run before \"retaining\" samples for further analysis. These \"burned in\" samples  are helpful for allowing a sampler to converge before retaining samples.</li> <li><code>num_chains</code>: the number of independent sequences of MCMC samples to generate (typically referred to in the literature as \"chains\")</li> <li><code>num_mcmc</code>: the number of \"retained\" samples of the posterior distribution</li> <li><code>keep_every</code>: after a sampler has \"burned in\", we will run the sampler for <code>keep_every</code> * <code>num_mcmc</code> iterations, retaining one of each <code>keep_every</code> iteration in a chain.</li> </ul> </li> <li>GFR (Grow-From-Root): A fast, greedy approximation of the BART MCMC sampling algorithm introduced in Krantsevich, He, and Hahn (2023). GFR sampler iterations are  governed by the <code>num_gfr</code> parameter, and there are two primary ways to use this sampler:<ul> <li>Standalone: setting <code>num_gfr &gt; 0</code> and both <code>num_burnin = 0</code> and <code>num_mcmc = 0</code> will only run and retain GFR samples of the posterior. This is typically referred to as \"XBART\" (accelerated BART).</li> <li>Initializer for MCMC: setting <code>num_gfr &gt; 0</code> and <code>num_mcmc &gt; 0</code> will use ensembles from the GFR algorithm to initialize <code>num_chains</code> independent MCMC BART samplers, which are run for <code>num_mcmc</code> iterations.  This is typically referred to as \"warm start BART\".</li> </ul> </li> </ul> <p>In addition to enabling multiple samplers, we support a broad set of models. First, note that the original BCF model of Hahn, Murray, Carvalho (2020) is</p> \\[\\begin{equation*} \\begin{aligned} y &amp;= a(X) + b_z(X) + \\epsilon\\\\ b_z(X) &amp;= (b_1 Z + b_0 (1-Z)) t(X)\\\\ b_0, b_1 &amp;\\sim N\\left(0, \\frac{1}{2}\\right)\\\\\\\\ a(X) &amp;\\sim \\text{BART}()\\\\ t(X) &amp;\\sim \\text{BART}()\\\\ \\epsilon &amp;\\sim N(0, \\sigma^2)\\\\ \\sigma^2 &amp;\\sim IG(a, b) \\end{aligned} \\end{equation*}\\] <p>for continuous outcome \\(y\\), binary treatment \\(Z\\), and covariates \\(X\\).</p> <p>In words, there are two nonparametric mean functions -- a \"prognostic\" function and a \"treatment effect\" function -- governed by tree ensembles with BART priors and an additive (mean-zero) Gaussian error  term, whose variance is parameterized with an inverse gamma prior.</p> <p>The <code>BCFModel</code> class supports the following extensions of this model:</p> <ul> <li>Continuous Treatment: If \\(Z\\) is continuous rather than binary, we define \\(b_z(X) = \\tau(X, Z) = Z \\tau(X)\\), where the \"leaf model\" for the \\(\\tau\\) forest is essentially a regression on continuous \\(Z\\).</li> <li>Heteroskedasticity: Rather than define \\(\\epsilon\\) parameterically, we can let a forest \\(\\sigma^2(X)\\) model a conditional error variance function. This can be done by setting <code>num_trees_variance &gt; 0</code> in the <code>params</code> dictionary passed to the <code>sample</code> method.</li> </ul>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.sample","title":"<code>sample(X_train, Z_train, y_train, pi_train=None, rfx_group_ids_train=None, rfx_basis_train=None, X_test=None, Z_test=None, pi_test=None, rfx_group_ids_test=None, rfx_basis_test=None, num_gfr=5, num_burnin=0, num_mcmc=100, general_params=None, prognostic_forest_params=None, treatment_effect_forest_params=None, variance_forest_params=None)</code>","text":"<p>Runs a BCF sampler on provided training set. Outcome predictions and estimates of the prognostic and treatment effect functions will be cached for the training set and (if provided) the test set.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>array or DataFrame</code> <p>Covariates used to split trees in the ensemble. Can be passed as either a matrix or dataframe.</p> required <code>Z_train</code> <code>array</code> <p>Array of (continuous or binary; univariate or multivariate) treatment assignments.</p> required <code>y_train</code> <code>array</code> <p>Outcome to be modeled by the ensemble.</p> required <code>pi_train</code> <code>array</code> <p>Optional vector of propensity scores. If not provided, this will be estimated from the data.</p> <code>None</code> <code>rfx_group_ids_train</code> <code>array</code> <p>Optional group labels used for an additive random effects model.</p> <code>None</code> <code>rfx_basis_train</code> <code>array</code> <p>Optional basis for \"random-slope\" regression in an additive random effects model.</p> <code>None</code> <code>X_test</code> <code>array</code> <p>Optional test set of covariates used to define \"out of sample\" evaluation data.</p> <code>None</code> <code>Z_test</code> <code>array</code> <p>Optional test set of (continuous or binary) treatment assignments. Must be provided if <code>X_test</code> is provided.</p> <code>None</code> <code>pi_test</code> <code>array</code> <p>Optional test set vector of propensity scores. If not provided (but <code>X_test</code> and <code>Z_test</code> are), this will be estimated from the data.</p> <code>None</code> <code>rfx_group_ids_test</code> <code>array</code> <p>Optional test set group labels used for an additive random effects model. We do not currently support (but plan to in the near future), test set evaluation for group labels that were not in the training set.</p> <code>None</code> <code>rfx_basis_test</code> <code>array</code> <p>Optional test set basis for \"random-slope\" regression in additive random effects model.</p> <code>None</code> <code>num_gfr</code> <code>int</code> <p>Number of \"warm-start\" iterations run using the grow-from-root algorithm (He and Hahn, 2021). Defaults to <code>5</code>.</p> <code>5</code> <code>num_burnin</code> <code>int</code> <p>Number of \"burn-in\" iterations of the MCMC sampler. Defaults to <code>0</code>. Ignored if <code>num_gfr &gt; 0</code>.</p> <code>0</code> <code>num_mcmc</code> <code>int</code> <p>Number of \"retained\" iterations of the MCMC sampler. Defaults to <code>100</code>. If this is set to 0, GFR (XBART) samples will be retained.</p> <code>100</code> <code>general_params</code> <code>dict</code> <p>Dictionary of general model parameters, each of which has a default value processed internally, so this argument is optional.</p> <ul> <li><code>cutpoint_grid_size</code> (<code>int</code>): Maximum number of cutpoints to consider for each feature. Defaults to <code>100</code>.</li> <li><code>standardize</code> (<code>bool</code>): Whether or not to standardize the outcome (and store the offset / scale in the model object). Defaults to <code>True</code>.</li> <li><code>sample_sigma2_global</code> (<code>bool</code>): Whether or not to update the <code>sigma^2</code> global error variance parameter based on <code>IG(sigma2_global_shape, sigma2_global_scale)</code>. Defaults to <code>True</code>.</li> <li><code>sigma2_global_init</code> (<code>float</code>): Starting value of global variance parameter. Set internally to the outcome variance (standardized if <code>standardize = True</code>) if not set here.</li> <li><code>sigma2_global_shape</code> (<code>float</code>): Shape parameter in the <code>IG(sigma2_global_shape, b_glsigma2_global_scaleobal)</code> global error variance model. Defaults to <code>0</code>.</li> <li><code>sigma2_global_scale</code> (<code>float</code>): Scale parameter in the <code>IG(sigma2_global_shape, b_glsigma2_global_scaleobal)</code> global error variance model. Defaults to <code>0</code>.</li> <li><code>variable_weights</code> (<code>np.array</code>): Numeric weights reflecting the relative probability of splitting on each variable in each of the forests. Does not need to sum to 1 but cannot be negative. Defaults to <code>np.repeat(1/X_train.shape[1], X_train.shape[1])</code> if not set here. Note that if the propensity score is included as a covariate in either forest, its weight will default to <code>1/X_train.shape[1]</code>. A workaround if you wish to provide a custom weight for the propensity score is to include it as a column in <code>X_train</code> and then set <code>propensity_covariate</code> to <code>'none'</code> and adjust <code>keep_vars</code> accordingly for the mu or tau forests.</li> <li><code>propensity_covariate</code> (<code>str</code>): Whether to include the propensity score as a covariate in either or both of the forests. Enter <code>\"none\"</code> for neither, <code>\"mu\"</code> for the prognostic forest, <code>\"tau\"</code> for the treatment forest, and <code>\"both\"</code> for both forests.     If this is not <code>\"none\"</code> and a propensity score is not provided, it will be estimated from (<code>X_train</code>, <code>Z_train</code>) using <code>BARTModel</code>. Defaults to <code>\"mu\"</code>.</li> <li><code>adaptive_coding</code> (<code>bool</code>): Whether or not to use an \"adaptive coding\" scheme in which a binary treatment variable is not coded manually as (0,1) or (-1,1) but learned via     parameters <code>b_0</code> and <code>b_1</code> that attach to the outcome model <code>[b_0 (1-Z) + b_1 Z] tau(X)</code>. This is ignored when Z is not binary. Defaults to True.</li> <li><code>control_coding_init</code> (<code>float</code>): Initial value of the \"control\" group coding parameter. This is ignored when <code>Z</code> is not binary. Default: <code>-0.5</code>.</li> <li><code>treated_coding_init</code> (<code>float</code>): Initial value of the \"treated\" group coding parameter. This is ignored when <code>Z</code> is not binary. Default: <code>0.5</code>.</li> <li><code>random_seed</code> (<code>int</code>): Integer parameterizing the C++ random number generator. If not specified, the C++ random number generator is seeded according to <code>std::random_device</code>.</li> <li><code>keep_burnin</code> (<code>bool</code>): Whether or not \"burnin\" samples should be included in predictions. Defaults to <code>False</code>. Ignored if <code>num_mcmc == 0</code>.</li> <li><code>keep_gfr</code> (<code>bool</code>): Whether or not \"warm-start\" / grow-from-root samples should be included in predictions. Defaults to <code>False</code>. Ignored if <code>num_mcmc == 0</code>.</li> <li><code>keep_every</code> (<code>int</code>): How many iterations of the burned-in MCMC sampler should be run before forests and parameters are retained. Defaults to <code>1</code>. Setting <code>keep_every = k</code> for some <code>k &gt; 1</code> will \"thin\" the MCMC samples by retaining every <code>k</code>-th sample, rather than simply every sample. This can reduce the autocorrelation of the MCMC samples.</li> <li><code>num_chains</code> (<code>int</code>): How many independent MCMC chains should be sampled. If <code>num_mcmc = 0</code>, this is ignored. If <code>num_gfr = 0</code>, then each chain is run from root for <code>num_mcmc * keep_every + num_burnin</code> iterations, with <code>num_mcmc</code> samples retained. If <code>num_gfr &gt; 0</code>, each MCMC chain will be initialized from a separate GFR ensemble, with the requirement that <code>num_gfr &gt;= num_chains</code>. Defaults to <code>1</code>.</li> </ul> <code>None</code> <code>prognostic_forest_params</code> <code>dict</code> <p>Dictionary of prognostic forest model parameters, each of which has a default value processed internally, so this argument is optional.</p> <ul> <li><code>num_trees</code> (<code>int</code>): Number of trees in the prognostic forest. Defaults to <code>250</code>. Must be a positive integer.</li> <li><code>alpha</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 in the prognostic forest. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>0.95</code>.</li> <li><code>beta</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 in the prognostic forest. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>2</code>.</li> <li><code>min_samples_leaf</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples, in the prognostic forest. Defaults to <code>5</code>.</li> <li><code>max_depth</code> (<code>int</code>): Maximum depth of any tree in the ensemble in the prognostic forest. Defaults to <code>10</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>variable_weights</code> (<code>np.array</code>): Numeric weights reflecting the relative probability of splitting on each variable in the prognostic forest. Does not need to sum to 1 but cannot be negative. Defaults to uniform over the columns of <code>X_train</code> if not provided.</li> <li><code>sample_sigma2_leaf</code> (<code>bool</code>): Whether or not to update the <code>tau</code> leaf scale variance parameter based on <code>IG(sigma2_leaf_shape, sigma2_leaf_scale)</code>. Cannot (currently) be set to true if <code>basis_train</code> has more than one column. Defaults to <code>False</code>.</li> <li><code>sigma2_leaf_init</code> (<code>float</code>): Starting value of leaf node scale parameter. Calibrated internally as <code>1/num_trees</code> if not set here.</li> <li><code>sigma2_leaf_shape</code> (<code>float</code>): Shape parameter in the <code>IG(sigma2_leaf_shape, sigma2_leaf_scale)</code> leaf node parameter variance model. Defaults to <code>3</code>.</li> <li><code>sigma2_leaf_scale</code> (<code>float</code>): Scale parameter in the <code>IG(sigma2_leaf_shape, sigma2_leaf_scale)</code> leaf node parameter variance model. Calibrated internally as <code>0.5/num_trees</code> if not set here.</li> <li><code>keep_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be included in the prognostic (<code>mu(X)</code>) forest. Defaults to <code>None</code>.</li> <li><code>drop_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be excluded from the prognostic (<code>mu(X)</code>) forest. Defaults to <code>None</code>. If both <code>drop_vars</code> and <code>keep_vars</code> are set, <code>drop_vars</code> will be ignored.</li> </ul> <code>None</code> <code>treatment_effect_forest_params</code> <code>dict</code> <p>Dictionary of treatment effect forest model parameters, each of which has a default value processed internally, so this argument is optional.</p> <ul> <li><code>num_trees</code> (<code>int</code>): Number of trees in the treatment effect forest. Defaults to <code>50</code>. Must be a positive integer.</li> <li><code>alpha</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 in the treatment effect forest. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>0.25</code>.</li> <li><code>beta</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 in the treatment effect forest. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>3</code>.</li> <li><code>min_samples_leaf</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples, in the treatment effect forest. Defaults to <code>5</code>.</li> <li><code>max_depth</code> (<code>int</code>): Maximum depth of any tree in the ensemble in the treatment effect forest. Defaults to <code>5</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>sample_sigma2_leaf</code> (<code>bool</code>): Whether or not to update the <code>tau</code> leaf scale variance parameter based on <code>IG(sigma2_leaf_shape, sigma2_leaf_scale)</code>. Cannot (currently) be set to true if <code>basis_train</code> has more than one column. Defaults to <code>False</code>.</li> <li><code>sigma2_leaf_init</code> (<code>float</code>): Starting value of leaf node scale parameter. Calibrated internally as <code>1/num_trees</code> if not set here.</li> <li><code>sigma2_leaf_shape</code> (<code>float</code>): Shape parameter in the <code>IG(sigma2_leaf_shape, sigma2_leaf_scale)</code> leaf node parameter variance model. Defaults to <code>3</code>.</li> <li><code>sigma2_leaf_scale</code> (<code>float</code>): Scale parameter in the <code>IG(sigma2_leaf_shape, sigma2_leaf_scale)</code> leaf node parameter variance model. Calibrated internally as <code>0.5/num_trees</code> if not set here.</li> <li><code>keep_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be included in the treatment effect (<code>tau(X)</code>) forest. Defaults to <code>None</code>.</li> <li><code>drop_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be excluded from the treatment effect (<code>tau(X)</code>) forest. Defaults to <code>None</code>. If both <code>drop_vars</code> and <code>keep_vars</code> are set, <code>drop_vars</code> will be ignored.</li> </ul> <code>None</code> <code>variance_forest_params</code> <code>dict</code> <p>Dictionary of variance forest model  parameters, each of which has a default value processed internally, so this argument is optional.</p> <ul> <li><code>num_trees</code> (<code>int</code>): Number of trees in the conditional variance model. Defaults to <code>0</code>. Variance is only modeled using a tree / forest if <code>num_trees &gt; 0</code>.</li> <li><code>alpha</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 in the conditional variance model. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>0.95</code>.</li> <li><code>beta</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 in the conditional variance model. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>. Defaults to <code>2</code>.</li> <li><code>min_samples_leaf</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples, in the conditional variance model. Defaults to <code>5</code>.</li> <li><code>max_depth</code> (<code>int</code>): Maximum depth of any tree in the ensemble in the conditional variance model. Defaults to <code>10</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>leaf_prior_calibration_param</code> (<code>float</code>): Hyperparameter used to calibrate the [optional] <code>IG(var_forest_prior_shape, var_forest_prior_scale)</code> conditional error variance model. If <code>var_forest_prior_shape</code> and <code>var_forest_prior_scale</code> are not set below, this calibration parameter is used to set these values to <code>num_trees / leaf_prior_calibration_param^2 + 0.5</code> and <code>num_trees / leaf_prior_calibration_param^2</code>, respectively. Defaults to <code>1.5</code>.</li> <li><code>var_forest_leaf_init</code> (<code>float</code>): Starting value of root forest prediction in conditional (heteroskedastic) error variance model. Calibrated internally as <code>np.log(0.6*np.var(y_train))/num_trees_variance</code>, where <code>y_train</code> is the possibly standardized outcome, if not set.</li> <li><code>var_forest_prior_shape</code> (<code>float</code>): Shape parameter in the [optional] <code>IG(var_forest_prior_shape, var_forest_prior_scale)</code> conditional error variance forest (which is only sampled if <code>num_trees &gt; 0</code>). Calibrated internally as <code>num_trees / 1.5^2 + 0.5</code> if not set here.</li> <li><code>var_forest_prior_scale</code> (<code>float</code>): Scale parameter in the [optional] <code>IG(var_forest_prior_shape, var_forest_prior_scale)</code> conditional error variance forest (which is only sampled if <code>num_trees &gt; 0</code>). Calibrated internally as <code>num_trees / 1.5^2</code> if not set here.</li> <li><code>keep_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be included in the variance forest. Defaults to <code>None</code>.</li> <li><code>drop_vars</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be excluded from the variance forest. Defaults to <code>None</code>. If both <code>drop_vars</code> and <code>keep_vars</code> are set, <code>drop_vars</code> will be ignored.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>BCFModel</code> <p>Sampled BCF Model.</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.predict_tau","title":"<code>predict_tau(X, Z, propensity=None)</code>","text":"<p>Predict CATE function for every provided observation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array or DataFrame</code> <p>Test set covariates.</p> required <code>Z</code> <code>array</code> <p>Test set treatment indicators.</p> required <code>propensity</code> <code>array</code> <p>Optional test set propensities. Must be provided if propensities were provided when the model was sampled.</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>Array with as many rows as in <code>X</code> and as many columns as retained samples of the algorithm.</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.predict_variance","title":"<code>predict_variance(covariates, propensity=None)</code>","text":"<p>Predict expected conditional variance from a BART model.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array</code> <p>Test set covariates.</p> required <code>propensity</code> <code>array</code> <p>Test set propensity scores. Optional (not currently used in variance forests).</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>Array of predictions corresponding to the variance forest. Each array will contain as many rows as in <code>covariates</code> and as many columns as retained samples of the algorithm.</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.predict","title":"<code>predict(X, Z, propensity=None, rfx_group_ids=None, rfx_basis=None)</code>","text":"<p>Predict outcome model components (CATE function and prognostic function) as well as overall outcome for every provided observation. Predicted outcomes are computed as <code>yhat = mu_x + Z*tau_x</code> where mu_x is a sample of the prognostic function and tau_x is a sample of the treatment effect (CATE) function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array or DataFrame</code> <p>Test set covariates.</p> required <code>Z</code> <code>array</code> <p>Test set treatment indicators.</p> required <code>propensity</code> <code>`np.array`</code> <p>Optional test set propensities. Must be provided if propensities were provided when the model was sampled.</p> <code>None</code> <code>rfx_group_ids</code> <code>array</code> <p>Optional group labels used for an additive random effects model.</p> <code>None</code> <code>rfx_basis</code> <code>array</code> <p>Optional basis for \"random-slope\" regression in an additive random effects model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tau_x</code> <code>array</code> <p>Conditional average treatment effect (CATE) samples for every observation provided.</p> <code>mu_x</code> <code>array</code> <p>Prognostic effect samples for every observation provided.</p> <code>rfx</code> <code>(array, optional)</code> <p>Random effect samples for every observation provided, if the model includes a random effects term.</p> <code>yhat_x</code> <code>array</code> <p>Outcome prediction samples for every observation provided.</p> <code>sigma2_x</code> <code>(array, optional)</code> <p>Variance forest samples for every observation provided. Only returned if the model includes a heteroskedasticity forest.</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.to_json","title":"<code>to_json()</code>","text":"<p>Converts a sampled BART model to JSON string representation (which can then be saved to a file or processed using the <code>json</code> library)</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.from_json","title":"<code>from_json(json_string)</code>","text":"<p>Converts a JSON string to an in-memory BART model.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p> required"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.from_json_string_list","title":"<code>from_json_string_list(json_string_list)</code>","text":"<p>Convert a list of (in-memory) JSON strings that represent BCF models to a single combined BCF model object which can be used for prediction, etc...</p> <p>Parameters:</p> Name Type Description Default <code>json_string_list</code> <code>list of str</code> <p>List of JSON strings which can be parsed to objects of type <code>JSONSerializer</code> containing Json representation of a BCF model</p> required"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.is_sampled","title":"<code>is_sampled()</code>","text":"<p>Whether or not a BCF model has been sampled.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if a BCF model has been sampled, <code>False</code> otherwise</p>"},{"location":"python_docs/api/low-level/index.html","title":"Low-Level API","text":"<p>In addition to high-level samplers for BART and BCF,  the <code>stochtree</code> Python library provides direct access to many of the computational structures that  underlie stochastic tree algorithms: tree ensembles, sampling algorithms, and \"tracking\" data structures  that enable the algorithms to work effectively. This interface consists of:</p> <ol> <li>Data API: loading and storing in-memory data needed to train <code>stochtree</code> models.</li> <li>Forest API: creating, storing, and modifying ensembles of decision trees that underlie all <code>stochtree</code> models.</li> <li>Sampler API: sampling from stochastic tree ensemble models as well as several supported parametric models.</li> <li>Utilities API: seeding a C++ random number generator, preprocessing data, and serializing models to JSON (files or in-memory strings).</li> </ol>"},{"location":"python_docs/api/low-level/dataset.html","title":"Data API","text":""},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset","title":"<code>stochtree.data.Dataset()</code>","text":"<p>Wrapper around a C++ class that stores all of the non-outcome data used in <code>stochtree</code>. This includes:</p> <ol> <li>Features used for partitioning (also referred to as \"covariates\" in many places in these docs).</li> <li>Basis vectors used to define non-constant leaf models. This is optional but may be included via the <code>add_basis</code> method.</li> <li>Variance weights used to define heteroskedastic or otherwise weighted models. This is optional but may be included via the <code>add_variance_weights</code> method.</li> </ol>"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset.add_covariates","title":"<code>add_covariates(covariates)</code>","text":"<p>Add covariates to a dataset</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array</code> <p>Numpy array of covariates. If data contain categorical, string, time series, or other columns in a dataframe, please first preprocess using the <code>CovariateTransformer</code>.</p> required"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset.add_basis","title":"<code>add_basis(basis)</code>","text":"<p>Add basis matrix to a dataset</p> <p>Parameters:</p> Name Type Description Default <code>basis</code> <code>array</code> <p>Numpy array of basis vectors.</p> required"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset.update_basis","title":"<code>update_basis(basis)</code>","text":"<p>Update basis matrix in a dataset. Allows users to build an ensemble whose leaves regress on bases that are updated throughout the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>basis</code> <code>array</code> <p>Numpy array of basis vectors.</p> required"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset.add_variance_weights","title":"<code>add_variance_weights(variance_weights)</code>","text":"<p>Add variance weights to a dataset</p> <p>Parameters:</p> Name Type Description Default <code>variance_weights</code> <code>array</code> <p>Univariate numpy array of variance weights.</p> required"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset.num_observations","title":"<code>num_observations()</code>","text":"<p>Query the number of observations in a dataset</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of observations in the dataset</p>"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset.num_covariates","title":"<code>num_covariates()</code>","text":"<p>Query the number of covariates (features) in a dataset</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of covariates in the dataset</p>"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset.num_basis","title":"<code>num_basis()</code>","text":"<p>Query the dimension of the basis vector in a dataset</p> <p>Returns:</p> Type Description <code>int</code> <p>Dimension of the basis vector in the dataset, returning 0 if the dataset does not have a basis</p>"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset.has_basis","title":"<code>has_basis()</code>","text":"<p>Whether or not a dataset has a basis vector (for leaf regression)</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the dataset has a basis, <code>False</code> otherwise</p>"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Dataset.has_variance_weights","title":"<code>has_variance_weights()</code>","text":"<p>Whether or not a dataset has variance weights</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the dataset has variance weights, <code>False</code> otherwise</p>"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Residual","title":"<code>stochtree.data.Residual(residual)</code>","text":"<p>Wrapper around a C++ class that stores residual data used in <code>stochtree</code>. This object becomes part of the real-time model \"state\" in that its contents always contain a full or partial residual, depending on the state of the sampler.</p> <p>Typically this object is initialized with the original outcome and then \"residualized\" by subtracting out the initial prediction value of every tree in every forest term (as well as the predictions of any other model term).</p> <p>Parameters:</p> Name Type Description Default <code>residual</code> <code>array</code> <p>Univariate numpy array of residual values.</p> required"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Residual.get_residual","title":"<code>get_residual()</code>","text":"<p>Extract the current values of the residual as a numpy array</p> <p>Returns:</p> Type Description <code>array</code> <p>Current values of the residual (which may be net of any forest / other model terms)</p>"},{"location":"python_docs/api/low-level/dataset.html#stochtree.data.Residual.update_data","title":"<code>update_data(new_vector)</code>","text":"<p>Update the current state of the outcome (i.e. partial residual) data by replacing each element with the elements of <code>new_vector</code></p> <p>Parameters:</p> Name Type Description Default <code>new_vector</code> <code>array</code> <p>Univariate numpy array of new residual values.</p> required"},{"location":"python_docs/api/low-level/forest.html","title":"Forest API","text":""},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest","title":"<code>stochtree.forest.Forest</code>","text":"<p>In-memory python wrapper around a C++ tree ensemble object</p> <p>Parameters:</p> Name Type Description Default <code>num_trees</code> <code>int</code> <p>Number of trees that each forest should contain</p> required <code>output_dimension</code> <code>int</code> <p>Dimension of the leaf node parameters in each tree</p> <code>1</code> <code>leaf_constant</code> <code>bool</code> <p>Whether the leaf node model is \"constant\" (i.e. prediction is simply a sum of leaf node parameters for every observation in a dataset) or not (i.e. each leaf node parameter is multiplied by a \"basis vector\" before being returned as a prediction).</p> <code>True</code> <code>is_exponentiated</code> <code>bool</code> <p>Whether or not the leaf node parameters are stored in log scale (in which case, they must be exponentiated before being returned as predictions).</p> <code>False</code>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.reset_root","title":"<code>reset_root()</code>","text":"<p>Reset forest to a forest with all single node (i.e. \"root\") trees</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.reset","title":"<code>reset(forest_container, forest_num)</code>","text":"<p>Reset forest to the forest indexed by <code>forest_num</code> in <code>forest_container</code></p> <p>Parameters:</p> Name Type Description Default <code>forest_container</code> <code>`ForestContainer</code> <p>Stochtree object storing tree ensembles</p> required <code>forest_num</code> <code>int</code> <p>Index of the ensemble used to reset the <code>Forest</code></p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.predict","title":"<code>predict(dataset)</code>","text":"<p>Predict from each forest in the container, using the provided <code>Dataset</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Python object wrapping the \"dataset\" class used by C++ sampling and prediction data structures.</p> required <p>Returns:</p> Type Description <code>array</code> <p>One-dimensional numpy array with length equal to the number of observations in <code>dataset</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.predict_raw","title":"<code>predict_raw(dataset)</code>","text":"<p>Predict raw leaf values for a every forest in the container, using the provided <code>Dataset</code> object</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Python object wrapping the \"dataset\" class used by C++ sampling and prediction data structures.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array with (<code>n</code>, <code>k</code>) dimensions, where <code>n</code> is the number of observations in <code>dataset</code> and <code>k</code> is the dimension of the leaf parameter. If <code>k = 1</code>, then the returned array is simply one-dimensional with <code>n</code> observations.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.set_root_leaves","title":"<code>set_root_leaves(leaf_value)</code>","text":"<p>Set constant (root) leaf node values for every tree in the forest. Assumes the forest consists of all root (single-node) trees.</p> <p>Parameters:</p> Name Type Description Default <code>leaf_value</code> <code>float or array</code> <p>Constant values to which root nodes are to be set. If the trees in forest <code>forest_num</code> are univariate, then <code>leaf_value</code> must be a <code>float</code>, while if the trees in forest <code>forest_num</code> are multivariate, then <code>leaf_value</code> must be a <code>np.array</code>.</p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.add_numeric_split","title":"<code>add_numeric_split(tree_num, leaf_num, feature_num, split_threshold, left_leaf_value, right_leaf_value)</code>","text":"<p>Add a numeric (i.e. X[,i] &lt;= c) split to a given tree in the forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be split</p> required <code>leaf_num</code> <code>int</code> <p>Leaf to be split</p> required <code>feature_num</code> <code>int</code> <p>Feature that defines the new split</p> required <code>split_threshold</code> <code>float</code> <p>Value that defines the cutoff of the new split</p> required <code>left_leaf_value</code> <code>float or array</code> <p>Value (or array of values) to assign to the newly created left node</p> required <code>right_leaf_value</code> <code>float or array</code> <p>Value (or array of values) to assign to the newly created right node</p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.get_tree_leaves","title":"<code>get_tree_leaves(tree_num)</code>","text":"<p>Retrieve a vector of indices of leaf nodes for a given tree in the forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>float or array</code> <p>Index of the tree for which leaf indices will be retrieved</p> required <p>Returns:</p> Type Description <code>array</code> <p>One-dimensional numpy array, containing the indices of leaf nodes in a given tree.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.get_tree_split_counts","title":"<code>get_tree_split_counts(tree_num, num_features)</code>","text":"<p>Retrieve a vector of split counts for every training set variable in a given tree in the forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree for which split counts will be retrieved</p> required <code>num_features</code> <code>int</code> <p>Total number of features in the training set</p> required <p>Returns:</p> Type Description <code>array</code> <p>One-dimensional numpy array with as many elements as in the forest model's training set, containing the split count for each feature for a given tree of the forest.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.get_overall_split_counts","title":"<code>get_overall_split_counts(num_features)</code>","text":"<p>Retrieve a vector of split counts for every training set variable in the forest</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Total number of features in the training set</p> required <p>Returns:</p> Type Description <code>array</code> <p>One-dimensional numpy array with as many elements as in the forest model's training set, containing the overall split count in the forest for each feature.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.get_granular_split_counts","title":"<code>get_granular_split_counts(num_features)</code>","text":"<p>Retrieve a vector of split counts for every training set variable in the forest, reported separately for each tree</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Total number of features in the training set</p> required <p>Returns:</p> Type Description <code>array</code> <p>One-dimensional numpy array with as many elements as in the forest model's training set, containing the split count for each feature for a every tree in the forest.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.num_forest_leaves","title":"<code>num_forest_leaves()</code>","text":"<p>Return the total number of leaves in a forest</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of leaves in a forest</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.sum_leaves_squared","title":"<code>sum_leaves_squared()</code>","text":"<p>Return the total sum of squared leaf values in a forest</p> <p>Returns:</p> Type Description <code>float</code> <p>Sum of squared leaf values in a forest</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.is_leaf_node","title":"<code>is_leaf_node(tree_num, node_id)</code>","text":"<p>Whether or not a given node of a given tree of a forest is a leaf</p> <p>tree_num : int     Index of the tree to be queried node_id : int     Index of the node to be queried</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if node <code>node_id</code> in tree <code>tree_num</code> is a leaf, <code>False</code> otherwise</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.is_numeric_split_node","title":"<code>is_numeric_split_node(tree_num, node_id)</code>","text":"<p>Whether or not a given node of a given tree of a forest is a numeric split node</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if node <code>node_id</code> in tree <code>tree_num</code> is a numeric split node, <code>False</code> otherwise</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.is_categorical_split_node","title":"<code>is_categorical_split_node(tree_num, node_id)</code>","text":"<p>Whether or not a given node of a given tree of a forest is a categorical split node</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if node <code>node_id</code> in tree <code>tree_num</code> is a categorical split node, <code>False</code> otherwise</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.parent_node","title":"<code>parent_node(tree_num, node_id)</code>","text":"<p>Parent node of given node of a given tree of a forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index of the parent of node <code>node_id</code> in tree <code>tree_num</code>. If <code>node_id</code> is a root node, returns <code>-1</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.left_child_node","title":"<code>left_child_node(tree_num, node_id)</code>","text":"<p>Left child node of given node of a given tree of a forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index of the left child of node <code>node_id</code> in tree <code>tree_num</code>. If <code>node_id</code> is a leaf, returns <code>-1</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.right_child_node","title":"<code>right_child_node(tree_num, node_id)</code>","text":"<p>Right child node of given node of a given tree of a forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index of the right child of node <code>node_id</code> in tree <code>tree_num</code>. If <code>node_id</code> is a leaf, returns <code>-1</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.node_depth","title":"<code>node_depth(tree_num, node_id)</code>","text":"<p>Depth of given node of a given tree of a forest Returns <code>-1</code> if the node is a leaf.</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Depth of node <code>node_id</code> in tree <code>tree_num</code>. The root node is defined as \"depth zero.\"</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.node_split_index","title":"<code>node_split_index(tree_num, node_id)</code>","text":"<p>Split index of given node of a given tree of a forest. Returns <code>-1</code> if the node is a leaf.</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Split index of <code>node_id</code> in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.node_split_threshold","title":"<code>node_split_threshold(tree_num, node_id)</code>","text":"<p>Threshold that defines a numeric split for a given node of a given tree of a forest. Returns <code>np.Inf</code> if the node is a leaf or a categorical split node.</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>float</code> <p>Threshold that defines a numeric split for node <code>node_id</code> in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.node_split_categories","title":"<code>node_split_categories(tree_num, node_id)</code>","text":"<p>Array of category indices that define a categorical split for a given node of a given tree of a forest. Returns <code>np.array([np.Inf])</code> if the node is a leaf or a numeric split node.</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of category indices that define a categorical split for node <code>node_id</code> in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.node_leaf_values","title":"<code>node_leaf_values(tree_num, node_id)</code>","text":"<p>Leaf node value(s) for a given node of a given tree of a forest. Values are stale if the node is a split node.</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of parameter values for node <code>node_id</code> in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.num_nodes","title":"<code>num_nodes(tree_num)</code>","text":"<p>Number of nodes in a given tree of a forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of nodes in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.num_leaves","title":"<code>num_leaves(tree_num)</code>","text":"<p>Number of leaves in a given tree of a forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of leaves in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.num_leaf_parents","title":"<code>num_leaf_parents(tree_num)</code>","text":"<p>Number of leaf parents in a given tree of a forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of leaf parents in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.num_split_nodes","title":"<code>num_split_nodes(tree_num)</code>","text":"<p>Number of split_nodes in a given tree of a forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of split nodes in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.nodes","title":"<code>nodes(tree_num)</code>","text":"<p>Array of node indices in a given tree of a forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of indices of nodes in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.leaves","title":"<code>leaves(tree_num)</code>","text":"<p>Array of leaf indices in a given tree of a forest</p> <p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of indices of leaf nodes in tree <code>tree_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.Forest.is_empty","title":"<code>is_empty()</code>","text":"<p>When a Forest object is created, it is \"empty\" in the sense that none of its component trees have leaves with values. There are two ways to \"initialize\" a Forest object. First, the <code>set_root_leaves()</code> method of the Forest class simply initializes every tree in the forest to a single node carrying the same (user-specified) leaf value. Second, the <code>prepare_for_sampler()</code> method of the ForestSampler class initializes every tree in the forest to a single node with the same value and also propagates this information through to the temporary tracking data structrues in a ForestSampler object, which must be synchronized with a Forest during a forest sampler loop.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if a Forest has not yet been initialized with a constant root value, <code>False</code> otherwise if the forest has already been initialized / grown.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer","title":"<code>stochtree.forest.ForestContainer</code>","text":"<p>Container that stores sampled (and retained) tree ensembles from BART, BCF or a custom sampler.</p> <p>Parameters:</p> Name Type Description Default <code>num_trees</code> <code>int</code> <p>Number of trees that each forest should contain</p> required <code>output_dimension</code> <code>int</code> <p>Dimension of the leaf node parameters in each tree</p> <code>1</code> <code>leaf_constant</code> <code>bool</code> <p>Whether the leaf node model is \"constant\" (i.e. prediction is simply a sum of leaf node parameters for every observation in a dataset) or not (i.e. each leaf node parameter is multiplied by a \"basis vector\" before being returned as a prediction).</p> <code>True</code> <code>is_exponentiated</code> <code>bool</code> <p>Whether or not the leaf node parameters are stored in log scale (in which case, they must be exponentiated before being returned as predictions).</p> <code>False</code>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.predict","title":"<code>predict(dataset)</code>","text":"<p>Predict from each forest in the container, using the provided <code>Dataset</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Python object wrapping the \"dataset\" class used by C++ sampling and prediction data structures.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array with (<code>n</code>, <code>m</code>) dimensions, where <code>n</code> is the number of observations in <code>dataset</code> and <code>m</code> is the number of samples in the forest container.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.predict_raw","title":"<code>predict_raw(dataset)</code>","text":"<p>Predict raw leaf values for a every forest in the container, using the provided <code>Dataset</code> object</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Python object wrapping the \"dataset\" class used by C++ sampling and prediction data structures.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array with (<code>n</code>, <code>k</code>, <code>m</code>) dimensions, where <code>n</code> is the number of observations in <code>dataset</code>, <code>k</code> is the dimension of the leaf parameter, and <code>m</code> is the number of samples in the forest container. If <code>k = 1</code>, then the returned array is simply (<code>n</code>, <code>m</code>) dimensions.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.predict_raw_single_forest","title":"<code>predict_raw_single_forest(dataset, forest_num)</code>","text":"<p>Predict raw leaf values for a specific forest (indexed by <code>forest_num</code>), using the provided <code>Dataset</code> object</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Python object wrapping the \"dataset\" class used by C++ sampling and prediction data structures.</p> required <code>forest_num</code> <code>int</code> <p>Index of the forest from which to predict. Forest indices are 0-based.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array with (<code>n</code>, <code>k</code>) dimensions, where <code>n</code> is the number of observations in <code>dataset</code> and <code>k</code> is the dimension of the leaf parameter.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.predict_raw_single_tree","title":"<code>predict_raw_single_tree(dataset, forest_num, tree_num)</code>","text":"<p>Predict raw leaf values for a specific tree of a specific forest (indexed by <code>tree_num</code> and <code>forest_num</code> respectively), using the provided <code>Dataset</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Python object wrapping the \"dataset\" class used by C++ sampling and prediction data structures.</p> required <code>forest_num</code> <code>int</code> <p>Index of the forest from which to predict. Forest indices are 0-based.</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree which to predict (within forest indexed by <code>forest_num</code>). Tree indices are 0-based.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array with (<code>n</code>, <code>k</code>) dimensions, where <code>n</code> is the number of observations in <code>dataset</code> and <code>k</code> is the dimension of the leaf parameter.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.set_root_leaves","title":"<code>set_root_leaves(forest_num, leaf_value)</code>","text":"<p>Set constant (root) leaf node values for every tree in the forest indexed by <code>forest_num</code>. Assumes the forest consists of all root (single-node) trees.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest for which we will set root node parameters.</p> required <code>leaf_value</code> <code>float or array</code> <p>Constant values to which root nodes are to be set. If the trees in forest <code>forest_num</code> are univariate, then <code>leaf_value</code> must be a <code>float</code>, while if the trees in forest <code>forest_num</code> are multivariate, then <code>leaf_value</code> must be a <code>np.array</code>.</p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.save_to_json_file","title":"<code>save_to_json_file(json_filename)</code>","text":"<p>Save the forests in the container to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>json_filename</code> <code>str</code> <p>Name of JSON file to which forest container state will be saved. May contain absolute or relative paths.</p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.load_from_json_file","title":"<code>load_from_json_file(json_filename)</code>","text":"<p>Load a forest container from output stored in a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>json_filename</code> <code>str</code> <p>Name of JSON file from which forest container state will be restored. May contain absolute or relative paths.</p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.dump_json_string","title":"<code>dump_json_string()</code>","text":"<p>Dump a forest container into an in-memory JSON string (which can be directly serialized or combined with other JSON strings before serialization).</p> <p>Returns:</p> Type Description <code>str</code> <p>In-memory string containing state of a forest container.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.load_from_json_string","title":"<code>load_from_json_string(json_string)</code>","text":"<p>Reload a forest container from an in-memory JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>In-memory string containing state of a forest container.</p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.load_from_json_object","title":"<code>load_from_json_object(json_object)</code>","text":"<p>Reload a forest container from an in-memory JSONSerializer object.</p> <p>Parameters:</p> Name Type Description Default <code>json_object</code> <code>JSONSerializer</code> <p>In-memory JSONSerializer object.</p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.add_sample","title":"<code>add_sample(leaf_value)</code>","text":"<p>Add a new all-root ensemble to the container, with all of the leaves set to the value / vector provided</p> <p>Parameters:</p> Name Type Description Default <code>leaf_value</code> <code>float or array</code> <p>Value (or vector of values) to initialize root nodes of every tree in a forest</p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.add_numeric_split","title":"<code>add_numeric_split(forest_num, tree_num, leaf_num, feature_num, split_threshold, left_leaf_value, right_leaf_value)</code>","text":"<p>Add a numeric (i.e. X[,i] &lt;= c) split to a given tree in the ensemble</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest which contains the tree to be split</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be split</p> required <code>leaf_num</code> <code>int</code> <p>Leaf to be split</p> required <code>feature_num</code> <code>int</code> <p>Feature that defines the new split</p> required <code>split_threshold</code> <code>float</code> <p>Value that defines the cutoff of the new split</p> required <code>left_leaf_value</code> <code>float or array</code> <p>Value (or array of values) to assign to the newly created left node</p> required <code>right_leaf_value</code> <code>float or array</code> <p>Value (or array of values) to assign to the newly created right node</p> required"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.get_tree_leaves","title":"<code>get_tree_leaves(forest_num, tree_num)</code>","text":"<p>Retrieve a vector of indices of leaf nodes for a given tree in a given forest</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest which contains tree <code>tree_num</code></p> required <code>tree_num</code> <code>float or array</code> <p>Index of the tree for which leaf indices will be retrieved</p> required <p>Returns:</p> Type Description <code>array</code> <p>One-dimensional numpy array, containing the indices of leaf nodes in a given tree.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.get_tree_split_counts","title":"<code>get_tree_split_counts(forest_num, tree_num, num_features)</code>","text":"<p>Retrieve a vector of split counts for every training set feature in a given tree in a given forest</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest which contains tree <code>tree_num</code></p> required <code>tree_num</code> <code>int</code> <p>Index of the tree for which split counts will be retrieved</p> required <code>num_features</code> <code>int</code> <p>Total number of features in the training set</p> required <p>Returns:</p> Type Description <code>array</code> <p>One-dimensional numpy array with as many elements as in the forest model's training set, containing the split count for each feature for a given forest and tree.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.get_forest_split_counts","title":"<code>get_forest_split_counts(forest_num, num_features)</code>","text":"<p>Retrieve a vector of split counts for every training set feature in a given forest</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest which contains tree <code>tree_num</code></p> required <code>num_features</code> <code>int</code> <p>Total number of features in the training set</p> required <p>Returns:</p> Type Description <code>array</code> <p>One-dimensional numpy array with as many elements as in the forest model's training set, containing the split count for each feature for a given forest (summed across every tree in the forest).</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.get_overall_split_counts","title":"<code>get_overall_split_counts(num_features)</code>","text":"<p>Retrieve a vector of split counts for every training set feature, aggregated across ensembles and trees.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Total number of features in the training set</p> required <p>Returns:</p> Type Description <code>array</code> <p>One-dimensional numpy array with as many elements as in the forest model's training set, containing the split count for each feature summed across every forest of every tree in the container.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.get_granular_split_counts","title":"<code>get_granular_split_counts(num_features)</code>","text":"<p>Retrieve a vector of split counts for every training set variable in a given forest, reported separately for each ensemble and tree</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Total number of features in the training set</p> required <p>Returns:</p> Type Description <code>array</code> <p>Three-dimensional numpy array, containing the number of splits a variable receives in each tree of each forest in a <code>ForestContainer</code>. Array will have dimensions (<code>m</code>,<code>b</code>,<code>p</code>) where <code>m</code> is the number of forests in the container, <code>b</code> is the number of trees in each forest, and <code>p</code> is the number of features in the forest model's training dataset.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.num_forest_leaves","title":"<code>num_forest_leaves(forest_num)</code>","text":"<p>Return the total number of leaves for a given forest in the <code>ForestContainer</code></p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of leaves in a given forest in a <code>ForestContainer</code></p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.sum_leaves_squared","title":"<code>sum_leaves_squared(forest_num)</code>","text":"<p>Return the total sum of squared leaf values for a given forest in the <code>ForestContainer</code></p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <p>Returns:</p> Type Description <code>float</code> <p>Sum of squared leaf values in a given forest in a <code>ForestContainer</code></p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.is_leaf_node","title":"<code>is_leaf_node(forest_num, tree_num, node_id)</code>","text":"<p>Whether or not a given node of a given tree in a given forest in the <code>ForestContainer</code> is a leaf</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code> is a leaf, <code>False</code> otherwise</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.is_numeric_split_node","title":"<code>is_numeric_split_node(forest_num, tree_num, node_id)</code>","text":"<p>Whether or not a given node of a given tree in a given forest in the <code>ForestContainer</code> is a numeric split node</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code> is a numeric split node, <code>False</code> otherwise</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.is_categorical_split_node","title":"<code>is_categorical_split_node(forest_num, tree_num, node_id)</code>","text":"<p>Whether or not a given node of a given tree in a given forest in the <code>ForestContainer</code> is a categorical split node</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code> is a categorical split node, <code>False</code> otherwise</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.parent_node","title":"<code>parent_node(forest_num, tree_num, node_id)</code>","text":"<p>Parent node of given node of a given tree in a given forest in the <code>ForestContainer</code></p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index of the parent of node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code>. If <code>node_id</code> is a root node, returns <code>-1</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.left_child_node","title":"<code>left_child_node(forest_num, tree_num, node_id)</code>","text":"<p>Left child node of given node of a given tree in a given forest in the <code>ForestContainer</code></p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index of the left child of node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code>. If <code>node_id</code> is a leaf, returns <code>-1</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.right_child_node","title":"<code>right_child_node(forest_num, tree_num, node_id)</code>","text":"<p>Right child node of given node of a given tree in a given forest in the <code>ForestContainer</code></p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index of the right child of node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code>. If <code>node_id</code> is a leaf, returns <code>-1</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.node_depth","title":"<code>node_depth(forest_num, tree_num, node_id)</code>","text":"<p>Depth of given node of a given tree in a given forest in the <code>ForestContainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Depth of node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code>. The root node is defined as \"depth zero.\"</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.node_split_index","title":"<code>node_split_index(forest_num, tree_num, node_id)</code>","text":"<p>Split index of given node of a given tree in a given forest in the <code>ForestContainer</code>. Returns <code>-1</code> if the node is a leaf.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Split index of <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.node_split_threshold","title":"<code>node_split_threshold(forest_num, tree_num, node_id)</code>","text":"<p>Threshold that defines a numeric split for a given node of a given tree in a given forest in the <code>ForestContainer</code>. Returns <code>np.Inf</code> if the node is a leaf or a categorical split node.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>float</code> <p>Threshold that defines a numeric split for node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.node_split_categories","title":"<code>node_split_categories(forest_num, tree_num, node_id)</code>","text":"<p>Array of category indices that define a categorical split for a given node of a given tree in a given forest in the <code>ForestContainer</code>. Returns <code>np.array([np.Inf])</code> if the node is a leaf or a numeric split node.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of category indices that define a categorical split for node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.node_leaf_values","title":"<code>node_leaf_values(forest_num, tree_num, node_id)</code>","text":"<p>Node parameter value(s) for a given node of a given tree in a given forest in the <code>ForestContainer</code>. Values are stale if the node is a split node.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <code>node_id</code> <code>int</code> <p>Index of the node to be queried</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of parameter values for node <code>node_id</code> in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.num_samples","title":"<code>num_samples()</code>","text":"<p>Number of forest samples in the <code>ForestContainer</code>.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total number of forest samples.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.num_nodes","title":"<code>num_nodes(forest_num, tree_num)</code>","text":"<p>Number of nodes in a given tree in a given forest in the <code>ForestContainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of nodes in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.num_leaves","title":"<code>num_leaves(forest_num, tree_num)</code>","text":"<p>Number of leaves in a given tree in a given forest in the <code>ForestContainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of leaves in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.num_leaf_parents","title":"<code>num_leaf_parents(forest_num, tree_num)</code>","text":"<p>Number of leaf parents (split nodes with two leaves as children) in a given tree in a given forest in the <code>ForestContainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of leaf parents in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.num_split_nodes","title":"<code>num_split_nodes(forest_num, tree_num)</code>","text":"<p>Number of split_nodes in a given tree in a given forest in the <code>ForestContainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of split nodes in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.nodes","title":"<code>nodes(forest_num, tree_num)</code>","text":"<p>Array of node indices in a given tree in a given forest in the <code>ForestContainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of indices of nodes in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.leaves","title":"<code>leaves(forest_num, tree_num)</code>","text":"<p>Array of leaf indices in a given tree in a given forest in the <code>ForestContainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be queried</p> required <code>tree_num</code> <code>int</code> <p>Index of the tree to be queried</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of indices of leaf nodes in tree <code>tree_num</code> of forest <code>forest_num</code>.</p>"},{"location":"python_docs/api/low-level/forest.html#stochtree.forest.ForestContainer.delete_sample","title":"<code>delete_sample(forest_num)</code>","text":"<p>Modify the <code>ForestContainer</code> by removing the forest sample indexed by <code>forest_num</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forest_num</code> <code>int</code> <p>Index of the forest to be removed from the <code>ForestContainer</code></p> required"},{"location":"python_docs/api/low-level/sampler.html","title":"Sampler API","text":""},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler","title":"<code>stochtree.sampler.ForestSampler</code>","text":"<p>Wrapper around many of the core C++ sampling data structures and algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p><code>stochtree</code> dataset object storing covariates / bases / weights</p> required <code>feature_types</code> <code>array</code> <p>Array of integer-coded values indicating the column type of each feature in <code>dataset</code>. Integer codes map <code>0</code> to \"numeric\" (continuous), <code>1</code> to \"ordered categorical, and <code>2</code> to \"unordered categorical\".</p> required <code>num_trees</code> <code>int</code> <p>Number of trees in the forest model that this sampler class will fit.</p> required <code>num_obs</code> <code>int</code> <p>Number of observations / \"rows\" in <code>dataset</code>.</p> required <code>alpha</code> <code>float</code> <p>Prior probability of splitting for a tree of depth 0 in a forest model. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>.</p> required <code>beta</code> <code>float</code> <p>Exponent that decreases split probabilities for nodes of depth &gt; 0 in a forest model. Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>.</p> required <code>min_samples_leaf</code> <code>int</code> <p>Minimum allowable size of a leaf, in terms of training samples, in a forest model.</p> required <code>max_depth</code> <code>int</code> <p>Maximum depth of any tree in the ensemble in a forest model.</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler.reconstitute_from_forest","title":"<code>reconstitute_from_forest(forest, dataset, residual, is_mean_model)</code>","text":"<p>Re-initialize a forest sampler tracking data structures from a specific forest in a <code>ForestContainer</code></p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p><code>stochtree</code> dataset object storing covariates / bases / weights</p> required <code>residual</code> <code>Residual</code> <p><code>stochtree</code> object storing continuously updated partial / full residual</p> required <code>forest</code> <code>Forest</code> <p><code>stochtree</code> object storing tree ensemble</p> required <code>is_mean_model</code> <code>bool</code> <p>Indicator of whether the model being updated a conditional mean model (<code>True</code>) or a conditional variance model (<code>False</code>)</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler.sample_one_iteration","title":"<code>sample_one_iteration(forest_container, forest, dataset, residual, rng, global_config, forest_config, keep_forest, gfr)</code>","text":"<p>Sample one iteration of a forest using the specified model and tree sampling algorithm</p> <p>Parameters:</p> Name Type Description Default <code>forest_container</code> <code>ForestContainer</code> <p><code>stochtree</code> object storing tree ensembles</p> required <code>forest</code> <code>Forest</code> <p><code>stochtree</code> object storing the \"active\" forest being sampled</p> required <code>dataset</code> <code>Dataset</code> <p><code>stochtree</code> dataset object storing covariates / bases / weights</p> required <code>residual</code> <code>Residual</code> <p><code>stochtree</code> object storing continuously updated partial / full residual</p> required <code>rng</code> <code>RNG</code> <p><code>stochtree</code> object storing C++ random number generator to be used sampling algorithm</p> required <code>global_config</code> <code>GlobalModelConfig</code> <p><code>GlobalModelConfig</code> object containing global model parameters and settings</p> required <code>forest_config</code> <code>ForestModelConfig</code> <p><code>ForestModelConfig</code> object containing forest model parameters and settings</p> required <code>keep_forest</code> <code>bool</code> <p>Whether or not the resulting forest should be retained in <code>forest_container</code> or discarded (due to burnin or thinning for example)</p> required <code>gfr</code> <code>bool</code> <p>Whether or not the \"grow-from-root\" (GFR) sampler is run (if this is <code>True</code> and <code>leaf_model_int=0</code> this is equivalent to XBART, if this is <code>FALSE</code> and <code>leaf_model_int=0</code> this is equivalent to the original BART)</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler.prepare_for_sampler","title":"<code>prepare_for_sampler(dataset, residual, forest, leaf_model, initial_values)</code>","text":"<p>Initialize forest and tracking data structures with constant root values before running a sampler</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p><code>stochtree</code> dataset object storing covariates / bases / weights</p> required <code>residual</code> <code>Residual</code> <p><code>stochtree</code> object storing continuously updated partial / full residual</p> required <code>forest</code> <code>Forest</code> <p><code>stochtree</code> object storing the \"active\" forest being sampled</p> required <code>leaf_model</code> <code>int</code> <p>Integer encoding the leaf model type</p> required <code>initial_values</code> <code>array</code> <p>Constant root node value(s) at which to initialize forest prediction (internally, it is divided by the number of trees and typically it is 0 for mean models and 1 for variance models).</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler.adjust_residual","title":"<code>adjust_residual(dataset, residual, forest, requires_basis, add)</code>","text":"<p>Method that \"adjusts\" the residual used for training tree ensembles by either adding or subtracting the prediction of each tree to the existing residual.</p> <p>This is typically run just once at the beginning of a forest sampling algorithm --- after trees are initialized with constant root node predictions, their root predictions are subtracted out of the residual.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p><code>stochtree</code> dataset object storing covariates / bases / weights</p> required <code>residual</code> <code>Residual</code> <p><code>stochtree</code> object storing continuously updated partial / full residual</p> required <code>forest</code> <code>Forest</code> <p><code>stochtree</code> object storing the \"active\" forest being sampled</p> required <code>requires_basis</code> <code>bool</code> <p>Whether or not the forest requires a basis dot product when predicting</p> required <code>add</code> <code>bool</code> <p>Whether the predictions of each tree are added (if <code>add=True</code>) or subtracted (<code>add=False</code>) from the outcome to form the new residual</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler.propagate_basis_update","title":"<code>propagate_basis_update(dataset, residual, forest)</code>","text":"<p>Propagates basis update through to the (full/partial) residual by iteratively (a) adding back in the previous prediction of each tree, (b) recomputing predictions for each tree (caching on the C++ side), (c) subtracting the new predictions from the residual.</p> <p>This is useful in cases where a basis (for e.g. leaf regression) is updated outside of a tree sampler (as with e.g. adaptive coding for binary treatment BCF). Once a basis has been updated, the overall \"function\" represented by a tree model has changed and this should be reflected through to the residual before the next sampling loop is run.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Stochtree dataset object storing covariates / bases / weights</p> required <code>residual</code> <code>Residual</code> <p>Stochtree object storing continuously updated partial / full residual</p> required <code>forest</code> <code>Forest</code> <p>Stochtree object storing the \"active\" forest being sampled</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler.update_alpha","title":"<code>update_alpha(alpha)</code>","text":"<p>Update <code>alpha</code> in the tree prior</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>New value of <code>alpha</code> to be used</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler.update_beta","title":"<code>update_beta(beta)</code>","text":"<p>Update <code>beta</code> in the tree prior</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>float</code> <p>New value of <code>beta</code> to be used</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler.update_min_samples_leaf","title":"<code>update_min_samples_leaf(min_samples_leaf)</code>","text":"<p>Update <code>min_samples_leaf</code> in the tree prior</p> <p>Parameters:</p> Name Type Description Default <code>min_samples_leaf</code> <code>int</code> <p>New value of <code>min_samples_leaf</code> to be used</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.ForestSampler.update_max_depth","title":"<code>update_max_depth(max_depth)</code>","text":"<p>Update <code>max_depth</code> in the tree prior</p> <p>Parameters:</p> Name Type Description Default <code>max_depth</code> <code>int</code> <p>New value of <code>max_depth</code> to be used</p> required"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.GlobalVarianceModel","title":"<code>stochtree.sampler.GlobalVarianceModel</code>","text":"<p>Wrapper around methods / functions for sampling a \"global\" error variance model with inverse gamma prior.</p>"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.GlobalVarianceModel.sample_one_iteration","title":"<code>sample_one_iteration(residual, rng, a, b)</code>","text":"<p>Sample one iteration of a global error variance parameter</p> <p>Parameters:</p> Name Type Description Default <code>residual</code> <code>Residual</code> <p><code>stochtree</code> object storing continuously updated partial / full residual</p> required <code>rng</code> <code>RNG</code> <p><code>stochtree</code> object storing C++ random number generator to be used sampling algorithm</p> required <code>a</code> <code>float</code> <p>Shape parameter for the inverse gamma error variance model</p> required <code>b</code> <code>float</code> <p>Scale parameter for the inverse gamma error variance model</p> required <p>Returns:</p> Type Description <code>float</code> <p>One draw from a Gibbs sampler for the error variance model, which depends on the rest of the model only through the \"full\" residual stored in a <code>Residual</code> object (net of predictions of any mean term such as a forest or an additive parametric fixed / random effect term).</p>"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.LeafVarianceModel","title":"<code>stochtree.sampler.LeafVarianceModel</code>","text":"<p>Wrapper around methods / functions for sampling a \"leaf scale\" model for the variance term of a Gaussian leaf model with inverse gamma prior.</p>"},{"location":"python_docs/api/low-level/sampler.html#stochtree.sampler.LeafVarianceModel.sample_one_iteration","title":"<code>sample_one_iteration(forest, rng, a, b)</code>","text":"<p>Sample one iteration of a forest leaf model's variance parameter (assuming a location-scale leaf model, most commonly <code>N(0, tau)</code>)</p> <p>Parameters:</p> Name Type Description Default <code>forest</code> <code>Forest</code> <p><code>stochtree</code> object storing the \"active\" forest being sampled</p> required <code>rng</code> <code>RNG</code> <p><code>stochtree</code> object storing C++ random number generator to be used sampling algorithm</p> required <code>a</code> <code>float</code> <p>Shape parameter for the inverse gamma leaf scale model</p> required <code>b</code> <code>float</code> <p>Scale parameter for the inverse gamma leaf scale model</p> required <p>Returns:</p> Type Description <code>float</code> <p>One draw from a Gibbs sampler for the leaf scale model, which depends on the rest of the model only through its respective forest.</p>"},{"location":"python_docs/api/low-level/utilities.html","title":"Utilies API","text":""},{"location":"python_docs/api/low-level/utilities.html#stochtree.sampler.RNG","title":"<code>stochtree.sampler.RNG</code>","text":"<p>Wrapper around the C++ standard library random number generator. Accepts an optional random seed at initialization for replicability.</p> <p>Parameters:</p> Name Type Description Default <code>random_seed</code> <code>int</code> <p>Random seed for replicability. If not specified, the default value of <code>-1</code> triggers an initialization of the RNG based on std::random_device.</p> <code>-1</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.preprocessing.CovariatePreprocessor","title":"<code>stochtree.preprocessing.CovariatePreprocessor</code>","text":"<p>Preprocessing engine for covariates provided as either <code>np.array</code> or <code>pd.DataFrame</code>, which standardizes inputs as a <code>np.array</code>.</p> <p><code>CovariatePreprocessor</code> uses column dtypes in provided dataframes to convert string / categorical variables to numeric variables, either by mapping ordinal variables to integers or by one-hot encoding unordered categorical variables.</p> <p>This class is modeled after the scikit-learn preprocessing classes.</p>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.preprocessing.CovariatePreprocessor.fit","title":"<code>fit(covariates)</code>","text":"<p>Fits a <code>CovariatePreprocessor</code> by unpacking (and storing) data type information on the input (raw) covariates and then converting to a numpy array which can be passed to a tree ensemble sampler.</p> <p>If <code>covariates</code> is a <code>pd.DataFrame</code>, column dtypes will be handled as follows:</p> <ul> <li><code>category</code>: one-hot encoded if unordered, ordinal encoded if ordered</li> <li><code>string</code>: one-hot encoded</li> <li><code>boolean</code>: passed through as binary integer, treated as ordered categorical by tree samplers</li> <li>integer (i.e. <code>Int8</code>, <code>Int16</code>, etc...): passed through as double (note: if you have categorical data stored as integers, you should explicitly convert it to categorical in pandas, see this user guide)</li> <li>float (i.e. <code>Float32</code>, <code>Float64</code>): passed through as double</li> <li><code>object</code>: currently unsupported, convert object columns to numeric or categorical before passing</li> <li>Datetime (i.e. <code>datetime64</code>): currently unsupported, though datetime columns can be converted to numeric features, see here</li> <li>Period (i.e. <code>period[&lt;freq&gt;]</code>): currently unsupported, though period columns can be converted to numeric features, see here</li> <li>Interval (i.e. <code>interval</code>, <code>Interval[datetime64[ns]]</code>): currently unsupported, though interval columns can be converted to numeric or categorical features, see here</li> <li>Sparse (i.e. <code>Sparse</code>, <code>Sparse[float]</code>): currently unsupported, convert sparse columns to dense before passing</li> </ul> <p>Columns with unsupported types will be ignored, with a warning.</p> <p>If <code>covariates</code> is a <code>np.array</code>, columns must be numeric and the only preprocessing done by <code>CovariateTransformer.fit()</code> is to auto-detect binary columns. All other integer-valued columns will be passed through to the tree sampler as (continuous) numeric data. If you would like to treat integer-valued data as categorical, you can either convert your numpy array to a pandas dataframe and explicitly tag such columns as ordered / unordered categorical, or preprocess manually using <code>sklearn.preprocessing.OneHotEncoder</code> and <code>sklearn.preprocessing.OrdinalEncoder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array or DataFrame</code> <p>Covariates to be preprocessed.</p> required"},{"location":"python_docs/api/low-level/utilities.html#stochtree.preprocessing.CovariatePreprocessor.transform","title":"<code>transform(covariates)</code>","text":"<p>Run a fitted a <code>CovariateTransformer</code> on a new covariate set, returning a numpy array of covariates preprocessed into a format needed to sample or predict from a <code>stochtree</code> ensemble.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array or DataFrame</code> <p>Covariates to be preprocessed.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array of preprocessed covariates, with as many rows as in <code>covariates</code> and as many columns as were created during pre-processing (including one-hot encoding categorical features).</p>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.preprocessing.CovariatePreprocessor.fit_transform","title":"<code>fit_transform(covariates)</code>","text":"<p>Runs the <code>fit()</code> and <code>transform()</code> methods in sequence.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array or DataFrame</code> <p>Covariates to be preprocessed.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Numpy array of preprocessed covariates, with as many rows as in <code>covariates</code> and as many columns as were created during pre-processing (including one-hot encoding categorical features).</p>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.preprocessing.CovariatePreprocessor.fetch_original_feature_indices","title":"<code>fetch_original_feature_indices()</code>","text":"<p>Map features in a preprocessed covariate set back to the original set of features provided to a <code>CovariateTransformer</code>.</p> <p>Returns:</p> Type Description <code>list</code> <p>List with as many entries as features in the preprocessed results returned by a fitted <code>CovariateTransformer</code>. Each element is a feature index indicating the feature from which a given preprocessed feature was generated. If a single categorical feature were one-hot encoded into 5 binary features, this method would return a list <code>[0,0,0,0,0]</code>. If the transformer merely passes through <code>k</code> numeric features, this method would return a list <code>[0,...,k-1]</code>.</p>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.preprocessing.CovariatePreprocessor.to_json","title":"<code>to_json()</code>","text":"<p>Converts a covariate preprocessor to JSON string representation (which can then be saved to a file or processed using the <code>json</code> library)</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.preprocessing.CovariatePreprocessor.from_json","title":"<code>from_json(json_string)</code>","text":"<p>Converts a JSON string to an in-memory BART model.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p> required"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer","title":"<code>stochtree.serialization.JSONSerializer</code>","text":"<p>Class that handles serialization and deserialization of stochastic forest models</p>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.return_json_string","title":"<code>return_json_string()</code>","text":"<p>Convert JSON object to in-memory string</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.load_from_json_string","title":"<code>load_from_json_string(json_string)</code>","text":"<p>Parse in-memory JSON string to <code>JsonCpp</code> object</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p> required"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.add_forest","title":"<code>add_forest(forest_samples)</code>","text":"<p>Adds a container of forest samples to a json object</p> <p>Parameters:</p> Name Type Description Default <code>forest_samples</code> <code>ForestContainer</code> <p>Samples of a tree ensemble</p> required"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.add_random_effects","title":"<code>add_random_effects(rfx_container)</code>","text":"<p>Adds a container of random effect samples to a json object</p> <p>Parameters:</p> Name Type Description Default <code>rfx_container</code> <code>RandomEffectsContainer</code> <p>Samples of a random effects model</p> required"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.add_scalar","title":"<code>add_scalar(field_name, field_value, subfolder_name=None)</code>","text":"<p>Adds a scalar (numeric) value to a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the numeric value will be stored</p> required <code>field_value</code> <code>float</code> <p>Numeric value to be stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.add_integer","title":"<code>add_integer(field_name, field_value, subfolder_name=None)</code>","text":"<p>Adds an integer value to a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the numeric value will be stored</p> required <code>field_value</code> <code>int</code> <p>Integer value to be stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.add_boolean","title":"<code>add_boolean(field_name, field_value, subfolder_name=None)</code>","text":"<p>Adds a scalar (boolean) value to a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the boolean value will be stored</p> required <code>field_value</code> <code>bool</code> <p>Boolean value to be stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.add_string","title":"<code>add_string(field_name, field_value, subfolder_name=None)</code>","text":"<p>Adds a string to a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the numeric value will be stored</p> required <code>field_value</code> <code>str</code> <p>String field to be stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.add_numeric_vector","title":"<code>add_numeric_vector(field_name, field_vector, subfolder_name=None)</code>","text":"<p>Adds a numeric vector (stored as a numpy array) to a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the numeric vector will be stored</p> required <code>field_vector</code> <code>array</code> <p>Numpy array containing the vector to be stored in json. Should be one-dimensional.</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.add_integer_vector","title":"<code>add_integer_vector(field_name, field_vector, subfolder_name=None)</code>","text":"<p>Adds a integer vector (stored as a numpy array) to a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the integer vector will be stored</p> required <code>field_vector</code> <code>array</code> <p>Numpy array containing the vector to be stored in json. Should be one-dimensional.</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.add_string_vector","title":"<code>add_string_vector(field_name, field_vector, subfolder_name=None)</code>","text":"<p>Adds a list of strings to a json object as an array</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the string list will be stored</p> required <code>field_vector</code> <code>list</code> <p>Python list of strings containing the array to be stored in json</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.get_scalar","title":"<code>get_scalar(field_name, subfolder_name=None)</code>","text":"<p>Retrieves a scalar (numeric) value from a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the numeric value is stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> is stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.get_integer","title":"<code>get_integer(field_name, subfolder_name=None)</code>","text":"<p>Retrieves an integer value from a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the numeric value is stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> is stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.get_boolean","title":"<code>get_boolean(field_name, subfolder_name=None)</code>","text":"<p>Retrieves a scalar (boolean) value from a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the boolean value is stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> is stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.get_string","title":"<code>get_string(field_name, subfolder_name=None)</code>","text":"<p>Retrieve a string from a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the string is stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> is stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.get_numeric_vector","title":"<code>get_numeric_vector(field_name, subfolder_name=None)</code>","text":"<p>Retrieve numeric vector from a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the numeric vector is stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.get_integer_vector","title":"<code>get_integer_vector(field_name, subfolder_name=None)</code>","text":"<p>Retrieve integer vector from a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the integer vector is stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.get_string_vector","title":"<code>get_string_vector(field_name, subfolder_name=None)</code>","text":"<p>Adds a string to a json object</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the json field / label under which the string list is stored</p> required <code>subfolder_name</code> <code>str</code> <p>Name of \"subfolder\" under which <code>field_name</code> to be stored in the json hierarchy</p> <code>None</code>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.get_forest_container","title":"<code>get_forest_container(forest_str)</code>","text":"<p>Converts a JSON string for a container of forests to a <code>ForestContainer</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>forest_str</code> <code>str</code> <p>String containing the label for a given forest in a JSON object</p> required <p>Returns:</p> Type Description <code>ForestContainer</code> <p>In-memory <code>ForestContainer</code> python object, created from JSON</p>"},{"location":"python_docs/api/low-level/utilities.html#stochtree.serialization.JSONSerializer.get_random_effects_container","title":"<code>get_random_effects_container(random_effects_str)</code>","text":"<p>Converts a JSON string for a random effects container to a <code>RandomEffectsContainer</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>random_effects_str</code> <code>str</code> <p>String containing the label for a given random effects term in a JSON object</p> required <p>Returns:</p> Type Description <code>RandomEffectsContainer</code> <p>In-memory <code>RandomEffectsContainer</code> python object, created from JSON</p>"},{"location":"python_docs/demo/index.html","title":"StochTree Python API Demo","text":"<p>The following demos showcase (some of) the functionality and output of the <code>stochtree</code> python package. </p> <ol> <li>Supervised Learning: using <code>BARTModel()</code> for classic supervised learning tasks</li> <li>Causal Inference: using <code>BCFModel()</code> for causal effect estimation</li> <li>Heteroskedastic Supervised Learning: using <code>BARTModel()</code> for supervised learning tasks with heteroskedasticity (covariate-dependent variance)</li> <li>Multivariate Treatment Causal Inference: using <code>BCFModel()</code> for causal effect estimation with a multivariate (continuous) treatment variable</li> <li>Model Serialization: saving and reloading <code>stochtree</code> models via JSON</li> <li>Internal Tree Inspection: inspecting the trees in a sampled <code>stochtree</code> forest</li> <li>Low-Level Interface: using the low-level <code>stochtree</code> interface to construct a custom sampling loop</li> </ol>"},{"location":"python_docs/demo/causal_inference.html","title":"Causal Inference","text":"<p>Load necessary libraries</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom stochtree import BCFModel\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.model_selection import train_test_split  from stochtree import BCFModel <p>Generate sample data</p> In\u00a0[2]: Copied! <pre># RNG\nrng = np.random.default_rng()\n\n# Generate covariates and basis\nn = 1000\np_X = 5\nX = rng.uniform(0, 1, (n, p_X))\npi_X = 0.25 + 0.5 * X[:, 0]\nZ = rng.binomial(1, pi_X, n).astype(float)\n\n# Define the outcome mean functions (prognostic and treatment effects)\nmu_X = pi_X * 5 + 2 * X[:, 2]\ntau_X = X[:, 1] * 2 - 1\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\ny = mu_X + tau_X * Z + epsilon\n</pre> # RNG rng = np.random.default_rng()  # Generate covariates and basis n = 1000 p_X = 5 X = rng.uniform(0, 1, (n, p_X)) pi_X = 0.25 + 0.5 * X[:, 0] Z = rng.binomial(1, pi_X, n).astype(float)  # Define the outcome mean functions (prognostic and treatment effects) mu_X = pi_X * 5 + 2 * X[:, 2] tau_X = X[:, 1] * 2 - 1  # Generate outcome epsilon = rng.normal(0, 1, n) y = mu_X + tau_X * Z + epsilon <p>Test-train split</p> In\u00a0[3]: Copied! <pre>sample_inds = np.arange(n)\ntrain_inds, test_inds = train_test_split(sample_inds, test_size=0.5)\nX_train = X[train_inds, :]\nX_test = X[test_inds, :]\nZ_train = Z[train_inds]\nZ_test = Z[test_inds]\ny_train = y[train_inds]\ny_test = y[test_inds]\npi_train = pi_X[train_inds]\npi_test = pi_X[test_inds]\nmu_train = mu_X[train_inds]\nmu_test = mu_X[test_inds]\ntau_train = tau_X[train_inds]\ntau_test = tau_X[test_inds]\n</pre> sample_inds = np.arange(n) train_inds, test_inds = train_test_split(sample_inds, test_size=0.5) X_train = X[train_inds, :] X_test = X[test_inds, :] Z_train = Z[train_inds] Z_test = Z[test_inds] y_train = y[train_inds] y_test = y[test_inds] pi_train = pi_X[train_inds] pi_test = pi_X[test_inds] mu_train = mu_X[train_inds] mu_test = mu_X[test_inds] tau_train = tau_X[train_inds] tau_test = tau_X[test_inds] <p>Run BCF</p> In\u00a0[4]: Copied! <pre>bcf_model = BCFModel()\ngeneral_params = {\"keep_every\": 5}\nbcf_model.sample(\n    X_train=X_train,\n    Z_train=Z_train,\n    y_train=y_train,\n    pi_train=pi_train,\n    X_test=X_test,\n    Z_test=Z_test,\n    pi_test=pi_test,\n    num_gfr=10,\n    num_mcmc=100,\n    general_params=general_params,\n)\n</pre> bcf_model = BCFModel() general_params = {\"keep_every\": 5} bcf_model.sample(     X_train=X_train,     Z_train=Z_train,     y_train=y_train,     pi_train=pi_train,     X_test=X_test,     Z_test=Z_test,     pi_test=pi_test,     num_gfr=10,     num_mcmc=100,     general_params=general_params, ) <p>Inspect the MCMC (BART) samples</p> In\u00a0[5]: Copied! <pre>forest_preds_y_mcmc = bcf_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True)\ny_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),\n    columns=[\"True outcome\", \"Average estimated outcome\"],\n)\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bcf_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True) y_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),     columns=[\"True outcome\", \"Average estimated outcome\"], ) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[6]: Copied! <pre>forest_preds_tau_mcmc = bcf_model.tau_hat_test\ntau_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis=1, keepdims=True)\ntau_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(tau_test, 1), tau_avg_mcmc), axis=1),\n    columns=[\"True tau\", \"Average estimated tau\"],\n)\nsns.scatterplot(data=tau_df_mcmc, x=\"True tau\", y=\"Average estimated tau\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_tau_mcmc = bcf_model.tau_hat_test tau_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis=1, keepdims=True) tau_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(tau_test, 1), tau_avg_mcmc), axis=1),     columns=[\"True tau\", \"Average estimated tau\"], ) sns.scatterplot(data=tau_df_mcmc, x=\"True tau\", y=\"Average estimated tau\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[7]: Copied! <pre>forest_preds_mu_mcmc = bcf_model.mu_hat_test\nmu_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis=1, keepdims=True)\nmu_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(mu_test, 1), mu_avg_mcmc), axis=1),\n    columns=[\"True mu\", \"Average estimated mu\"],\n)\nsns.scatterplot(data=mu_df_mcmc, x=\"True mu\", y=\"Average estimated mu\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_mu_mcmc = bcf_model.mu_hat_test mu_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis=1, keepdims=True) mu_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(mu_test, 1), mu_avg_mcmc), axis=1),     columns=[\"True mu\", \"Average estimated mu\"], ) sns.scatterplot(data=mu_df_mcmc, x=\"True mu\", y=\"Average estimated mu\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[8]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(bcf_model.num_samples), axis=1),\n            np.expand_dims(bcf_model.global_var_samples, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(bcf_model.num_samples), axis=1),             np.expand_dims(bcf_model.global_var_samples, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\") plt.show() In\u00a0[9]: Copied! <pre>b_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(bcf_model.num_samples), axis=1),\n            np.expand_dims(bcf_model.b0_samples, axis=1),\n            np.expand_dims(bcf_model.b1_samples, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Beta_0\", \"Beta_1\"],\n)\nsns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_0\")\nsns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_1\")\nplt.show()\n</pre> b_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(bcf_model.num_samples), axis=1),             np.expand_dims(bcf_model.b0_samples, axis=1),             np.expand_dims(bcf_model.b1_samples, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Beta_0\", \"Beta_1\"], ) sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_0\") sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_1\") plt.show()"},{"location":"python_docs/demo/causal_inference.html#causal-inference","title":"Causal Inference\u00b6","text":""},{"location":"python_docs/demo/heteroskedastic_supervised_learning.html","title":"Heteroskedastic Supervised Learning","text":"<p>Load necessary libraries</p> In\u00a0[1]: Copied! <pre>from math import sqrt\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom stochtree import BARTModel\n</pre> from math import sqrt  import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.model_selection import train_test_split  from stochtree import BARTModel <p>Generate sample data</p> In\u00a0[2]: Copied! <pre># RNG\nrandom_seed = 1234\nrng = np.random.default_rng(random_seed)\n\n# Generate covariates and basis\nn = 1000\np_X = 10\np_W = 1\nX = rng.uniform(0, 1, (n, p_X))\nW = rng.uniform(0, 1, (n, p_W))\n\n\n# Define the outcome mean function\ndef outcome_mean(X, W):\n    return np.where(\n        (X[:, 0] &gt;= 0.0) &amp; (X[:, 0] &lt; 0.25),\n        -7.5 * W[:, 0],\n        np.where(\n            (X[:, 0] &gt;= 0.25) &amp; (X[:, 0] &lt; 0.5),\n            -2.5 * W[:, 0],\n            np.where((X[:, 0] &gt;= 0.5) &amp; (X[:, 0] &lt; 0.75), 2.5 * W[:, 0], 7.5 * W[:, 0]),\n        ),\n    )\n\n\n# Define the outcome standard deviation function\ndef outcome_stddev(X):\n    return np.where(\n        (X[:, 1] &gt;= 0.0) &amp; (X[:, 1] &lt; 0.25),\n        sqrt(0.5),\n        np.where(\n            (X[:, 1] &gt;= 0.25) &amp; (X[:, 1] &lt; 0.5),\n            1.0,\n            np.where((X[:, 1] &gt;= 0.5) &amp; (X[:, 1] &lt; 0.75), 2.0, 3.0),\n        ),\n    )\n\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\nf_x = outcome_mean(X, W)\ns_x = outcome_stddev(X)\ny = f_x + epsilon * s_x\n\n# Standardize outcome\ny_bar = np.mean(y)\ny_std = np.std(y)\nresid = (y - y_bar) / y_std\n</pre> # RNG random_seed = 1234 rng = np.random.default_rng(random_seed)  # Generate covariates and basis n = 1000 p_X = 10 p_W = 1 X = rng.uniform(0, 1, (n, p_X)) W = rng.uniform(0, 1, (n, p_W))   # Define the outcome mean function def outcome_mean(X, W):     return np.where(         (X[:, 0] &gt;= 0.0) &amp; (X[:, 0] &lt; 0.25),         -7.5 * W[:, 0],         np.where(             (X[:, 0] &gt;= 0.25) &amp; (X[:, 0] &lt; 0.5),             -2.5 * W[:, 0],             np.where((X[:, 0] &gt;= 0.5) &amp; (X[:, 0] &lt; 0.75), 2.5 * W[:, 0], 7.5 * W[:, 0]),         ),     )   # Define the outcome standard deviation function def outcome_stddev(X):     return np.where(         (X[:, 1] &gt;= 0.0) &amp; (X[:, 1] &lt; 0.25),         sqrt(0.5),         np.where(             (X[:, 1] &gt;= 0.25) &amp; (X[:, 1] &lt; 0.5),             1.0,             np.where((X[:, 1] &gt;= 0.5) &amp; (X[:, 1] &lt; 0.75), 2.0, 3.0),         ),     )   # Generate outcome epsilon = rng.normal(0, 1, n) f_x = outcome_mean(X, W) s_x = outcome_stddev(X) y = f_x + epsilon * s_x  # Standardize outcome y_bar = np.mean(y) y_std = np.std(y) resid = (y - y_bar) / y_std <p>Test-train split</p> In\u00a0[3]: Copied! <pre>sample_inds = np.arange(n)\ntrain_inds, test_inds = train_test_split(sample_inds, test_size=0.5)\nX_train = X[train_inds, :]\nX_test = X[test_inds, :]\nbasis_train = W[train_inds, :]\nbasis_test = W[test_inds, :]\ny_train = y[train_inds]\ny_test = y[test_inds]\nf_x_train = f_x[train_inds]\nf_x_test = f_x[test_inds]\ns_x_train = s_x[train_inds]\ns_x_test = s_x[test_inds]\n</pre> sample_inds = np.arange(n) train_inds, test_inds = train_test_split(sample_inds, test_size=0.5) X_train = X[train_inds, :] X_test = X[test_inds, :] basis_train = W[train_inds, :] basis_test = W[test_inds, :] y_train = y[train_inds] y_test = y[test_inds] f_x_train = f_x[train_inds] f_x_test = f_x[test_inds] s_x_train = s_x[train_inds] s_x_test = s_x[test_inds] <p>Run BART</p> In\u00a0[4]: Copied! <pre>bart_model = BARTModel()\nglobal_params = {\"sample_sigma2_global\": True}\nmean_params = {\"num_trees\": 100, \"sample_sigma2_leaf\": False}\nvariance_params = {\"num_trees\": 50}\nbart_model.sample(\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    leaf_basis_train=basis_train,\n    leaf_basis_test=basis_test,\n    num_gfr=10,\n    num_mcmc=100,\n    general_params=global_params,\n    mean_forest_params=mean_params,\n    variance_forest_params=variance_params,\n)\n</pre> bart_model = BARTModel() global_params = {\"sample_sigma2_global\": True} mean_params = {\"num_trees\": 100, \"sample_sigma2_leaf\": False} variance_params = {\"num_trees\": 50} bart_model.sample(     X_train=X_train,     y_train=y_train,     X_test=X_test,     leaf_basis_train=basis_train,     leaf_basis_test=basis_test,     num_gfr=10,     num_mcmc=100,     general_params=global_params,     mean_forest_params=mean_params,     variance_forest_params=variance_params, ) <p>Inspect the MCMC (BART) samples</p> In\u00a0[5]: Copied! <pre>forest_preds_y_mcmc = bart_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True)\ny_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),\n    columns=[\"True outcome\", \"Average estimated outcome\"],\n)\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bart_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True) y_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),     columns=[\"True outcome\", \"Average estimated outcome\"], ) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[6]: Copied! <pre>forest_preds_s_x_mcmc = np.sqrt(bart_model.sigma2_x_test)\ns_x_avg_mcmc = np.squeeze(forest_preds_s_x_mcmc).mean(axis=1, keepdims=True)\ns_x_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(s_x_test, 1), s_x_avg_mcmc), axis=1),\n    columns=[\"True standard deviation\", \"Average estimated standard deviation\"],\n)\nsns.scatterplot(\n    data=s_x_df_mcmc,\n    x=\"Average estimated standard deviation\",\n    y=\"True standard deviation\",\n)\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_s_x_mcmc = np.sqrt(bart_model.sigma2_x_test) s_x_avg_mcmc = np.squeeze(forest_preds_s_x_mcmc).mean(axis=1, keepdims=True) s_x_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(s_x_test, 1), s_x_avg_mcmc), axis=1),     columns=[\"True standard deviation\", \"Average estimated standard deviation\"], ) sns.scatterplot(     data=s_x_df_mcmc,     x=\"Average estimated standard deviation\",     y=\"True standard deviation\", ) plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[7]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(bart_model.global_var_samples.shape[0]), axis=1),\n            np.expand_dims(bart_model.global_var_samples, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(bart_model.global_var_samples.shape[0]), axis=1),             np.expand_dims(bart_model.global_var_samples, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\") plt.show() <p>Compute the test set RMSE</p> In\u00a0[8]: Copied! <pre>np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2)))\n</pre> np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2))) Out[8]: <pre>np.float64(1.9426155144388664)</pre>"},{"location":"python_docs/demo/heteroskedastic_supervised_learning.html#heteroskedastic-supervised-learning","title":"Heteroskedastic Supervised Learning\u00b6","text":""},{"location":"python_docs/demo/multivariate_treatment_causal_inference.html","title":"Multivariate Treatment Causal Inference","text":"<p>Load necessary libraries</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom stochtree import BCFModel\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.model_selection import train_test_split  from stochtree import BCFModel <p>Generate sample data</p> In\u00a0[2]: Copied! <pre># RNG\nrng = np.random.default_rng()\n\n# Generate covariates and basis\nn = 500\np_X = 5\nX = rng.uniform(0, 1, (n, p_X))\npi_X = np.c_[0.25 + 0.5 * X[:, 0], 0.75 - 0.5 * X[:, 1]]\n# Z = rng.uniform(0, 1, (n, 2))\nZ = rng.binomial(1, pi_X, (n, 2))\n\n# Define the outcome mean functions (prognostic and treatment effects)\nmu_X = pi_X[:, 0] * 5 + pi_X[:, 1] * 2 + 2 * X[:, 2]\ntau_X = np.stack((X[:, 1], X[:, 2]), axis=-1)\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\ntreatment_term = np.multiply(tau_X, Z).sum(axis=1)\ny = mu_X + treatment_term + epsilon\n</pre> # RNG rng = np.random.default_rng()  # Generate covariates and basis n = 500 p_X = 5 X = rng.uniform(0, 1, (n, p_X)) pi_X = np.c_[0.25 + 0.5 * X[:, 0], 0.75 - 0.5 * X[:, 1]] # Z = rng.uniform(0, 1, (n, 2)) Z = rng.binomial(1, pi_X, (n, 2))  # Define the outcome mean functions (prognostic and treatment effects) mu_X = pi_X[:, 0] * 5 + pi_X[:, 1] * 2 + 2 * X[:, 2] tau_X = np.stack((X[:, 1], X[:, 2]), axis=-1)  # Generate outcome epsilon = rng.normal(0, 1, n) treatment_term = np.multiply(tau_X, Z).sum(axis=1) y = mu_X + treatment_term + epsilon <p>Test-train split</p> In\u00a0[3]: Copied! <pre>sample_inds = np.arange(n)\ntrain_inds, test_inds = train_test_split(sample_inds, test_size=0.5)\nX_train = X[train_inds, :]\nX_test = X[test_inds, :]\nZ_train = Z[train_inds, :]\nZ_test = Z[test_inds, :]\ny_train = y[train_inds]\ny_test = y[test_inds]\npi_train = pi_X[train_inds]\npi_test = pi_X[test_inds]\nmu_train = mu_X[train_inds]\nmu_test = mu_X[test_inds]\ntau_train = tau_X[train_inds, :]\ntau_test = tau_X[test_inds, :]\n</pre> sample_inds = np.arange(n) train_inds, test_inds = train_test_split(sample_inds, test_size=0.5) X_train = X[train_inds, :] X_test = X[test_inds, :] Z_train = Z[train_inds, :] Z_test = Z[test_inds, :] y_train = y[train_inds] y_test = y[test_inds] pi_train = pi_X[train_inds] pi_test = pi_X[test_inds] mu_train = mu_X[train_inds] mu_test = mu_X[test_inds] tau_train = tau_X[train_inds, :] tau_test = tau_X[test_inds, :] <p>Run BCF</p> In\u00a0[4]: Copied! <pre>bcf_model = BCFModel()\nbcf_model.sample(\n    X_train=X_train,\n    Z_train=Z_train,\n    y_train=y_train,\n    pi_train=pi_train,\n    X_test=X_test,\n    Z_test=Z_test,\n    pi_test=pi_test,\n    num_gfr=10,\n    num_mcmc=100,\n)\n</pre> bcf_model = BCFModel() bcf_model.sample(     X_train=X_train,     Z_train=Z_train,     y_train=y_train,     pi_train=pi_train,     X_test=X_test,     Z_test=Z_test,     pi_test=pi_test,     num_gfr=10,     num_mcmc=100, ) <p>Inspect the MCMC (BART) samples</p> In\u00a0[5]: Copied! <pre>forest_preds_y_mcmc = bcf_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True)\ny_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),\n    columns=[\"True outcome\", \"Average estimated outcome\"],\n)\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bcf_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True) y_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),     columns=[\"True outcome\", \"Average estimated outcome\"], ) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[6]: Copied! <pre>np.sqrt(np.mean(np.power(y_avg_mcmc - y_test, 2)))\n</pre> np.sqrt(np.mean(np.power(y_avg_mcmc - y_test, 2))) Out[6]: <pre>np.float64(1.9315260896366326)</pre> In\u00a0[7]: Copied! <pre>treatment_idx = 0\nforest_preds_tau_mcmc = np.squeeze(bcf_model.tau_hat_test[:, :, treatment_idx])\ntau_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis=1, keepdims=True)\ntau_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (np.expand_dims(tau_test[:, treatment_idx], 1), tau_avg_mcmc), axis=1\n    ),\n    columns=[\"True tau\", \"Average estimated tau\"],\n)\nsns.scatterplot(data=tau_df_mcmc, x=\"True tau\", y=\"Average estimated tau\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> treatment_idx = 0 forest_preds_tau_mcmc = np.squeeze(bcf_model.tau_hat_test[:, :, treatment_idx]) tau_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis=1, keepdims=True) tau_df_mcmc = pd.DataFrame(     np.concatenate(         (np.expand_dims(tau_test[:, treatment_idx], 1), tau_avg_mcmc), axis=1     ),     columns=[\"True tau\", \"Average estimated tau\"], ) sns.scatterplot(data=tau_df_mcmc, x=\"True tau\", y=\"Average estimated tau\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[8]: Copied! <pre>treatment_idx = 1\nforest_preds_tau_mcmc = np.squeeze(bcf_model.tau_hat_test[:, :, treatment_idx])\ntau_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis=1, keepdims=True)\ntau_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (np.expand_dims(tau_test[:, treatment_idx], 1), tau_avg_mcmc), axis=1\n    ),\n    columns=[\"True tau\", \"Average estimated tau\"],\n)\nsns.scatterplot(data=tau_df_mcmc, x=\"True tau\", y=\"Average estimated tau\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> treatment_idx = 1 forest_preds_tau_mcmc = np.squeeze(bcf_model.tau_hat_test[:, :, treatment_idx]) tau_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis=1, keepdims=True) tau_df_mcmc = pd.DataFrame(     np.concatenate(         (np.expand_dims(tau_test[:, treatment_idx], 1), tau_avg_mcmc), axis=1     ),     columns=[\"True tau\", \"Average estimated tau\"], ) sns.scatterplot(data=tau_df_mcmc, x=\"True tau\", y=\"Average estimated tau\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[9]: Copied! <pre>treatment_term_mcmc_test = np.multiply(\n    np.atleast_3d(Z_test).swapaxes(1, 2), bcf_model.tau_hat_test\n).sum(axis=2)\ntreatment_term_test = np.multiply(tau_test, Z_test).sum(axis=1)\ntreatment_term_mcmc_avg = np.squeeze(treatment_term_mcmc_test).mean(\n    axis=1, keepdims=True\n)\nmu_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (np.expand_dims(treatment_term_test, 1), treatment_term_mcmc_avg), axis=1\n    ),\n    columns=[\"True treatment term\", \"Average estimated treatment term\"],\n)\nsns.scatterplot(\n    data=mu_df_mcmc, x=\"True treatment term\", y=\"Average estimated treatment term\"\n)\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> treatment_term_mcmc_test = np.multiply(     np.atleast_3d(Z_test).swapaxes(1, 2), bcf_model.tau_hat_test ).sum(axis=2) treatment_term_test = np.multiply(tau_test, Z_test).sum(axis=1) treatment_term_mcmc_avg = np.squeeze(treatment_term_mcmc_test).mean(     axis=1, keepdims=True ) mu_df_mcmc = pd.DataFrame(     np.concatenate(         (np.expand_dims(treatment_term_test, 1), treatment_term_mcmc_avg), axis=1     ),     columns=[\"True treatment term\", \"Average estimated treatment term\"], ) sns.scatterplot(     data=mu_df_mcmc, x=\"True treatment term\", y=\"Average estimated treatment term\" ) plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[10]: Copied! <pre>forest_preds_mu_mcmc = bcf_model.mu_hat_test\nmu_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis=1, keepdims=True)\nmu_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(mu_test, 1), mu_avg_mcmc), axis=1),\n    columns=[\"True mu\", \"Average estimated mu\"],\n)\nsns.scatterplot(data=mu_df_mcmc, x=\"True mu\", y=\"Average estimated mu\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_mu_mcmc = bcf_model.mu_hat_test mu_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis=1, keepdims=True) mu_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(mu_test, 1), mu_avg_mcmc), axis=1),     columns=[\"True mu\", \"Average estimated mu\"], ) sns.scatterplot(data=mu_df_mcmc, x=\"True mu\", y=\"Average estimated mu\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[11]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(\n                np.arange(bcf_model.num_samples - bcf_model.num_gfr), axis=1\n            ),\n            np.expand_dims(bcf_model.global_var_samples[bcf_model.num_gfr :], axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(                 np.arange(bcf_model.num_samples - bcf_model.num_gfr), axis=1             ),             np.expand_dims(bcf_model.global_var_samples[bcf_model.num_gfr :], axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\") plt.show()"},{"location":"python_docs/demo/multivariate_treatment_causal_inference.html#multivariate-treatment-causal-inference","title":"Multivariate Treatment Causal Inference\u00b6","text":""},{"location":"python_docs/demo/prototype_interface.html","title":"Low-Level Interface","text":"<p>While the functions <code>bart()</code> and <code>bcf()</code> provide simple and performant interfaces for supervised learning / causal inference, <code>stochtree</code> also offers access to many of the \"low-level\" data structures that are typically implemented in C++. This low-level interface is not designed for performance or even simplicity --- rather the intent is to provide a \"prototype\" interface to the C++ code that doesn't require modifying any C++.</p> <p>To illustrate when such a prototype interface might be useful, consider that that \"classic\" BART algorithm is essentially a Metropolis-within-Gibbs sampler, in which the forest is sampled by MCMC, conditional on all of the other model parameters, and then the model parameters are updated by Gibbs.</p> <p>While the algorithm itself is conceptually simple, much of the core computation is carried out in low-level languages such as C or C++ because of the tree data structures. As a result, any changes to this algorithm, such as supporting heteroskedasticity and categorical outcomes (Murray 2021) or causal effect estimation (Hahn et al 2020) require modifying low-level code.</p> <p>The prototype interface exposes the core components of the loop above at the R level, thus making it possible to interchange C++ computation for steps like \"update forest via Metropolis-Hastings\" with R computation for a custom variance model, other user-specified additive mean model components, and so on.</p> <p>Load necessary libraries</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom stochtree import (\n    RNG,\n    Dataset,\n    Forest,\n    ForestContainer,\n    ForestSampler,\n    GlobalVarianceModel,\n    LeafVarianceModel,\n    Residual, \n    ForestModelConfig, \n    GlobalModelConfig,\n)\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns  from stochtree import (     RNG,     Dataset,     Forest,     ForestContainer,     ForestSampler,     GlobalVarianceModel,     LeafVarianceModel,     Residual,      ForestModelConfig,      GlobalModelConfig, ) <p>Generate sample data</p> In\u00a0[2]: Copied! <pre># RNG\nrandom_seed = 1234\nrng = np.random.default_rng(random_seed)\n\n# Generate covariates and basis\nn = 500\np_X = 10\np_W = 1\nX = rng.uniform(0, 1, (n, p_X))\nW = rng.uniform(0, 1, (n, p_W))\n\n# Define the outcome mean function\ndef outcome_mean(X, W):\n    return np.where(\n        (X[:, 0] &gt;= 0.0) &amp; (X[:, 0] &lt; 0.25),\n        -7.5 * W[:, 0],\n        np.where(\n            (X[:, 0] &gt;= 0.25) &amp; (X[:, 0] &lt; 0.5),\n            -2.5 * W[:, 0],\n            np.where((X[:, 0] &gt;= 0.5) &amp; (X[:, 0] &lt; 0.75), 2.5 * W[:, 0], 7.5 * W[:, 0]),\n        ),\n    )\n\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\ny = outcome_mean(X, W) + epsilon\n\n# Standardize outcome\ny_bar = np.mean(y)\ny_std = np.std(y)\nresid = (y - y_bar) / y_std\n</pre> # RNG random_seed = 1234 rng = np.random.default_rng(random_seed)  # Generate covariates and basis n = 500 p_X = 10 p_W = 1 X = rng.uniform(0, 1, (n, p_X)) W = rng.uniform(0, 1, (n, p_W))  # Define the outcome mean function def outcome_mean(X, W):     return np.where(         (X[:, 0] &gt;= 0.0) &amp; (X[:, 0] &lt; 0.25),         -7.5 * W[:, 0],         np.where(             (X[:, 0] &gt;= 0.25) &amp; (X[:, 0] &lt; 0.5),             -2.5 * W[:, 0],             np.where((X[:, 0] &gt;= 0.5) &amp; (X[:, 0] &lt; 0.75), 2.5 * W[:, 0], 7.5 * W[:, 0]),         ),     )   # Generate outcome epsilon = rng.normal(0, 1, n) y = outcome_mean(X, W) + epsilon  # Standardize outcome y_bar = np.mean(y) y_std = np.std(y) resid = (y - y_bar) / y_std <p>Set some sampling parameters</p> In\u00a0[3]: Copied! <pre>alpha = 0.9\nbeta = 1.25\nmin_samples_leaf = 1\nmax_depth = -1\nnum_trees = 100\ncutpoint_grid_size = 100\nglobal_variance_init = 1.0\ntau_init = 0.5\nleaf_prior_scale = np.array([[tau_init]], order=\"C\")\na_global = 4.0\nb_global = 2.0\na_leaf = 2.0\nb_leaf = 0.5\nleaf_regression = True\nfeature_types = np.repeat(0, p_X).astype(int)  # 0 = numeric\nvar_weights = np.repeat(1 / p_X, p_X)\nif not leaf_regression:\n    leaf_model = 0\n    leaf_dimension = 1\nelif leaf_regression and p_W == 1:\n    leaf_model = 1\n    leaf_dimension = 1\nelse:\n    leaf_model = 2\n    leaf_dimension = p_W\n</pre> alpha = 0.9 beta = 1.25 min_samples_leaf = 1 max_depth = -1 num_trees = 100 cutpoint_grid_size = 100 global_variance_init = 1.0 tau_init = 0.5 leaf_prior_scale = np.array([[tau_init]], order=\"C\") a_global = 4.0 b_global = 2.0 a_leaf = 2.0 b_leaf = 0.5 leaf_regression = True feature_types = np.repeat(0, p_X).astype(int)  # 0 = numeric var_weights = np.repeat(1 / p_X, p_X) if not leaf_regression:     leaf_model = 0     leaf_dimension = 1 elif leaf_regression and p_W == 1:     leaf_model = 1     leaf_dimension = 1 else:     leaf_model = 2     leaf_dimension = p_W <p>Convert data from numpy to <code>StochTree</code> representation</p> In\u00a0[4]: Copied! <pre># Dataset (covariates and basis)\ndataset = Dataset()\ndataset.add_covariates(X)\ndataset.add_basis(W)\n\n# Residual\nresidual = Residual(resid)\n</pre> # Dataset (covariates and basis) dataset = Dataset() dataset.add_covariates(X) dataset.add_basis(W)  # Residual residual = Residual(resid) <p>Initialize tracking and sampling classes</p> In\u00a0[5]: Copied! <pre>forest_container = ForestContainer(num_trees, W.shape[1], False, False)\nactive_forest = Forest(num_trees, W.shape[1], False, False)\nglobal_model_config = GlobalModelConfig(global_error_variance=global_variance_init)\nforest_model_config = ForestModelConfig(\n    num_trees=num_trees,\n    num_features=p_X,\n    num_observations=n,\n    feature_types=feature_types,\n    variable_weights=var_weights,\n    leaf_dimension=1,\n    alpha=alpha,\n    beta=beta,\n    min_samples_leaf=min_samples_leaf,\n    max_depth=max_depth,\n    leaf_model_type=leaf_model,\n    leaf_model_scale=leaf_prior_scale,\n    cutpoint_grid_size=cutpoint_grid_size,\n)\nforest_sampler = ForestSampler(\n    dataset, global_model_config, forest_model_config\n)\ncpp_rng = RNG(random_seed)\nglobal_var_model = GlobalVarianceModel()\nleaf_var_model = LeafVarianceModel()\n\n# Initialize the leaves of each tree in the mean forest\nif leaf_regression:\n    forest_init_val = np.repeat(0.0, W.shape[1])\nelse:\n    forest_init_val = np.array([0.0])\nforest_sampler.prepare_for_sampler(\n    dataset,\n    residual,\n    active_forest,\n    leaf_model,\n    forest_init_val,\n)\n</pre> forest_container = ForestContainer(num_trees, W.shape[1], False, False) active_forest = Forest(num_trees, W.shape[1], False, False) global_model_config = GlobalModelConfig(global_error_variance=global_variance_init) forest_model_config = ForestModelConfig(     num_trees=num_trees,     num_features=p_X,     num_observations=n,     feature_types=feature_types,     variable_weights=var_weights,     leaf_dimension=1,     alpha=alpha,     beta=beta,     min_samples_leaf=min_samples_leaf,     max_depth=max_depth,     leaf_model_type=leaf_model,     leaf_model_scale=leaf_prior_scale,     cutpoint_grid_size=cutpoint_grid_size, ) forest_sampler = ForestSampler(     dataset, global_model_config, forest_model_config ) cpp_rng = RNG(random_seed) global_var_model = GlobalVarianceModel() leaf_var_model = LeafVarianceModel()  # Initialize the leaves of each tree in the mean forest if leaf_regression:     forest_init_val = np.repeat(0.0, W.shape[1]) else:     forest_init_val = np.array([0.0]) forest_sampler.prepare_for_sampler(     dataset,     residual,     active_forest,     leaf_model,     forest_init_val, ) <p>Prepare to run the sampler</p> In\u00a0[6]: Copied! <pre>num_warmstart = 10\nnum_mcmc = 100\nnum_samples = num_warmstart + num_mcmc\nglobal_var_samples = np.concatenate(\n    (np.array([global_variance_init]), np.repeat(0, num_samples))\n)\nleaf_scale_samples = np.concatenate((np.array([tau_init]), np.repeat(0, num_samples)))\n</pre> num_warmstart = 10 num_mcmc = 100 num_samples = num_warmstart + num_mcmc global_var_samples = np.concatenate(     (np.array([global_variance_init]), np.repeat(0, num_samples)) ) leaf_scale_samples = np.concatenate((np.array([tau_init]), np.repeat(0, num_samples))) <p>Run the \"grow-from-root\" (XBART) sampler</p> In\u00a0[7]: Copied! <pre>for i in range(num_warmstart):\n    forest_sampler.sample_one_iteration(\n        forest_container,\n        active_forest,\n        dataset,\n        residual,\n        cpp_rng,\n        global_model_config, \n        forest_model_config,\n        True,\n        True,\n    )\n    global_var_samples[i + 1] = global_var_model.sample_one_iteration(\n        residual, cpp_rng, a_global, b_global\n    )\n    leaf_scale_samples[i + 1] = leaf_var_model.sample_one_iteration(\n        active_forest, cpp_rng, a_leaf, b_leaf\n    )\n    leaf_prior_scale[0, 0] = leaf_scale_samples[i + 1]\n</pre> for i in range(num_warmstart):     forest_sampler.sample_one_iteration(         forest_container,         active_forest,         dataset,         residual,         cpp_rng,         global_model_config,          forest_model_config,         True,         True,     )     global_var_samples[i + 1] = global_var_model.sample_one_iteration(         residual, cpp_rng, a_global, b_global     )     leaf_scale_samples[i + 1] = leaf_var_model.sample_one_iteration(         active_forest, cpp_rng, a_leaf, b_leaf     )     leaf_prior_scale[0, 0] = leaf_scale_samples[i + 1] <p>Run the MCMC (BART) sampler, initialized at the last XBART sample</p> In\u00a0[8]: Copied! <pre>for i in range(num_warmstart, num_samples):\n    forest_sampler.sample_one_iteration(\n        forest_container,\n        active_forest,\n        dataset,\n        residual,\n        cpp_rng,\n        global_model_config, \n        forest_model_config,\n        True,\n        False,\n    )\n    global_var_samples[i + 1] = global_var_model.sample_one_iteration(\n        residual, cpp_rng, a_global, b_global\n    )\n    leaf_scale_samples[i + 1] = leaf_var_model.sample_one_iteration(\n        active_forest, cpp_rng, a_leaf, b_leaf\n    )\n    leaf_prior_scale[0, 0] = leaf_scale_samples[i + 1]\n</pre> for i in range(num_warmstart, num_samples):     forest_sampler.sample_one_iteration(         forest_container,         active_forest,         dataset,         residual,         cpp_rng,         global_model_config,          forest_model_config,         True,         False,     )     global_var_samples[i + 1] = global_var_model.sample_one_iteration(         residual, cpp_rng, a_global, b_global     )     leaf_scale_samples[i + 1] = leaf_var_model.sample_one_iteration(         active_forest, cpp_rng, a_leaf, b_leaf     )     leaf_prior_scale[0, 0] = leaf_scale_samples[i + 1] <p>Extract mean function and error variance posterior samples</p> In\u00a0[9]: Copied! <pre># Forest predictions\nforest_preds = forest_container.predict(dataset) * y_std + y_bar\nforest_preds_gfr = forest_preds[:, :num_warmstart]\nforest_preds_mcmc = forest_preds[:, num_warmstart:num_samples]\n\n# Global error variance\nsigma2_samples = global_var_samples * y_std * y_std\nsigma2_samples_gfr = sigma2_samples[:num_warmstart]\nsigma2_samples_mcmc = sigma2_samples[num_warmstart:num_samples]\n</pre> # Forest predictions forest_preds = forest_container.predict(dataset) * y_std + y_bar forest_preds_gfr = forest_preds[:, :num_warmstart] forest_preds_mcmc = forest_preds[:, num_warmstart:num_samples]  # Global error variance sigma2_samples = global_var_samples * y_std * y_std sigma2_samples_gfr = sigma2_samples[:num_warmstart] sigma2_samples_mcmc = sigma2_samples[num_warmstart:num_samples] <p>Inspect the GFR (XBART) samples</p> In\u00a0[10]: Copied! <pre>forest_pred_avg_gfr = forest_preds_gfr.mean(axis=1, keepdims=True)\nforest_pred_df_gfr = pd.DataFrame(\n    np.concatenate((np.expand_dims(y, axis=1), forest_pred_avg_gfr), axis=1),\n    columns=[\"True y\", \"Average predicted y\"],\n)\nsns.scatterplot(data=forest_pred_df_gfr, x=\"True y\", y=\"Average predicted y\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_pred_avg_gfr = forest_preds_gfr.mean(axis=1, keepdims=True) forest_pred_df_gfr = pd.DataFrame(     np.concatenate((np.expand_dims(y, axis=1), forest_pred_avg_gfr), axis=1),     columns=[\"True y\", \"Average predicted y\"], ) sns.scatterplot(data=forest_pred_df_gfr, x=\"True y\", y=\"Average predicted y\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[11]: Copied! <pre>sigma_df_gfr = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(num_warmstart), axis=1),\n            np.expand_dims(sigma2_samples_gfr, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma^2\"],\n)\nsns.scatterplot(data=sigma_df_gfr, x=\"Sample\", y=\"Sigma^2\")\nplt.show()\n</pre> sigma_df_gfr = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(num_warmstart), axis=1),             np.expand_dims(sigma2_samples_gfr, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma^2\"], ) sns.scatterplot(data=sigma_df_gfr, x=\"Sample\", y=\"Sigma^2\") plt.show() <p>Inspect the MCMC (BART) samples</p> In\u00a0[12]: Copied! <pre>forest_pred_avg_mcmc = forest_preds_mcmc.mean(axis=1, keepdims=True)\nforest_pred_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(y, axis=1), forest_pred_avg_mcmc), axis=1),\n    columns=[\"True y\", \"Average predicted y\"],\n)\nsns.scatterplot(data=forest_pred_df_mcmc, x=\"True y\", y=\"Average predicted y\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_pred_avg_mcmc = forest_preds_mcmc.mean(axis=1, keepdims=True) forest_pred_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(y, axis=1), forest_pred_avg_mcmc), axis=1),     columns=[\"True y\", \"Average predicted y\"], ) sns.scatterplot(data=forest_pred_df_mcmc, x=\"True y\", y=\"Average predicted y\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[13]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(num_samples - num_warmstart), axis=1),\n            np.expand_dims(sigma2_samples_mcmc, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma^2\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(num_samples - num_warmstart), axis=1),             np.expand_dims(sigma2_samples_mcmc, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma^2\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\") plt.show() <p>Generate sample data</p> In\u00a0[14]: Copied! <pre># RNG\nrandom_seed = 101\nrng = np.random.default_rng(random_seed)\n\n# Generate covariates and basis\nn = 500\np_X = 5\nX = rng.uniform(0, 1, (n, p_X))\npi_X = 0.35 + 0.3 * X[:, 0]\nZ = rng.binomial(1, pi_X, n).astype(float)\n\n# Define the outcome mean functions (prognostic and treatment effects)\nmu_X = (pi_X - 0.5) * 30\n# tau_X = np.sin(X[:,1]*2*np.pi)\ntau_X = X[:, 1] * 2\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\ny = mu_X + tau_X * Z + epsilon\n\n# Standardize outcome\ny_bar = np.mean(y)\ny_std = np.std(y)\nresid = (y - y_bar) / y_std\n</pre> # RNG random_seed = 101 rng = np.random.default_rng(random_seed)  # Generate covariates and basis n = 500 p_X = 5 X = rng.uniform(0, 1, (n, p_X)) pi_X = 0.35 + 0.3 * X[:, 0] Z = rng.binomial(1, pi_X, n).astype(float)  # Define the outcome mean functions (prognostic and treatment effects) mu_X = (pi_X - 0.5) * 30 # tau_X = np.sin(X[:,1]*2*np.pi) tau_X = X[:, 1] * 2  # Generate outcome epsilon = rng.normal(0, 1, n) y = mu_X + tau_X * Z + epsilon  # Standardize outcome y_bar = np.mean(y) y_std = np.std(y) resid = (y - y_bar) / y_std <p>Set some sampling parameters</p> In\u00a0[15]: Copied! <pre># Prognostic forest parameters\nalpha_mu = 0.95\nbeta_mu = 2.0\nmin_samples_leaf_mu = 1\nmax_depth_mu = -1\nnum_trees_mu = 200\ncutpoint_grid_size_mu = 100\ntau_init_mu = 1 / num_trees_mu\nleaf_prior_scale_mu = np.array([[tau_init_mu]], order=\"C\")\na_leaf_mu = 3.0\nb_leaf_mu = 1 / num_trees_mu\nleaf_regression_mu = False\nfeature_types_mu = np.repeat(0, p_X + 1).astype(int)  # 0 = numeric\nvar_weights_mu = np.repeat(1 / (p_X + 1), p_X + 1)\nleaf_model_mu = 0\nleaf_dimension_mu = 1\n\n# Treatment forest parameters\nalpha_tau = 0.75\nbeta_tau = 3.0\nmin_samples_leaf_tau = 1\nmax_depth_tau = -1\nnum_trees_tau = 100\ncutpoint_grid_size_tau = 100\ntau_init_tau = 1 / num_trees_tau\nleaf_prior_scale_tau = np.array([[tau_init_tau]], order=\"C\")\na_leaf_tau = 3.0\nb_leaf_tau = 1 / num_trees_tau\nleaf_regression_tau = True\nfeature_types_tau = np.repeat(0, p_X).astype(int)  # 0 = numeric\nvar_weights_tau = np.repeat(1 / p_X, p_X)\nleaf_model_tau = 1\nleaf_dimension_tau = 1\n\n# Global parameters\na_global = 2.0\nb_global = 1.0\nglobal_variance_init = 1.0\n</pre> # Prognostic forest parameters alpha_mu = 0.95 beta_mu = 2.0 min_samples_leaf_mu = 1 max_depth_mu = -1 num_trees_mu = 200 cutpoint_grid_size_mu = 100 tau_init_mu = 1 / num_trees_mu leaf_prior_scale_mu = np.array([[tau_init_mu]], order=\"C\") a_leaf_mu = 3.0 b_leaf_mu = 1 / num_trees_mu leaf_regression_mu = False feature_types_mu = np.repeat(0, p_X + 1).astype(int)  # 0 = numeric var_weights_mu = np.repeat(1 / (p_X + 1), p_X + 1) leaf_model_mu = 0 leaf_dimension_mu = 1  # Treatment forest parameters alpha_tau = 0.75 beta_tau = 3.0 min_samples_leaf_tau = 1 max_depth_tau = -1 num_trees_tau = 100 cutpoint_grid_size_tau = 100 tau_init_tau = 1 / num_trees_tau leaf_prior_scale_tau = np.array([[tau_init_tau]], order=\"C\") a_leaf_tau = 3.0 b_leaf_tau = 1 / num_trees_tau leaf_regression_tau = True feature_types_tau = np.repeat(0, p_X).astype(int)  # 0 = numeric var_weights_tau = np.repeat(1 / p_X, p_X) leaf_model_tau = 1 leaf_dimension_tau = 1  # Global parameters a_global = 2.0 b_global = 1.0 global_variance_init = 1.0 <p>Convert data from numpy to <code>StochTree</code> representation</p> In\u00a0[16]: Copied! <pre># Prognostic Forest Dataset (covariates)\ndataset_mu = Dataset()\ndataset_mu.add_covariates(np.c_[X, pi_X])\n\n# Treatment Forest Dataset (covariates and treatment variable)\ndataset_tau = Dataset()\ndataset_tau.add_covariates(X)\ndataset_tau.add_basis(Z)\n\n# Residual\nresidual = Residual(resid)\n</pre> # Prognostic Forest Dataset (covariates) dataset_mu = Dataset() dataset_mu.add_covariates(np.c_[X, pi_X])  # Treatment Forest Dataset (covariates and treatment variable) dataset_tau = Dataset() dataset_tau.add_covariates(X) dataset_tau.add_basis(Z)  # Residual residual = Residual(resid) <p>Initialize tracking and sampling classes</p> In\u00a0[17]: Copied! <pre># Global classes\nglobal_model_config = GlobalModelConfig(global_error_variance=global_variance_init)\ncpp_rng = RNG(random_seed)\nglobal_var_model = GlobalVarianceModel()\n\n# Prognostic forest sampling classes\nforest_container_mu = ForestContainer(num_trees_mu, 1, True, False)\nactive_forest_mu = Forest(num_trees_mu, 1, True, False)\nforest_model_config_mu = ForestModelConfig(\n    num_trees=num_trees_mu,\n    num_features=p_X + 1,\n    num_observations=n,\n    feature_types=feature_types_mu,\n    variable_weights=var_weights_mu,\n    leaf_dimension=leaf_dimension_mu,\n    alpha=alpha_mu,\n    beta=beta_mu,\n    min_samples_leaf=min_samples_leaf_mu,\n    max_depth=max_depth_mu,\n    leaf_model_type=leaf_model_mu,\n    leaf_model_scale=leaf_prior_scale_mu,\n    cutpoint_grid_size=cutpoint_grid_size_mu,\n)\nforest_sampler_mu = ForestSampler(\n    dataset_mu,\n    global_model_config, \n    forest_model_config_mu\n)\nleaf_var_model_mu = LeafVarianceModel()\n\n# Treatment forest sampling classes\nforest_container_tau = ForestContainer(\n    num_trees_tau, 1 if np.ndim(Z) == 1 else Z.shape[1], False, False\n)\nactive_forest_tau = Forest(\n    num_trees_tau, 1 if np.ndim(Z) == 1 else Z.shape[1], False, False\n)\nforest_model_config_tau = ForestModelConfig(\n    num_trees=num_trees_tau,\n    num_features=p_X,\n    num_observations=n,\n    feature_types=feature_types_tau,\n    variable_weights=var_weights_tau,\n    leaf_dimension=leaf_dimension_tau,\n    alpha=alpha_tau,\n    beta=beta_tau,\n    min_samples_leaf=min_samples_leaf_tau,\n    max_depth=max_depth_tau,\n    leaf_model_type=leaf_model_tau,\n    leaf_model_scale=leaf_prior_scale_tau,\n    cutpoint_grid_size=cutpoint_grid_size_tau,\n)\nforest_sampler_tau = ForestSampler(\n    dataset_tau,\n    global_model_config, \n    forest_model_config_tau\n)\nleaf_var_model_tau = LeafVarianceModel()\n</pre> # Global classes global_model_config = GlobalModelConfig(global_error_variance=global_variance_init) cpp_rng = RNG(random_seed) global_var_model = GlobalVarianceModel()  # Prognostic forest sampling classes forest_container_mu = ForestContainer(num_trees_mu, 1, True, False) active_forest_mu = Forest(num_trees_mu, 1, True, False) forest_model_config_mu = ForestModelConfig(     num_trees=num_trees_mu,     num_features=p_X + 1,     num_observations=n,     feature_types=feature_types_mu,     variable_weights=var_weights_mu,     leaf_dimension=leaf_dimension_mu,     alpha=alpha_mu,     beta=beta_mu,     min_samples_leaf=min_samples_leaf_mu,     max_depth=max_depth_mu,     leaf_model_type=leaf_model_mu,     leaf_model_scale=leaf_prior_scale_mu,     cutpoint_grid_size=cutpoint_grid_size_mu, ) forest_sampler_mu = ForestSampler(     dataset_mu,     global_model_config,      forest_model_config_mu ) leaf_var_model_mu = LeafVarianceModel()  # Treatment forest sampling classes forest_container_tau = ForestContainer(     num_trees_tau, 1 if np.ndim(Z) == 1 else Z.shape[1], False, False ) active_forest_tau = Forest(     num_trees_tau, 1 if np.ndim(Z) == 1 else Z.shape[1], False, False ) forest_model_config_tau = ForestModelConfig(     num_trees=num_trees_tau,     num_features=p_X,     num_observations=n,     feature_types=feature_types_tau,     variable_weights=var_weights_tau,     leaf_dimension=leaf_dimension_tau,     alpha=alpha_tau,     beta=beta_tau,     min_samples_leaf=min_samples_leaf_tau,     max_depth=max_depth_tau,     leaf_model_type=leaf_model_tau,     leaf_model_scale=leaf_prior_scale_tau,     cutpoint_grid_size=cutpoint_grid_size_tau, ) forest_sampler_tau = ForestSampler(     dataset_tau,     global_model_config,      forest_model_config_tau ) leaf_var_model_tau = LeafVarianceModel() <p>Initialize the leaves of the prognostic and treatment forests</p> In\u00a0[18]: Copied! <pre>init_mu = np.array([np.squeeze(np.mean(resid))])\nforest_sampler_mu.prepare_for_sampler(\n    dataset_mu,\n    residual,\n    active_forest_mu,\n    leaf_model_mu,\n    init_mu,\n)\n\ninit_tau = np.array([0.0])\nforest_sampler_tau.prepare_for_sampler(\n    dataset_tau,\n    residual,\n    active_forest_tau,\n    leaf_model_tau,\n    init_tau,\n)\n</pre> init_mu = np.array([np.squeeze(np.mean(resid))]) forest_sampler_mu.prepare_for_sampler(     dataset_mu,     residual,     active_forest_mu,     leaf_model_mu,     init_mu, )  init_tau = np.array([0.0]) forest_sampler_tau.prepare_for_sampler(     dataset_tau,     residual,     active_forest_tau,     leaf_model_tau,     init_tau, ) <p>Prepare to run the sampler</p> In\u00a0[19]: Copied! <pre>num_warmstart = 10\nnum_mcmc = 100\nnum_samples = num_warmstart + num_mcmc\nglobal_var_samples = np.empty(num_samples)\nleaf_scale_samples_mu = np.empty(num_samples)\nleaf_scale_samples_tau = np.empty(num_samples)\nleaf_prior_scale_mu = np.array([[tau_init_mu]])\nleaf_prior_scale_tau = np.array([[tau_init_tau]])\ncurrent_b0 = -0.5\ncurrent_b1 = 0.5\nb_0_samples = np.empty(num_samples)\nb_1_samples = np.empty(num_samples)\ntau_basis = (1 - Z) * current_b0 + Z * current_b1\ndataset_tau.update_basis(tau_basis)\n</pre> num_warmstart = 10 num_mcmc = 100 num_samples = num_warmstart + num_mcmc global_var_samples = np.empty(num_samples) leaf_scale_samples_mu = np.empty(num_samples) leaf_scale_samples_tau = np.empty(num_samples) leaf_prior_scale_mu = np.array([[tau_init_mu]]) leaf_prior_scale_tau = np.array([[tau_init_tau]]) current_b0 = -0.5 current_b1 = 0.5 b_0_samples = np.empty(num_samples) b_1_samples = np.empty(num_samples) tau_basis = (1 - Z) * current_b0 + Z * current_b1 dataset_tau.update_basis(tau_basis) <p>Run the \"grow-from-root\" (XBART) sampler</p> In\u00a0[20]: Copied! <pre>for i in range(num_warmstart):\n    # Sample the prognostic forest\n    forest_sampler_mu.sample_one_iteration(\n        forest_container_mu,\n        active_forest_mu,\n        dataset_mu,\n        residual,\n        cpp_rng,\n        global_model_config, \n        forest_model_config_mu,\n        True,\n        True,\n    )\n    # Sample global variance\n    current_sigma2 = global_var_model.sample_one_iteration(\n        residual, cpp_rng, a_global, b_global\n    )\n    global_model_config.update_global_error_variance(current_sigma2)\n    # Sample prognostic forest leaf scale\n    leaf_prior_scale_mu[0, 0] = leaf_var_model_mu.sample_one_iteration(\n        active_forest_mu, cpp_rng, a_leaf_mu, b_leaf_mu\n    )\n    leaf_scale_samples_mu[i] = leaf_prior_scale_mu[0, 0]\n    forest_model_config_mu.update_leaf_model_scale(\n        leaf_prior_scale_mu\n    )\n\n    # Sample the treatment effect forest\n    forest_sampler_tau.sample_one_iteration(\n        forest_container_tau,\n        active_forest_tau,\n        dataset_tau,\n        residual,\n        cpp_rng,\n        global_model_config, \n        forest_model_config_tau,\n        True,\n        True,\n    )\n    \n    # Sample adaptive coding parameters\n    mu_x = active_forest_mu.predict_raw(dataset_mu)\n    tau_x = np.squeeze(active_forest_tau.predict_raw(dataset_tau))\n    s_tt0 = np.sum(tau_x * tau_x * (Z == 0))\n    s_tt1 = np.sum(tau_x * tau_x * (Z == 1))\n    partial_resid_mu = resid - np.squeeze(mu_x)\n    s_ty0 = np.sum(tau_x * partial_resid_mu * (Z == 0))\n    s_ty1 = np.sum(tau_x * partial_resid_mu * (Z == 1))\n    current_b0 = rng.normal(\n        loc=(s_ty0 / (s_tt0 + 2 * current_sigma2)),\n        scale=np.sqrt(current_sigma2 / (s_tt0 + 2 * current_sigma2)),\n        size=1,\n    )[0]\n    current_b1 = rng.normal(\n        loc=(s_ty1 / (s_tt1 + 2 * current_sigma2)),\n        scale=np.sqrt(current_sigma2 / (s_tt1 + 2 * current_sigma2)),\n        size=1,\n    )[0]\n    tau_basis = (1 - Z) * current_b0 + Z * current_b1\n    dataset_tau.update_basis(tau_basis)\n    forest_sampler_tau.propagate_basis_update(dataset_tau, residual, active_forest_tau)\n    b_0_samples[i] = current_b0\n    b_1_samples[i] = current_b1\n\n    # Sample global variance\n    current_sigma2 = global_var_model.sample_one_iteration(\n        residual, cpp_rng, a_global, b_global\n    )\n    global_model_config.update_global_error_variance(current_sigma2)\n    global_var_samples[i] = current_sigma2\n    # Sample treatment forest leaf scale\n    leaf_prior_scale_tau[0, 0] = leaf_var_model_tau.sample_one_iteration(\n        active_forest_tau, cpp_rng, a_leaf_tau, b_leaf_tau\n    )\n    leaf_scale_samples_tau[i] = leaf_prior_scale_tau[0, 0]\n    forest_model_config_tau.update_leaf_model_scale(\n        leaf_prior_scale_tau\n    )\n</pre> for i in range(num_warmstart):     # Sample the prognostic forest     forest_sampler_mu.sample_one_iteration(         forest_container_mu,         active_forest_mu,         dataset_mu,         residual,         cpp_rng,         global_model_config,          forest_model_config_mu,         True,         True,     )     # Sample global variance     current_sigma2 = global_var_model.sample_one_iteration(         residual, cpp_rng, a_global, b_global     )     global_model_config.update_global_error_variance(current_sigma2)     # Sample prognostic forest leaf scale     leaf_prior_scale_mu[0, 0] = leaf_var_model_mu.sample_one_iteration(         active_forest_mu, cpp_rng, a_leaf_mu, b_leaf_mu     )     leaf_scale_samples_mu[i] = leaf_prior_scale_mu[0, 0]     forest_model_config_mu.update_leaf_model_scale(         leaf_prior_scale_mu     )      # Sample the treatment effect forest     forest_sampler_tau.sample_one_iteration(         forest_container_tau,         active_forest_tau,         dataset_tau,         residual,         cpp_rng,         global_model_config,          forest_model_config_tau,         True,         True,     )          # Sample adaptive coding parameters     mu_x = active_forest_mu.predict_raw(dataset_mu)     tau_x = np.squeeze(active_forest_tau.predict_raw(dataset_tau))     s_tt0 = np.sum(tau_x * tau_x * (Z == 0))     s_tt1 = np.sum(tau_x * tau_x * (Z == 1))     partial_resid_mu = resid - np.squeeze(mu_x)     s_ty0 = np.sum(tau_x * partial_resid_mu * (Z == 0))     s_ty1 = np.sum(tau_x * partial_resid_mu * (Z == 1))     current_b0 = rng.normal(         loc=(s_ty0 / (s_tt0 + 2 * current_sigma2)),         scale=np.sqrt(current_sigma2 / (s_tt0 + 2 * current_sigma2)),         size=1,     )[0]     current_b1 = rng.normal(         loc=(s_ty1 / (s_tt1 + 2 * current_sigma2)),         scale=np.sqrt(current_sigma2 / (s_tt1 + 2 * current_sigma2)),         size=1,     )[0]     tau_basis = (1 - Z) * current_b0 + Z * current_b1     dataset_tau.update_basis(tau_basis)     forest_sampler_tau.propagate_basis_update(dataset_tau, residual, active_forest_tau)     b_0_samples[i] = current_b0     b_1_samples[i] = current_b1      # Sample global variance     current_sigma2 = global_var_model.sample_one_iteration(         residual, cpp_rng, a_global, b_global     )     global_model_config.update_global_error_variance(current_sigma2)     global_var_samples[i] = current_sigma2     # Sample treatment forest leaf scale     leaf_prior_scale_tau[0, 0] = leaf_var_model_tau.sample_one_iteration(         active_forest_tau, cpp_rng, a_leaf_tau, b_leaf_tau     )     leaf_scale_samples_tau[i] = leaf_prior_scale_tau[0, 0]     forest_model_config_tau.update_leaf_model_scale(         leaf_prior_scale_tau     ) <p>Run the MCMC (BART) sampler, initialized at the last XBART sample</p> In\u00a0[21]: Copied! <pre>for i in range(num_warmstart, num_samples):\n    # Sample the prognostic forest\n    forest_sampler_mu.sample_one_iteration(\n        forest_container_mu,\n        active_forest_mu,\n        dataset_mu,\n        residual,\n        cpp_rng,\n        global_model_config, \n        forest_model_config_mu,\n        True,\n        False,\n    )\n    # Sample global variance\n    current_sigma2 = global_var_model.sample_one_iteration(\n        residual, cpp_rng, a_global, b_global\n    )\n    global_model_config.update_global_error_variance(current_sigma2)\n    # Sample prognostic forest leaf scale\n    leaf_prior_scale_mu[0, 0] = leaf_var_model_mu.sample_one_iteration(\n        active_forest_mu, cpp_rng, a_leaf_mu, b_leaf_mu\n    )\n    leaf_scale_samples_mu[i] = leaf_prior_scale_mu[0, 0]\n    forest_model_config_mu.update_leaf_model_scale(\n        leaf_prior_scale_mu\n    )\n\n    # Sample the treatment effect forest\n    forest_sampler_tau.sample_one_iteration(\n        forest_container_tau,\n        active_forest_tau,\n        dataset_tau,\n        residual,\n        cpp_rng,\n        global_model_config, \n        forest_model_config_tau,\n        True,\n        False,\n    )\n    \n    # Sample adaptive coding parameters\n    mu_x = active_forest_mu.predict_raw(dataset_mu)\n    tau_x = np.squeeze(active_forest_tau.predict_raw(dataset_tau))\n    s_tt0 = np.sum(tau_x * tau_x * (Z == 0))\n    s_tt1 = np.sum(tau_x * tau_x * (Z == 1))\n    partial_resid_mu = resid - np.squeeze(mu_x)\n    s_ty0 = np.sum(tau_x * partial_resid_mu * (Z == 0))\n    s_ty1 = np.sum(tau_x * partial_resid_mu * (Z == 1))\n    current_b0 = rng.normal(\n        loc=(s_ty0 / (s_tt0 + 2 * current_sigma2)),\n        scale=np.sqrt(current_sigma2 / (s_tt0 + 2 * current_sigma2)),\n        size=1,\n    )[0]\n    current_b1 = rng.normal(\n        loc=(s_ty1 / (s_tt1 + 2 * current_sigma2)),\n        scale=np.sqrt(current_sigma2 / (s_tt1 + 2 * current_sigma2)),\n        size=1,\n    )[0]\n    tau_basis = (1 - Z) * current_b0 + Z * current_b1\n    dataset_tau.update_basis(tau_basis)\n    forest_sampler_tau.propagate_basis_update(dataset_tau, residual, active_forest_tau)\n    b_0_samples[i] = current_b0\n    b_1_samples[i] = current_b1\n\n    # Sample global variance\n    current_sigma2 = global_var_model.sample_one_iteration(\n        residual, cpp_rng, a_global, b_global\n    )\n    global_model_config.update_global_error_variance(current_sigma2)\n    global_var_samples[i] = current_sigma2\n    # Sample treatment forest leaf scale\n    leaf_prior_scale_tau[0, 0] = leaf_var_model_tau.sample_one_iteration(\n        active_forest_tau, cpp_rng, a_leaf_tau, b_leaf_tau\n    )\n    leaf_scale_samples_tau[i] = leaf_prior_scale_tau[0, 0]\n    forest_model_config_tau.update_leaf_model_scale(\n        leaf_prior_scale_tau\n    )\n</pre> for i in range(num_warmstart, num_samples):     # Sample the prognostic forest     forest_sampler_mu.sample_one_iteration(         forest_container_mu,         active_forest_mu,         dataset_mu,         residual,         cpp_rng,         global_model_config,          forest_model_config_mu,         True,         False,     )     # Sample global variance     current_sigma2 = global_var_model.sample_one_iteration(         residual, cpp_rng, a_global, b_global     )     global_model_config.update_global_error_variance(current_sigma2)     # Sample prognostic forest leaf scale     leaf_prior_scale_mu[0, 0] = leaf_var_model_mu.sample_one_iteration(         active_forest_mu, cpp_rng, a_leaf_mu, b_leaf_mu     )     leaf_scale_samples_mu[i] = leaf_prior_scale_mu[0, 0]     forest_model_config_mu.update_leaf_model_scale(         leaf_prior_scale_mu     )      # Sample the treatment effect forest     forest_sampler_tau.sample_one_iteration(         forest_container_tau,         active_forest_tau,         dataset_tau,         residual,         cpp_rng,         global_model_config,          forest_model_config_tau,         True,         False,     )          # Sample adaptive coding parameters     mu_x = active_forest_mu.predict_raw(dataset_mu)     tau_x = np.squeeze(active_forest_tau.predict_raw(dataset_tau))     s_tt0 = np.sum(tau_x * tau_x * (Z == 0))     s_tt1 = np.sum(tau_x * tau_x * (Z == 1))     partial_resid_mu = resid - np.squeeze(mu_x)     s_ty0 = np.sum(tau_x * partial_resid_mu * (Z == 0))     s_ty1 = np.sum(tau_x * partial_resid_mu * (Z == 1))     current_b0 = rng.normal(         loc=(s_ty0 / (s_tt0 + 2 * current_sigma2)),         scale=np.sqrt(current_sigma2 / (s_tt0 + 2 * current_sigma2)),         size=1,     )[0]     current_b1 = rng.normal(         loc=(s_ty1 / (s_tt1 + 2 * current_sigma2)),         scale=np.sqrt(current_sigma2 / (s_tt1 + 2 * current_sigma2)),         size=1,     )[0]     tau_basis = (1 - Z) * current_b0 + Z * current_b1     dataset_tau.update_basis(tau_basis)     forest_sampler_tau.propagate_basis_update(dataset_tau, residual, active_forest_tau)     b_0_samples[i] = current_b0     b_1_samples[i] = current_b1      # Sample global variance     current_sigma2 = global_var_model.sample_one_iteration(         residual, cpp_rng, a_global, b_global     )     global_model_config.update_global_error_variance(current_sigma2)     global_var_samples[i] = current_sigma2     # Sample treatment forest leaf scale     leaf_prior_scale_tau[0, 0] = leaf_var_model_tau.sample_one_iteration(         active_forest_tau, cpp_rng, a_leaf_tau, b_leaf_tau     )     leaf_scale_samples_tau[i] = leaf_prior_scale_tau[0, 0]     forest_model_config_tau.update_leaf_model_scale(         leaf_prior_scale_tau     ) <p>Extract mean function and error variance posterior samples</p> In\u00a0[22]: Copied! <pre># Forest predictions\nforest_preds_mu = forest_container_mu.predict(dataset_mu) * y_std + y_bar\nforest_preds_mu_gfr = forest_preds_mu[:, :num_warmstart]\nforest_preds_mu_mcmc = forest_preds_mu[:, num_warmstart:num_samples]\ntreatment_coding_samples = b_1_samples - b_0_samples\nforest_preds_tau = (\n    forest_container_tau.predict_raw(dataset_tau)\n    * y_std\n    * np.expand_dims(treatment_coding_samples, axis=(0, 2))\n)\nforest_preds_tau_gfr = forest_preds_tau[:, :num_warmstart]\nforest_preds_tau_mcmc = forest_preds_tau[:, num_warmstart:num_samples]\n\n# Global error variance\nsigma2_samples = global_var_samples * y_std * y_std\nsigma2_samples_gfr = sigma2_samples[:num_warmstart]\nsigma2_samples_mcmc = sigma2_samples[num_warmstart:num_samples]\n\n# Adaptive coding parameters\nb_1_samples_gfr = b_1_samples[:num_warmstart] * y_std\nb_0_samples_gfr = b_0_samples[:num_warmstart] * y_std\nb_1_samples_mcmc = b_1_samples[num_warmstart:] * y_std\nb_0_samples_mcmc = b_0_samples[num_warmstart:] * y_std\n</pre> # Forest predictions forest_preds_mu = forest_container_mu.predict(dataset_mu) * y_std + y_bar forest_preds_mu_gfr = forest_preds_mu[:, :num_warmstart] forest_preds_mu_mcmc = forest_preds_mu[:, num_warmstart:num_samples] treatment_coding_samples = b_1_samples - b_0_samples forest_preds_tau = (     forest_container_tau.predict_raw(dataset_tau)     * y_std     * np.expand_dims(treatment_coding_samples, axis=(0, 2)) ) forest_preds_tau_gfr = forest_preds_tau[:, :num_warmstart] forest_preds_tau_mcmc = forest_preds_tau[:, num_warmstart:num_samples]  # Global error variance sigma2_samples = global_var_samples * y_std * y_std sigma2_samples_gfr = sigma2_samples[:num_warmstart] sigma2_samples_mcmc = sigma2_samples[num_warmstart:num_samples]  # Adaptive coding parameters b_1_samples_gfr = b_1_samples[:num_warmstart] * y_std b_0_samples_gfr = b_0_samples[:num_warmstart] * y_std b_1_samples_mcmc = b_1_samples[num_warmstart:] * y_std b_0_samples_mcmc = b_0_samples[num_warmstart:] * y_std <p>Inspect the GFR (XBART) samples</p> In\u00a0[23]: Copied! <pre>forest_preds_tau_avg_gfr = np.squeeze(forest_preds_tau_gfr).mean(axis=1, keepdims=True)\nforest_pred_tau_df_gfr = pd.DataFrame(\n    np.concatenate((np.expand_dims(tau_X, 1), forest_preds_tau_avg_gfr), axis=1),\n    columns=[\"True tau\", \"Average estimated tau\"],\n)\nsns.scatterplot(data=forest_pred_tau_df_gfr, x=\"True tau\", y=\"Average estimated tau\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_tau_avg_gfr = np.squeeze(forest_preds_tau_gfr).mean(axis=1, keepdims=True) forest_pred_tau_df_gfr = pd.DataFrame(     np.concatenate((np.expand_dims(tau_X, 1), forest_preds_tau_avg_gfr), axis=1),     columns=[\"True tau\", \"Average estimated tau\"], ) sns.scatterplot(data=forest_pred_tau_df_gfr, x=\"True tau\", y=\"Average estimated tau\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[24]: Copied! <pre>forest_pred_avg_gfr = np.squeeze(forest_preds_mu_gfr).mean(axis=1, keepdims=True)\nforest_pred_df_gfr = pd.DataFrame(\n    np.concatenate((np.expand_dims(mu_X, 1), forest_pred_avg_gfr), axis=1),\n    columns=[\"True mu\", \"Average estimated mu\"],\n)\nsns.scatterplot(data=forest_pred_df_gfr, x=\"True mu\", y=\"Average estimated mu\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_pred_avg_gfr = np.squeeze(forest_preds_mu_gfr).mean(axis=1, keepdims=True) forest_pred_df_gfr = pd.DataFrame(     np.concatenate((np.expand_dims(mu_X, 1), forest_pred_avg_gfr), axis=1),     columns=[\"True mu\", \"Average estimated mu\"], ) sns.scatterplot(data=forest_pred_df_gfr, x=\"True mu\", y=\"Average estimated mu\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[25]: Copied! <pre>sigma_df_gfr = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(num_warmstart), axis=1),\n            np.expand_dims(sigma2_samples_gfr, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma^2\"],\n)\nsns.scatterplot(data=sigma_df_gfr, x=\"Sample\", y=\"Sigma^2\")\nplt.show()\n</pre> sigma_df_gfr = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(num_warmstart), axis=1),             np.expand_dims(sigma2_samples_gfr, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma^2\"], ) sns.scatterplot(data=sigma_df_gfr, x=\"Sample\", y=\"Sigma^2\") plt.show() In\u00a0[26]: Copied! <pre>b_df_gfr = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(num_warmstart), axis=1),\n            np.expand_dims(b_0_samples_gfr, axis=1),\n            np.expand_dims(b_1_samples_gfr, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Beta_0\", \"Beta_1\"],\n)\nsns.scatterplot(data=b_df_gfr, x=\"Sample\", y=\"Beta_0\")\nsns.scatterplot(data=b_df_gfr, x=\"Sample\", y=\"Beta_1\")\nplt.show()\n</pre> b_df_gfr = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(num_warmstart), axis=1),             np.expand_dims(b_0_samples_gfr, axis=1),             np.expand_dims(b_1_samples_gfr, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Beta_0\", \"Beta_1\"], ) sns.scatterplot(data=b_df_gfr, x=\"Sample\", y=\"Beta_0\") sns.scatterplot(data=b_df_gfr, x=\"Sample\", y=\"Beta_1\") plt.show() <p>Inspect the MCMC (BART) samples</p> In\u00a0[27]: Copied! <pre>forest_pred_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis=1, keepdims=True)\nforest_pred_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(tau_X, 1), forest_pred_avg_mcmc), axis=1),\n    columns=[\"True tau\", \"Average estimated tau\"],\n)\nsns.scatterplot(data=forest_pred_df_mcmc, x=\"True tau\", y=\"Average estimated tau\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_pred_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis=1, keepdims=True) forest_pred_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(tau_X, 1), forest_pred_avg_mcmc), axis=1),     columns=[\"True tau\", \"Average estimated tau\"], ) sns.scatterplot(data=forest_pred_df_mcmc, x=\"True tau\", y=\"Average estimated tau\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[28]: Copied! <pre>forest_pred_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis=1, keepdims=True)\nforest_pred_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(mu_X, 1), forest_pred_avg_mcmc), axis=1),\n    columns=[\"True mu\", \"Average estimated mu\"],\n)\nsns.scatterplot(data=forest_pred_df_mcmc, x=\"True mu\", y=\"Average estimated mu\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_pred_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis=1, keepdims=True) forest_pred_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(mu_X, 1), forest_pred_avg_mcmc), axis=1),     columns=[\"True mu\", \"Average estimated mu\"], ) sns.scatterplot(data=forest_pred_df_mcmc, x=\"True mu\", y=\"Average estimated mu\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[29]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(num_samples - num_warmstart), axis=1),\n            np.expand_dims(sigma2_samples_mcmc, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma^2\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(num_samples - num_warmstart), axis=1),             np.expand_dims(sigma2_samples_mcmc, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma^2\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\") plt.show() In\u00a0[30]: Copied! <pre>b_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(num_samples - num_warmstart), axis=1),\n            np.expand_dims(b_0_samples_mcmc, axis=1),\n            np.expand_dims(b_1_samples_mcmc, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Beta_0\", \"Beta_1\"],\n)\nsns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_0\")\nsns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_1\")\nplt.show()\n</pre> b_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(num_samples - num_warmstart), axis=1),             np.expand_dims(b_0_samples_mcmc, axis=1),             np.expand_dims(b_1_samples_mcmc, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Beta_0\", \"Beta_1\"], ) sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_0\") sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_1\") plt.show() <p>Murray, Jared S. \"Log-linear Bayesian additive regression trees for multinomial logistic and count regression models.\" Journal of the American Statistical Association 116, no. 534 (2021): 756-769.</p> <p>Hahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. \"Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion).\" Bayesian Analysis 15, no. 3 (2020): 965-1056.</p>"},{"location":"python_docs/demo/prototype_interface.html#low-level-interface","title":"Low-Level Interface\u00b6","text":""},{"location":"python_docs/demo/prototype_interface.html#scenario-1-supervised-learning","title":"Scenario 1: Supervised Learning\u00b6","text":""},{"location":"python_docs/demo/prototype_interface.html#scenario-2-causal-inference","title":"Scenario 2: Causal Inference\u00b6","text":""},{"location":"python_docs/demo/prototype_interface.html#references","title":"References\u00b6","text":""},{"location":"python_docs/demo/serialization.html","title":"Model Serialization","text":"<p>Load necessary libraries</p> In\u00a0[1]: Copied! <pre>import json\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom stochtree import BARTModel\n</pre> import json import os  import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.model_selection import train_test_split  from stochtree import BARTModel <p>Generate sample data</p> In\u00a0[2]: Copied! <pre># RNG\nrandom_seed = 1234\nrng = np.random.default_rng(random_seed)\n\n# Generate covariates and basis\nn = 100\np_X = 10\np_W = 1\nX = rng.uniform(0, 1, (n, p_X))\nW = rng.uniform(0, 1, (n, p_W))\n\n\n# Define the outcome mean function\ndef outcome_mean(X, W):\n    return np.where(\n        (X[:, 0] &gt;= 0.0) &amp; (X[:, 0] &lt; 0.25),\n        -7.5 * W[:, 0],\n        np.where(\n            (X[:, 0] &gt;= 0.25) &amp; (X[:, 0] &lt; 0.5),\n            -2.5 * W[:, 0],\n            np.where((X[:, 0] &gt;= 0.5) &amp; (X[:, 0] &lt; 0.75), 2.5 * W[:, 0], 7.5 * W[:, 0]),\n        ),\n    )\n\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\ny = outcome_mean(X, W) + epsilon\n\n# Standardize outcome\ny_bar = np.mean(y)\ny_std = np.std(y)\nresid = (y - y_bar) / y_std\n</pre> # RNG random_seed = 1234 rng = np.random.default_rng(random_seed)  # Generate covariates and basis n = 100 p_X = 10 p_W = 1 X = rng.uniform(0, 1, (n, p_X)) W = rng.uniform(0, 1, (n, p_W))   # Define the outcome mean function def outcome_mean(X, W):     return np.where(         (X[:, 0] &gt;= 0.0) &amp; (X[:, 0] &lt; 0.25),         -7.5 * W[:, 0],         np.where(             (X[:, 0] &gt;= 0.25) &amp; (X[:, 0] &lt; 0.5),             -2.5 * W[:, 0],             np.where((X[:, 0] &gt;= 0.5) &amp; (X[:, 0] &lt; 0.75), 2.5 * W[:, 0], 7.5 * W[:, 0]),         ),     )   # Generate outcome epsilon = rng.normal(0, 1, n) y = outcome_mean(X, W) + epsilon  # Standardize outcome y_bar = np.mean(y) y_std = np.std(y) resid = (y - y_bar) / y_std <p>Test-train split</p> In\u00a0[3]: Copied! <pre>sample_inds = np.arange(n)\ntrain_inds, test_inds = train_test_split(sample_inds, test_size=0.5)\nX_train = X[train_inds, :]\nX_test = X[test_inds, :]\nbasis_train = W[train_inds, :]\nbasis_test = W[test_inds, :]\ny_train = y[train_inds]\ny_test = y[test_inds]\n</pre> sample_inds = np.arange(n) train_inds, test_inds = train_test_split(sample_inds, test_size=0.5) X_train = X[train_inds, :] X_test = X[test_inds, :] basis_train = W[train_inds, :] basis_test = W[test_inds, :] y_train = y[train_inds] y_test = y[test_inds] <p>Run BART</p> In\u00a0[4]: Copied! <pre>bart_model = BARTModel()\nbart_model.sample(\n    X_train=X_train,\n    y_train=y_train,\n    leaf_basis_train=basis_train,\n    X_test=X_test,\n    leaf_basis_test=basis_test,\n    num_gfr=10,\n    num_mcmc=10,\n)\n</pre> bart_model = BARTModel() bart_model.sample(     X_train=X_train,     y_train=y_train,     leaf_basis_train=basis_train,     X_test=X_test,     leaf_basis_test=basis_test,     num_gfr=10,     num_mcmc=10, ) <p>Inspect the MCMC (BART) samples</p> In\u00a0[5]: Copied! <pre>forest_preds_y_mcmc = bart_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True)\ny_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),\n    columns=[\"True outcome\", \"Average estimated outcome\"],\n)\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bart_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True) y_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),     columns=[\"True outcome\", \"Average estimated outcome\"], ) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[6]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(bart_model.num_samples), axis=1),\n            np.expand_dims(bart_model.global_var_samples, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma^2\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(bart_model.num_samples), axis=1),             np.expand_dims(bart_model.global_var_samples, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma^2\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\") plt.show() <p>Compute the test set RMSE</p> In\u00a0[7]: Copied! <pre>np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2)))\n</pre> np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2))) Out[7]: <pre>np.float64(1.9502110630153409)</pre> <p>Serialize the BART model to JSON</p> In\u00a0[8]: Copied! <pre>bart_json_string = bart_model.to_json()\n</pre> bart_json_string = bart_model.to_json() <p>Deserialize BART model from JSON string</p> In\u00a0[9]: Copied! <pre>bart_model_deserialized = BARTModel()\nbart_model_deserialized.from_json(bart_json_string)\n</pre> bart_model_deserialized = BARTModel() bart_model_deserialized.from_json(bart_json_string) <p>Compare predictions</p> In\u00a0[10]: Copied! <pre>y_hat_deserialized = bart_model_deserialized.predict(X_test, basis_test)\ny_avg_mcmc_deserialized = np.squeeze(y_hat_deserialized).mean(axis=1, keepdims=True)\ny_df = pd.DataFrame(\n    np.concatenate((y_avg_mcmc, y_avg_mcmc_deserialized), axis=1),\n    columns=[\"Original model\", \"Deserialized model\"],\n)\nsns.scatterplot(data=y_df, x=\"Original model\", y=\"Deserialized model\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> y_hat_deserialized = bart_model_deserialized.predict(X_test, basis_test) y_avg_mcmc_deserialized = np.squeeze(y_hat_deserialized).mean(axis=1, keepdims=True) y_df = pd.DataFrame(     np.concatenate((y_avg_mcmc, y_avg_mcmc_deserialized), axis=1),     columns=[\"Original model\", \"Deserialized model\"], ) sns.scatterplot(data=y_df, x=\"Original model\", y=\"Deserialized model\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() <p>Compare parameter samples</p> In\u00a0[11]: Copied! <pre>sigma2_df = pd.DataFrame(\n    np.c_[bart_model.global_var_samples, bart_model_deserialized.global_var_samples],\n    columns=[\"Original model\", \"Deserialized model\"],\n)\nsns.scatterplot(data=sigma2_df, x=\"Original model\", y=\"Deserialized model\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> sigma2_df = pd.DataFrame(     np.c_[bart_model.global_var_samples, bart_model_deserialized.global_var_samples],     columns=[\"Original model\", \"Deserialized model\"], ) sns.scatterplot(data=sigma2_df, x=\"Original model\", y=\"Deserialized model\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() <p>Save to JSON file</p> In\u00a0[12]: Copied! <pre>with open(\"bart.json\", \"w\") as f:\n    bart_json_python = json.loads(bart_json_string)\n    json.dump(bart_json_python, f)\n</pre> with open(\"bart.json\", \"w\") as f:     bart_json_python = json.loads(bart_json_string)     json.dump(bart_json_python, f) <p>Reload from JSON file</p> In\u00a0[13]: Copied! <pre>with open(\"bart.json\", \"r\") as f:\n    bart_json_python_reload = json.load(f)\nbart_json_string_reload = json.dumps(bart_json_python_reload)\nbart_model_file_deserialized = BARTModel()\nbart_model_file_deserialized.from_json(bart_json_string_reload)\n</pre> with open(\"bart.json\", \"r\") as f:     bart_json_python_reload = json.load(f) bart_json_string_reload = json.dumps(bart_json_python_reload) bart_model_file_deserialized = BARTModel() bart_model_file_deserialized.from_json(bart_json_string_reload) <p>Compare predictions</p> In\u00a0[14]: Copied! <pre>y_hat_file_deserialized = bart_model_file_deserialized.predict(X_test, basis_test)\ny_avg_mcmc_file_deserialized = np.squeeze(y_hat_file_deserialized).mean(\n    axis=1, keepdims=True\n)\ny_df = pd.DataFrame(\n    np.concatenate((y_avg_mcmc, y_avg_mcmc_file_deserialized), axis=1),\n    columns=[\"Original model\", \"Deserialized model\"],\n)\nsns.scatterplot(data=y_df, x=\"Original model\", y=\"Deserialized model\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> y_hat_file_deserialized = bart_model_file_deserialized.predict(X_test, basis_test) y_avg_mcmc_file_deserialized = np.squeeze(y_hat_file_deserialized).mean(     axis=1, keepdims=True ) y_df = pd.DataFrame(     np.concatenate((y_avg_mcmc, y_avg_mcmc_file_deserialized), axis=1),     columns=[\"Original model\", \"Deserialized model\"], ) sns.scatterplot(data=y_df, x=\"Original model\", y=\"Deserialized model\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() <p>Compare parameter samples</p> In\u00a0[15]: Copied! <pre>sigma2_df = pd.DataFrame(\n    np.c_[\n        bart_model.global_var_samples, bart_model_file_deserialized.global_var_samples\n    ],\n    columns=[\"Original model\", \"Deserialized model\"],\n)\nsns.scatterplot(data=sigma2_df, x=\"Original model\", y=\"Deserialized model\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> sigma2_df = pd.DataFrame(     np.c_[         bart_model.global_var_samples, bart_model_file_deserialized.global_var_samples     ],     columns=[\"Original model\", \"Deserialized model\"], ) sns.scatterplot(data=sigma2_df, x=\"Original model\", y=\"Deserialized model\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() <p>Clean up JSON file</p> In\u00a0[16]: Copied! <pre>os.remove(\"bart.json\")\n</pre> os.remove(\"bart.json\")"},{"location":"python_docs/demo/serialization.html#model-serialization","title":"Model Serialization\u00b6","text":""},{"location":"python_docs/demo/serialization.html#demo-1-supervised-learning","title":"Demo 1: Supervised Learning\u00b6","text":""},{"location":"python_docs/demo/supervised_learning.html","title":"Supervised Learning","text":"<p>Load necessary libraries</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom stochtree import BARTModel\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.model_selection import train_test_split  from stochtree import BARTModel <p>Generate sample data</p> In\u00a0[2]: Copied! <pre># RNG\nrandom_seed = 1234\nrng = np.random.default_rng(random_seed)\n\n# Generate covariates and basis\nn = 1000\np_X = 10\np_W = 1\nX = rng.uniform(0, 1, (n, p_X))\nW = rng.uniform(0, 1, (n, p_W))\n\n\n# Define the outcome mean function\ndef outcome_mean(X, W):\n    return np.where(\n        (X[:, 0] &gt;= 0.0) &amp; (X[:, 0] &lt; 0.25),\n        -7.5 * W[:, 0],\n        np.where(\n            (X[:, 0] &gt;= 0.25) &amp; (X[:, 0] &lt; 0.5),\n            -2.5 * W[:, 0],\n            np.where((X[:, 0] &gt;= 0.5) &amp; (X[:, 0] &lt; 0.75), 2.5 * W[:, 0], 7.5 * W[:, 0]),\n        ),\n    )\n\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\ny = outcome_mean(X, W) + epsilon\n\n# Standardize outcome\ny_bar = np.mean(y)\ny_std = np.std(y)\nresid = (y - y_bar) / y_std\n</pre> # RNG random_seed = 1234 rng = np.random.default_rng(random_seed)  # Generate covariates and basis n = 1000 p_X = 10 p_W = 1 X = rng.uniform(0, 1, (n, p_X)) W = rng.uniform(0, 1, (n, p_W))   # Define the outcome mean function def outcome_mean(X, W):     return np.where(         (X[:, 0] &gt;= 0.0) &amp; (X[:, 0] &lt; 0.25),         -7.5 * W[:, 0],         np.where(             (X[:, 0] &gt;= 0.25) &amp; (X[:, 0] &lt; 0.5),             -2.5 * W[:, 0],             np.where((X[:, 0] &gt;= 0.5) &amp; (X[:, 0] &lt; 0.75), 2.5 * W[:, 0], 7.5 * W[:, 0]),         ),     )   # Generate outcome epsilon = rng.normal(0, 1, n) y = outcome_mean(X, W) + epsilon  # Standardize outcome y_bar = np.mean(y) y_std = np.std(y) resid = (y - y_bar) / y_std <p>Test-train split</p> In\u00a0[3]: Copied! <pre>sample_inds = np.arange(n)\ntrain_inds, test_inds = train_test_split(sample_inds, test_size=0.5)\nX_train = X[train_inds, :]\nX_test = X[test_inds, :]\nbasis_train = W[train_inds, :]\nbasis_test = W[test_inds, :]\ny_train = y[train_inds]\ny_test = y[test_inds]\n</pre> sample_inds = np.arange(n) train_inds, test_inds = train_test_split(sample_inds, test_size=0.5) X_train = X[train_inds, :] X_test = X[test_inds, :] basis_train = W[train_inds, :] basis_test = W[test_inds, :] y_train = y[train_inds] y_test = y[test_inds] <p>Run BART</p> In\u00a0[4]: Copied! <pre>bart_model = BARTModel()\ngeneral_params = {\"num_chains\": 3}\nbart_model.sample(\n    X_train=X_train,\n    y_train=y_train,\n    leaf_basis_train=basis_train,\n    X_test=X_test,\n    leaf_basis_test=basis_test,\n    num_gfr=10,\n    num_mcmc=100,\n    general_params=general_params,\n)\n</pre> bart_model = BARTModel() general_params = {\"num_chains\": 3} bart_model.sample(     X_train=X_train,     y_train=y_train,     leaf_basis_train=basis_train,     X_test=X_test,     leaf_basis_test=basis_test,     num_gfr=10,     num_mcmc=100,     general_params=general_params, ) <p>Inspect the MCMC (BART) samples</p> In\u00a0[5]: Copied! <pre>forest_preds_y_mcmc = bart_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True)\ny_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),\n    columns=[\"True outcome\", \"Average estimated outcome\"],\n)\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bart_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True) y_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),     columns=[\"True outcome\", \"Average estimated outcome\"], ) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[6]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(bart_model.num_samples), axis=1),\n            np.expand_dims(bart_model.global_var_samples, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma^2\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(bart_model.num_samples), axis=1),             np.expand_dims(bart_model.global_var_samples, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma^2\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\") plt.show() <p>Compute the test set RMSE</p> In\u00a0[7]: Copied! <pre>np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2)))\n</pre> np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2))) Out[7]: <pre>np.float64(1.1589830673526327)</pre> <p>Run BART</p> In\u00a0[8]: Copied! <pre>bart_model = BARTModel()\nX_train_aug = np.c_[X_train, basis_train]\nX_test_aug = np.c_[X_test, basis_test]\nbart_model.sample(\n    X_train=X_train_aug, y_train=y_train, X_test=X_test_aug, num_gfr=10, num_mcmc=100\n)\n</pre> bart_model = BARTModel() X_train_aug = np.c_[X_train, basis_train] X_test_aug = np.c_[X_test, basis_test] bart_model.sample(     X_train=X_train_aug, y_train=y_train, X_test=X_test_aug, num_gfr=10, num_mcmc=100 ) <p>Inspect the MCMC (BART) samples</p> In\u00a0[9]: Copied! <pre>forest_preds_y_mcmc = bart_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True)\ny_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),\n    columns=[\"True outcome\", \"Average estimated outcome\"],\n)\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bart_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True) y_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),     columns=[\"True outcome\", \"Average estimated outcome\"], ) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[10]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(bart_model.num_samples), axis=1),\n            np.expand_dims(bart_model.global_var_samples, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma^2\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(bart_model.num_samples), axis=1),             np.expand_dims(bart_model.global_var_samples, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma^2\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\") plt.show() <p>Compute the test set RMSE</p> In\u00a0[11]: Copied! <pre>np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2)))\n</pre> np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2))) Out[11]: <pre>np.float64(1.3555904041759936)</pre> <p>Run BART</p> In\u00a0[12]: Copied! <pre>bart_model = BARTModel()\nbart_model.sample(\n    X_train=X_train, y_train=y_train, X_test=X_test, num_gfr=10, num_mcmc=100\n)\n</pre> bart_model = BARTModel() bart_model.sample(     X_train=X_train, y_train=y_train, X_test=X_test, num_gfr=10, num_mcmc=100 ) <p>Inspect the MCMC (BART) samples</p> In\u00a0[13]: Copied! <pre>forest_preds_y_mcmc = bart_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True)\ny_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),\n    columns=[\"True outcome\", \"Average estimated outcome\"],\n)\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bart_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True) y_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),     columns=[\"True outcome\", \"Average estimated outcome\"], ) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[14]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(bart_model.num_samples), axis=1),\n            np.expand_dims(bart_model.global_var_samples, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma^2\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(bart_model.num_samples), axis=1),             np.expand_dims(bart_model.global_var_samples, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma^2\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma^2\") plt.show() <p>Compute the test set RMSE</p> In\u00a0[15]: Copied! <pre>np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2)))\n</pre> np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2))) Out[15]: <pre>np.float64(2.038273189451284)</pre>"},{"location":"python_docs/demo/supervised_learning.html#supervised-learning","title":"Supervised Learning\u00b6","text":""},{"location":"python_docs/demo/supervised_learning.html#demo-1-using-w-in-a-linear-leaf-regression","title":"Demo 1: Using <code>W</code> in a linear leaf regression\u00b6","text":""},{"location":"python_docs/demo/supervised_learning.html#demo-2-including-w-as-a-covariate-in-the-standard-constant-leaf-bart-model","title":"Demo 2: Including <code>W</code> as a covariate in the standard \"constant leaf\" BART model\u00b6","text":""},{"location":"python_docs/demo/supervised_learning.html#demo-3-omitting-w-entirely","title":"Demo 3: Omitting <code>W</code> entirely\u00b6","text":""},{"location":"python_docs/demo/tree_inspection.html","title":"Internal Tree Inspection","text":"<p>Load necessary libraries</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom stochtree import BARTModel\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.model_selection import train_test_split  from stochtree import BARTModel <p>Generate sample data where feature 10 is the only \"important\" feature.</p> In\u00a0[2]: Copied! <pre># RNG\nrandom_seed = 1234\nrng = np.random.default_rng(random_seed)\n\n# Generate covariates and basis\nn = 500\np_X = 10\nX = rng.uniform(0, 1, (n, p_X))\n\n\n# Define the outcome mean function\ndef outcome_mean(X):\n    return np.where(\n        (X[:, 9] &gt;= 0.0) &amp; (X[:, 9] &lt; 0.25),\n        -7.5,\n        np.where(\n            (X[:, 9] &gt;= 0.25) &amp; (X[:, 9] &lt; 0.5),\n            -2.5,\n            np.where((X[:, 9] &gt;= 0.5) &amp; (X[:, 9] &lt; 0.75), 2.5, 7.5),\n        ),\n    )\n\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\ny = outcome_mean(X) + epsilon\n\n# Standardize outcome\ny_bar = np.mean(y)\ny_std = np.std(y)\nresid = (y - y_bar) / y_std\n</pre> # RNG random_seed = 1234 rng = np.random.default_rng(random_seed)  # Generate covariates and basis n = 500 p_X = 10 X = rng.uniform(0, 1, (n, p_X))   # Define the outcome mean function def outcome_mean(X):     return np.where(         (X[:, 9] &gt;= 0.0) &amp; (X[:, 9] &lt; 0.25),         -7.5,         np.where(             (X[:, 9] &gt;= 0.25) &amp; (X[:, 9] &lt; 0.5),             -2.5,             np.where((X[:, 9] &gt;= 0.5) &amp; (X[:, 9] &lt; 0.75), 2.5, 7.5),         ),     )   # Generate outcome epsilon = rng.normal(0, 1, n) y = outcome_mean(X) + epsilon  # Standardize outcome y_bar = np.mean(y) y_std = np.std(y) resid = (y - y_bar) / y_std <p>Test-train split</p> In\u00a0[3]: Copied! <pre>sample_inds = np.arange(n)\ntrain_inds, test_inds = train_test_split(sample_inds, test_size=0.5)\nX_train = X[train_inds, :]\nX_test = X[test_inds, :]\ny_train = y[train_inds]\ny_test = y[test_inds]\n</pre> sample_inds = np.arange(n) train_inds, test_inds = train_test_split(sample_inds, test_size=0.5) X_train = X[train_inds, :] X_test = X[test_inds, :] y_train = y[train_inds] y_test = y[test_inds] <p>Run BART</p> In\u00a0[4]: Copied! <pre>bart_model = BARTModel()\nparam_dict = {\"keep_gfr\": True}\nbart_model.sample(\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    num_gfr=10,\n    num_mcmc=10,\n    mean_forest_params=param_dict,\n)\n</pre> bart_model = BARTModel() param_dict = {\"keep_gfr\": True} bart_model.sample(     X_train=X_train,     y_train=y_train,     X_test=X_test,     num_gfr=10,     num_mcmc=10,     mean_forest_params=param_dict, ) <p>Inspect the MCMC (BART) samples</p> In\u00a0[5]: Copied! <pre>forest_preds_y_mcmc = bart_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True)\ny_df_mcmc = pd.DataFrame(\n    np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),\n    columns=[\"True outcome\", \"Average estimated outcome\"],\n)\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bart_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis=1, keepdims=True) y_df_mcmc = pd.DataFrame(     np.concatenate((np.expand_dims(y_test, 1), y_avg_mcmc), axis=1),     columns=[\"True outcome\", \"Average estimated outcome\"], ) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3))) plt.show() In\u00a0[6]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(\n    np.concatenate(\n        (\n            np.expand_dims(np.arange(bart_model.num_samples), axis=1),\n            np.expand_dims(bart_model.global_var_samples, axis=1),\n        ),\n        axis=1,\n    ),\n    columns=[\"Sample\", \"Sigma\"],\n)\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(     np.concatenate(         (             np.expand_dims(np.arange(bart_model.num_samples), axis=1),             np.expand_dims(bart_model.global_var_samples, axis=1),         ),         axis=1,     ),     columns=[\"Sample\", \"Sigma\"], ) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\") plt.show() <p>Compute the test set RMSE</p> In\u00a0[7]: Copied! <pre>np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2)))\n</pre> np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc), 2))) Out[7]: <pre>np.float64(1.6095700739648515)</pre> <p>Check the variable split count in the last \"GFR\" sample</p> In\u00a0[8]: Copied! <pre>bart_model.forest_container_mean.get_forest_split_counts(9, p_X)\n</pre> bart_model.forest_container_mean.get_forest_split_counts(9, p_X) Out[8]: <pre>array([28, 30, 23, 15, 18, 24, 29, 21, 20, 36], dtype=int32)</pre> In\u00a0[9]: Copied! <pre>bart_model.forest_container_mean.get_overall_split_counts(p_X)\n</pre> bart_model.forest_container_mean.get_overall_split_counts(p_X) Out[9]: <pre>array([264, 298, 265, 192, 204, 229, 331, 230, 232, 350], dtype=int32)</pre> <p>The split counts appear relatively uniform across features, so let's dig deeper and look at individual trees, starting with the first tree in the last \"grow-from-root\" sample.</p> In\u00a0[10]: Copied! <pre>splits = bart_model.forest_container_mean.get_granular_split_counts(p_X)\n</pre> splits = bart_model.forest_container_mean.get_granular_split_counts(p_X) In\u00a0[11]: Copied! <pre>splits[9, 0, :]\n</pre> splits[9, 0, :] Out[11]: <pre>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int32)</pre> <p>This tree has a single split on the only \"important\" feature. Now, let's look at the second tree.</p> In\u00a0[12]: Copied! <pre>splits[9, 1, :]\n</pre> splits[9, 1, :] Out[12]: <pre>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 3], dtype=int32)</pre> <p>This tree also only splits on the important feature.</p> In\u00a0[13]: Copied! <pre>splits[9, 20, :]\n</pre> splits[9, 20, :] Out[13]: <pre>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int32)</pre> In\u00a0[14]: Copied! <pre>splits[9, 30, :]\n</pre> splits[9, 30, :] Out[14]: <pre>array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)</pre> <p>We see that \"later\" trees are splitting on other features, but we also note that these trees are fitting an outcome that is already residualized many \"relevant splits\" made by trees 1 and 2.</p> <p>Now, let's inspect the first tree for this last GFR sample in more depth, following this scikit-learn vignette.</p> In\u00a0[15]: Copied! <pre>forest_num = 9\ntree_num = 0\n</pre> forest_num = 9 tree_num = 0 In\u00a0[16]: Copied! <pre>nodes = np.sort(bart_model.forest_container_mean.nodes(forest_num, tree_num))\nfor nid in nodes:\n    if bart_model.forest_container_mean.is_leaf_node(forest_num, tree_num, nid):\n        print(\n            \"{space}node={node} is a leaf node with value={value}.\".format(\n                space=bart_model.forest_container_mean.node_depth(\n                    forest_num, tree_num, nid\n                )\n                * \"\\t\",\n                node=nid,\n                value=np.around(\n                    bart_model.forest_container_mean.node_leaf_values(\n                        forest_num, tree_num, nid\n                    ),\n                    3,\n                ),\n            )\n        )\n    else:\n        print(\n            \"{space}node={node} is a split node, which tells us to \"\n            \"go to node {left} if X[:, {feature}] &lt;= {threshold} \"\n            \"else to node {right}.\".format(\n                space=bart_model.forest_container_mean.node_depth(\n                    forest_num, tree_num, nid\n                )\n                * \"\\t\",\n                node=nid,\n                left=bart_model.forest_container_mean.left_child_node(\n                    forest_num, tree_num, nid\n                ),\n                feature=bart_model.forest_container_mean.node_split_index(\n                    forest_num, tree_num, nid\n                ),\n                threshold=bart_model.forest_container_mean.node_split_threshold(\n                    forest_num, tree_num, nid\n                ),\n                right=bart_model.forest_container_mean.right_child_node(\n                    forest_num, tree_num, nid\n                ),\n            )\n        )\n</pre> nodes = np.sort(bart_model.forest_container_mean.nodes(forest_num, tree_num)) for nid in nodes:     if bart_model.forest_container_mean.is_leaf_node(forest_num, tree_num, nid):         print(             \"{space}node={node} is a leaf node with value={value}.\".format(                 space=bart_model.forest_container_mean.node_depth(                     forest_num, tree_num, nid                 )                 * \"\\t\",                 node=nid,                 value=np.around(                     bart_model.forest_container_mean.node_leaf_values(                         forest_num, tree_num, nid                     ),                     3,                 ),             )         )     else:         print(             \"{space}node={node} is a split node, which tells us to \"             \"go to node {left} if X[:, {feature}] &lt;= {threshold} \"             \"else to node {right}.\".format(                 space=bart_model.forest_container_mean.node_depth(                     forest_num, tree_num, nid                 )                 * \"\\t\",                 node=nid,                 left=bart_model.forest_container_mean.left_child_node(                     forest_num, tree_num, nid                 ),                 feature=bart_model.forest_container_mean.node_split_index(                     forest_num, tree_num, nid                 ),                 threshold=bart_model.forest_container_mean.node_split_threshold(                     forest_num, tree_num, nid                 ),                 right=bart_model.forest_container_mean.right_child_node(                     forest_num, tree_num, nid                 ),             )         ) <pre>node=0 is a split node, which tells us to go to node 1 if X[:, 9] &lt;= 0.46012932980693555 else to node 2.\n\tnode=1 is a leaf node with value=[0.002].\n\tnode=2 is a leaf node with value=[0.221].\n</pre>"},{"location":"python_docs/demo/tree_inspection.html#internal-tree-inspection","title":"Internal Tree Inspection\u00b6","text":"<p>While out of sample evaluation and MCMC diagnostics on parametric BART components (i.e. $\\sigma^2$, the global error variance) are helpful, it's important to be able to inspect the trees in a BART / BCF model (or a custom tree ensemble model). This vignette walks through some of the features <code>stochtree</code> provides to query and understand the forests / trees in a model.</p>"},{"location":"python_docs/demo/tree_inspection.html#demo-1-supervised-learning","title":"Demo 1: Supervised Learning\u00b6","text":""},{"location":"vignettes/index.html","title":"Advanced StochTree Vignettes","text":"<p>While the R and Python package documentation contains ample documentation and examples,  <code>stochtree</code>'s origin in the world of academic decision tree research means it is  actively used to develop novel algorithms and applications.  This section includes in-depth tutorials on how to implement new methods using stochtree,  with an R and Python version for each vignette.</p> <p>Current advanced vignettes include:</p> <ol> <li>Using <code>stochtree</code> for Regression Discontinuity Design (R, Python)</li> <li>Using <code>stochtree</code> for Instrumental Variables Analysis (R, Python)</li> </ol>"}]}