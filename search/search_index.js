var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"StochTree","text":"<p><code>stochtree</code> (stochastic tree) is software for building stochastic tree ensembles (i.e. BART, XBART) for supervised learning and causal inference.</p>"},{"location":"getting-started.html","title":"Getting Started","text":"<p><code>stochtree</code> is composed of a C++ \"core\" and R / Python interfaces to that core. Details on installation and use are available below:</p>"},{"location":"getting-started.html#python-package","title":"Python Package","text":"<p>The python package is not yet on PyPI but can be installed from source using pip's git interface.  To proceed, you will need a working version of git and python 3.8 or greater (available from several sources, one of the most  straightforward being the anaconda suite).</p>"},{"location":"getting-started.html#quick-start","title":"Quick start","text":"<p>Without worrying about virtual environments (detailed further below), <code>stochtree</code> can be installed from the command line</p> <pre><code>pip install numpy scipy pytest pandas scikit-learn pybind11\npip install git+https://github.com/StochasticTree/stochtree.git\n</code></pre>"},{"location":"getting-started.html#virtual-environment-installation","title":"Virtual environment installation","text":"<p>Often, users prefer to manage different projects (with different package / python version requirements) in virtual environments. </p>"},{"location":"getting-started.html#conda","title":"Conda","text":"<p>Conda provides a straightforward experience in managing python dependencies, avoiding version conflicts / ABI issues / etc.</p> <p>To build stochtree using a <code>conda</code> based workflow, first create and activate a conda environment with the requisite dependencies</p> <pre><code>conda create -n stochtree-dev -c conda-forge python=3.10 numpy scipy pytest pandas pybind11 scikit-learn matplotlib seaborn\nconda activate stochtree-dev\n</code></pre> <p>Then install the package from github via pip</p> <pre><code>pip install git+https://github.com/StochasticTree/stochtree.git\n</code></pre> <p>(Note: if you'd also like to run <code>stochtree</code>'s notebook examples, you will also need jupyterlab, seaborn, and matplotlib)</p> <pre><code>conda install matplotlib seaborn\npip install jupyterlab\n</code></pre> <p>With these dependencies installed, you can clone the repo and run the <code>demo/</code> examples.</p>"},{"location":"getting-started.html#venv","title":"Venv","text":"<p>You could also use venv for environment management. First, navigate to the folder in which you usually store virtual environments  (i.e. <code>cd /path/to/envs</code>) and create and activate a virtual environment:</p> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Install all of the package (and demo notebook) dependencies</p> <pre><code>pip install numpy scipy pytest pandas scikit-learn pybind11\n</code></pre> <p>Then install stochtree via</p> <pre><code>pip install git+https://github.com/StochasticTree/stochtree.git\n</code></pre> <p>As above, if you'd like to run the notebook examples in the <code>demo/</code> subfolder, you will also need jupyterlab, seaborn, and matplotlib and you will have to clone the repo</p> <pre><code>pip install matplotlib seaborn jupyterlab\n</code></pre>"},{"location":"getting-started.html#r-package","title":"R Package","text":"<p>The package can be installed in R via</p> <pre><code>remotes::install_github(\"StochasticTree/stochtree\", ref=\"r-dev\")\n</code></pre>"},{"location":"getting-started.html#c-core","title":"C++ Core","text":"<p>While the C++ core links to both R and Python for a performant, high-level interface,  the C++ code can be compiled and unit-tested and compiled into a standalone  debug program.</p>"},{"location":"getting-started.html#compilation","title":"Compilation","text":""},{"location":"getting-started.html#cloning-the-repository","title":"Cloning the Repository","text":"<p>To clone the repository, you must have git installed, which you can do following these instructions. </p> <p>Once git is available at the command line, navigate to the folder that will store this project (in bash / zsh, this is done by running <code>cd</code> followed by the path to the directory).  Then, clone the <code>stochtree</code> repo as a subfolder by running</p> <pre><code>git clone --recursive https://github.com/StochasticTree/stochtree.git\n</code></pre> <p>NOTE: this project incorporates several dependencies as git submodules,  which is why the <code>--recursive</code> flag is necessary (some systems may perform a recursive clone without this flag, but  <code>--recursive</code> ensures this behavior on all platforms). If you have already cloned the repo without the <code>--recursive</code> flag,  you can retrieve the submodules recursively by running <code>git submodule update --init --recursive</code> in the main repo directory.</p>"},{"location":"getting-started.html#cmake-build","title":"CMake Build","text":"<p>The C++ project can be built independently from the R / Python packages using <code>cmake</code>.  See here for details on installing cmake (alternatively,  on MacOS, <code>cmake</code> can be installed using homebrew). Once <code>cmake</code> is installed, you can build the CLI by navigating to the main  project directory at your command line (i.e. <code>cd /path/to/stochtree</code>) and  running the following code </p> <pre><code>rm -rf build\nmkdir build\ncmake -S . -B build\ncmake --build build\n</code></pre> <p>The CMake build has two primary targets, which are detailed below</p>"},{"location":"getting-started.html#debug-program","title":"Debug Program","text":"<p><code>debug/api_debug.cpp</code> defines a standalone target that can be straightforwardly run with a debugger (i.e. <code>lldb</code>, <code>gdb</code>)  while making non-trivial changes to the C++ code. This debugging program is compiled as part of the CMake build if the <code>BUILD_DEBUG_TARGETS</code> option in <code>CMakeLists.txt</code> is set to <code>ON</code>.</p> <p>Once the program has been built, it can be run from the command line via <code>./build/debugstochtree</code> or attached to a debugger  via <code>lldb ./build/debugstochtree</code> (clang) or <code>gdb ./build/debugstochtree</code> (gcc).</p>"},{"location":"getting-started.html#unit-tests","title":"Unit Tests","text":"<p>We test <code>stochtree</code> using the GoogleTest framework. Unit tests are compiled into a single target as part of the CMake build if the <code>BUILD_TEST</code> option is set to <code>ON</code>  and the test suite can be run after compilation via <code>./build/teststochtree</code></p>"},{"location":"getting-started.html#xcode","title":"Xcode","text":"<p>While using <code>gdb</code> or <code>lldb</code> on <code>debugstochtree</code> at the command line is very helpful, users may prefer debugging in a full-fledged IDE like xcode. This project's C++ core can be converted to an xcode project from <code>CMakeLists.txt</code>, but first you must turn off sanitizers (xcode seems to have its own way of setting this at build time for different configurations, and having injected  <code>-fsanitize=address</code> statically into compiler arguments will cause xcode errors). To do this, modify the <code>USE_SANITIZER</code> line in <code>CMakeLists.txt</code>:</p> <pre><code>option(USE_SANITIZER \"Use santizer flags\" OFF)\n</code></pre> <p>To generate an XCode project based on the build targets and specifications defined in a <code>CMakeLists.txt</code>, navigate to the main project folder (i.e. <code>cd /path/to/project</code>) and run the following commands:</p> <pre><code>rm -rf xcode/\nmkdir xcode\ncd xcode\ncmake -G Xcode .. -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=c++ -DUSE_SANITIZER=OFF -DUSE_DEBUG=OFF\ncd ..\n</code></pre> <p>Now, if you navigate to the xcode subfolder (in Finder), you should be able to click on a <code>.xcodeproj</code> file and the project will open in XCode.</p>"},{"location":"cpp_docs/index.html","title":"StochTree C++ API","text":"<p>This page documents the data structures and interfaces that constitute the <code>stochtree</code> C++ core. </p> <p>It may be useful to researchers building novel tree algorithms or users seeking a deeper understanding of the algorithms implemented in <code>stochtree</code>.</p>"},{"location":"cpp_docs/tracking.html","title":"Forest Sampling Tracker API","text":"<p>A truly minimalist tree ensemble library only needs </p> <ul> <li>A representation of a decision tree</li> <li>A container for grouping / storing ensembles of trees</li> <li>In-memory access to / representation of training data</li> <li>Routines / functions to construct the trees</li> </ul> <p>Most algorithms for optimizing or sampling tree ensembles frequently perform the following operations</p> <ul> <li>Determine which leaf a training observation falls into for a decision tree (to compute its prediction and update the residual / outcome)</li> <li>Evaluate potential split candidates for a leaf of a decision</li> </ul> <p>With only the \"minimalist\" tools above, these two tasks proceed largely as follows</p> <ul> <li>For every observation in the dataset, traverse the tree (runtime depends on the tree topology but in a fully balanced tree with \\(k\\) leaf nodes, this has time complexity \\(O(N \\log (k))\\)).</li> <li>For a given node, determine which observations in the training set fall into this node. This requires \\(O(N)\\) boolean operations.</li> </ul> <p>These operations both perform unnecessary computation which can be avoided with some additional real-time tracking. Essentially, we want </p> <ol> <li>A mapping from dataset row index to leaf node id for every tree in an ensemble (so that we can skip the tree traversal during prediction)</li> <li>A mapping from leaf node id to dataset row indices every tree in an ensemble (so that we can skip the full pass through the training data at split evaluation)</li> </ol>"},{"location":"cpp_docs/tracking.html#forest-tracker","title":"Forest Tracker","text":"<p>The <code>ForestTracker</code> class is a wrapper around several implementations of the mappings discussed above. </p>"},{"location":"cpp_docs/tree.html","title":"Decision Tree API","text":""},{"location":"cpp_docs/tree.html#tree","title":"Tree","text":"<p>The fundamental building block of the C++ tree interface is the <code>Tree</code> class. </p>"},{"location":"cpp_docs/tree.html#tree-split","title":"Tree Split","text":"<p>Numeric and categorical splits are represented by a <code>TreeSplit</code> class.</p>"},{"location":"python_docs/index.html","title":"StochTree Python Library","text":"<p>The stochtree python library provides two essential components:</p> <ol> <li>High-level functionality for sampling, predicting, and serializing BART and BCF models</li> <li>Lower-level definition and control of a stochastic tree sampler</li> </ol>"},{"location":"python_docs/api/index.html","title":"StochTree Python API Reference","text":"<p>Overview of the <code>stochtree</code> python library's key classes and functions</p>"},{"location":"python_docs/api/bart.html","title":"BART","text":""},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel","title":"<code>stochtree.bart.BARTModel</code>","text":"<p>Class that handles sampling, storage, and serialization of stochastic forest models for supervised learning.  The class takes its name from Bayesian Additive Regression Trees, an MCMC sampler originally developed in  Chipman, George, McCulloch (2010), but supports several sampling algorithms:</p> <ul> <li>MCMC: The \"classic\" sampler defined in Chipman, George, McCulloch (2010). In order to run the MCMC sampler, set <code>num_gfr = 0</code> (explained below) and then define a sampler according to several parameters:<ul> <li><code>num_burnin</code>: the number of iterations to run before \"retaining\" samples for further analysis. These \"burned in\" samples are helpful for allowing a sampler to converge before retaining samples.</li> <li><code>num_chains</code>: the number of independent sequences of MCMC samples to generate (typically referred to in the literature as \"chains\")</li> <li><code>num_mcmc</code>: the number of \"retained\" samples of the posterior distribution</li> <li><code>keep_every</code>: after a sampler has \"burned in\", we will run the sampler for <code>keep_every</code> * <code>num_mcmc</code> iterations, retaining one of each <code>keep_every</code> iteration in a chain.</li> </ul> </li> <li>GFR (Grow-From-Root): A fast, greedy approximation of the BART MCMC sampling algorithm introduced in He and Hahn (2021). GFR sampler iterations are governed by the <code>num_gfr</code> parameter, and there are two primary ways to use this sampler:<ul> <li>Standalone: setting <code>num_gfr &gt; 0</code> and both <code>num_burnin = 0</code> and <code>num_mcmc = 0</code> will only run and retain GFR samples of the posterior. This is typically referred to as \"XBART\" (accelerated BART).</li> <li>Initializer for MCMC: setting <code>num_gfr &gt; 0</code> and <code>num_mcmc &gt; 0</code> will use ensembles from the GFR algorithm to initialize <code>num_chains</code> independent MCMC BART samplers, which are run for <code>num_mcmc</code> iterations. This is typically referred to as \"warm start BART\".</li> </ul> </li> </ul> <p>In addition to enabling multiple samplers, we support a broad set of models. First, note that the original BART model of Chipman, George, McCulloch (2010) is</p> \\[\\begin{equation*} \\begin{aligned} y &amp;= f(X) + \\epsilon\\\\ f(X) &amp;\\sim \\text{BART}(\\cdot)\\\\ \\epsilon &amp;\\sim N(0, \\sigma^2)\\\\ \\sigma^2 &amp;\\sim IG(\\nu, \\nu\\lambda) \\end{aligned} \\end{equation*}\\] <p>In words, there is a nonparametric mean function governed by a tree ensemble with a BART prior and an additive (mean-zero) Gaussian error  term, whose variance is parameterized with an inverse gamma prior.</p> <p>The <code>BARTModel</code> class supports the following extensions of this model:</p> <ul> <li>Leaf Regression: Rather than letting <code>f(X)</code> define a standard decision tree ensemble, in which each tree uses <code>X</code> to partition the data and then serve up constant predictions, we allow for models <code>f(X,Z)</code> in which <code>X</code> and <code>Z</code> together define a partitioned linear model (<code>X</code> partitions the data and <code>Z</code> serves as the basis for regression models). This model can be run by specifying <code>basis_train</code> in the <code>sample</code> method.</li> <li>Heteroskedasticity: Rather than define \\(\\epsilon\\) parameterically, we can let a forest \\(\\sigma^2(X)\\) model a conditional error variance function. This can be done by setting <code>num_trees_variance &gt; 0</code> in the <code>params</code> dictionary passed to the <code>sample</code> method.</li> </ul>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.sample","title":"<code>sample(X_train, y_train, basis_train=None, X_test=None, basis_test=None, num_gfr=5, num_burnin=0, num_mcmc=100, params=None)</code>","text":"<p>Runs a BART sampler on provided training set. Predictions will be cached for the training set and (if provided) the test set.  Does not require a leaf regression basis. </p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>`np.array`</code> <p>Training set covariates on which trees may be partitioned.</p> required <code>y_train</code> <code>`np.array`</code> <p>Training set outcome.</p> required <code>basis_train</code> <code>`np.array`</code> <p>Optional training set basis vector used to define a regression to be run in the leaves of each tree.</p> <code>None</code> <code>X_test</code> <code>`np.array`</code> <p>Optional test set covariates.</p> <code>None</code> <code>basis_test</code> <code>`np.array`</code> <p>Optional test set basis vector used to define a regression to be run in the leaves of each tree.  Must be included / omitted consistently (i.e. if basis_train is provided, then basis_test must be provided alongside X_test).</p> <code>None</code> <code>num_gfr</code> <code>`int`</code> <p>Number of \"warm-start\" iterations run using the grow-from-root algorithm (He and Hahn, 2021). Defaults to <code>5</code>.</p> <code>5</code> <code>num_burnin</code> <code>`int`</code> <p>Number of \"burn-in\" iterations of the MCMC sampler. Defaults to <code>0</code>. Ignored if <code>num_gfr &gt; 0</code>.</p> <code>0</code> <code>num_mcmc</code> <code>`int`</code> <p>Number of \"retained\" iterations of the MCMC sampler. Defaults to <code>100</code>. If this is set to 0, GFR (XBART) samples will be retained.</p> <code>100</code> <code>params</code> <code>`dict`</code> <p>Dictionary of model parameters, each of which has a default value.</p> <ul> <li><code>cutpoint_grid_size</code> (<code>int</code>): Maximum number of cutpoints to consider for each feature. Defaults to <code>100</code>.</li> <li><code>sigma_leaf</code> (<code>float</code>): Scale parameter on the (conditional mean) leaf node regression model.</li> <li><code>alpha_mean</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 in the conditional mean model. Tree split prior combines <code>alpha_mean</code> and <code>beta_mean</code> via <code>alpha_mean*(1+node_depth)^-beta_mean</code>.</li> <li><code>beta_mean</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 in the conditional mean model. Tree split prior combines <code>alpha_mean</code> and <code>beta_mean</code> via <code>alpha_mean*(1+node_depth)^-beta_mean</code>.</li> <li><code>min_samples_leaf_mean</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples, in the conditional mean model. Defaults to <code>5</code>.</li> <li><code>max_depth_mean</code> (<code>int</code>): Maximum depth of any tree in the ensemble in the conditional mean model. Defaults to <code>10</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>alpha_variance</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 in the conditional variance model. Tree split prior combines <code>alpha_variance</code> and <code>beta_variance</code> via <code>alpha_variance*(1+node_depth)^-beta_variance</code>.</li> <li><code>beta_variance</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 in the conditional variance model. Tree split prior combines <code>alpha_variance</code> and <code>beta_variance</code> via <code>alpha_variance*(1+node_depth)^-beta_variance</code>.</li> <li><code>min_samples_leaf_variance</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples in the conditional variance model. Defaults to <code>5</code>.</li> <li><code>max_depth_variance</code> (<code>int</code>): Maximum depth of any tree in the ensemble in the conditional variance model. Defaults to <code>10</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>a_global</code> (<code>float</code>): Shape parameter in the <code>IG(a_global, b_global)</code> global error variance model. Defaults to <code>0</code>.</li> <li><code>b_global</code> (<code>float</code>): Scale parameter in the <code>IG(a_global, b_global)</code> global error variance prior. Defaults to <code>0</code>.</li> <li><code>a_leaf</code> (<code>float</code>): Shape parameter in the <code>IG(a_leaf, b_leaf)</code> leaf node parameter variance model. Defaults to <code>3</code>.</li> <li><code>b_leaf</code> (<code>float</code>): Scale parameter in the <code>IG(a_leaf, b_leaf)</code> leaf node parameter variance model. Calibrated internally as <code>0.5/num_trees_mean</code> if not set here.</li> <li><code>a_forest</code> (<code>float</code>): Shape parameter in the [optional] <code>IG(a_forest, b_forest)</code> conditional error variance forest (which is only sampled if <code>num_trees_variance &gt; 0</code>). Calibrated internally as <code>num_trees_variance / 1.5^2 + 0.5</code> if not set here.</li> <li><code>b_forest</code> (<code>float</code>): Scale parameter in the [optional] <code>IG(a_forest, b_forest)</code> conditional error variance forest (which is only sampled if <code>num_trees_variance &gt; 0</code>). Calibrated internally as <code>num_trees_variance / 1.5^2</code> if not set here.</li> <li><code>sigma2_init</code> (<code>float</code>): Starting value of global variance parameter. Set internally as a percentage of the standardized outcome variance if not set here.</li> <li><code>variance_forest_leaf_init</code> (<code>float</code>): Starting value of root forest prediction in conditional (heteroskedastic) error variance model. Calibrated internally as <code>np.log(pct_var_variance_forest_init*np.var((y-np.mean(y))/np.std(y)))/num_trees_variance</code> if not set.</li> <li><code>pct_var_sigma2_init</code> (<code>float</code>): Percentage of standardized outcome variance used to initialize global error variance parameter. Superseded by <code>sigma2</code>. Defaults to <code>1</code>.</li> <li><code>pct_var_variance_forest_init</code> (<code>float</code>): Percentage of standardized outcome variance used to initialize global error variance parameter. Default: <code>1</code>. Superseded by <code>variance_forest_init</code>.</li> <li><code>variance_scale</code> (<code>float</code>): Variance after the data have been scaled. Default: <code>1</code>.</li> <li><code>variable_weights_mean</code> (<code>np.array</code>): Numeric weights reflecting the relative probability of splitting on each variable in the mean forest. Does not need to sum to 1 but cannot be negative. Defaults to uniform over the columns of <code>X_train</code> if not provided.</li> <li><code>variable_weights_variance</code> (<code>np.array</code>): Numeric weights reflecting the relative probability of splitting on each variable in the variance forest. Does not need to sum to 1 but cannot be negative. Defaults to uniform over the columns of <code>X_train</code> if not provided.</li> <li><code>num_trees_mean</code> (<code>int</code>): Number of trees in the ensemble for the conditional mean model. Defaults to <code>200</code>. If <code>num_trees_mean = 0</code>, the conditional mean will not be modeled using a forest and the function will only proceed if <code>num_trees_variance &gt; 0</code>.</li> <li><code>num_trees_variance</code> (<code>int</code>): Number of trees in the ensemble for the conditional variance model. Defaults to <code>0</code>. Variance is only modeled using a tree / forest if <code>num_trees_variance &gt; 0</code>.</li> <li><code>sample_sigma_global</code> (<code>bool</code>): Whether or not to update the <code>sigma^2</code> global error variance parameter based on <code>IG(a_global, b_global)</code>. Defaults to <code>True</code>.</li> <li><code>sample_sigma_leaf</code> (<code>bool</code>): Whether or not to update the <code>tau</code> leaf scale variance parameter based on <code>IG(a_leaf, b_leaf)</code>. Cannot (currently) be set to true if <code>basis_train</code> has more than one column. Defaults to <code>False</code>.</li> <li><code>random_seed</code> (<code>int</code>): Integer parameterizing the C++ random number generator. If not specified, the C++ random number generator is seeded according to <code>std::random_device</code>.</li> <li><code>keep_burnin</code> (<code>bool</code>): Whether or not \"burnin\" samples should be included in predictions. Defaults to <code>False</code>. Ignored if <code>num_mcmc == 0</code>.</li> <li><code>keep_gfr</code> (<code>bool</code>): Whether or not \"warm-start\" / grow-from-root samples should be included in predictions. Defaults to <code>False</code>. Ignored if <code>num_mcmc == 0</code>.</li> <li><code>num_chains</code> (<code>int</code>): How many independent MCMC chains should be sampled. If <code>num_mcmc = 0</code>, this is ignored. If <code>num_gfr = 0</code>, then each chain is run from root for <code>num_mcmc * keep_every + num_burnin</code> iterations, with <code>num_mcmc</code> samples retained. If <code>num_gfr &gt; 0</code>, each MCMC chain will be initialized from a separate GFR ensemble, with the requirement that <code>num_gfr &gt;= num_chains</code>. Default: <code>1</code>.</li> <li><code>keep_every</code> (<code>int</code>): How many iterations of the burned-in MCMC sampler should be run before forests and parameters are retained. Defaults to <code>1</code>. Setting <code>keep_every = k</code> for some <code>k &gt; 1</code> will \"thin\" the MCMC samples by retaining every <code>k</code>-th sample, rather than simply every sample. This can reduce the autocorrelation of the MCMC samples.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>BARTModel</code> <p>Sampled BART Model.</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.predict","title":"<code>predict(covariates, basis=None)</code>","text":"<p>Return predictions from every forest sampled (either / both of mean and variance)</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>`np.array`</code> <p>Test set covariates.</p> required <code>basis</code> <code>`np.array`</code> <p>Optional test set basis vector, must be provided if the model was trained with a leaf regression basis.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple of `np.array`</code> <p>Tuple of arrays of predictions corresponding to each forest (mean and variance, depending on whether either / both was included). Each array will contain as many rows as in <code>covariates</code> and as many columns as retained samples of the algorithm.</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.predict_mean","title":"<code>predict_mean(covariates, basis=None)</code>","text":"<p>Predict expected conditional outcome from a BART model.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>`np.array`</code> <p>Test set covariates.</p> required <code>basis</code> <code>`np.array`</code> <p>Optional test set basis vector, must be provided if the model was trained with a leaf regression basis.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple of `np.array`</code> <p>Tuple of arrays of predictions corresponding to each forest (mean and variance, depending on whether either / both was included). Each array will contain as many rows as in <code>covariates</code> and as many columns as retained samples of the algorithm.</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.predict_variance","title":"<code>predict_variance(covariates)</code>","text":"<p>Predict expected conditional variance from a BART model.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array</code> <p>Test set covariates.</p> required <p>Returns:</p> Type Description <code>tuple of `np.array`</code> <p>Tuple of arrays of predictions corresponding to the variance forest. Each array will contain as many rows as in <code>covariates</code> and as many columns as retained samples of the algorithm.</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.to_json","title":"<code>to_json()</code>","text":"<p>Converts a sampled BART model to JSON string representation (which can then be saved to a file or  processed using the <code>json</code> library)</p> <p>Returns:</p> Type Description <code>`str`</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p>"},{"location":"python_docs/api/bart.html#stochtree.bart.BARTModel.from_json","title":"<code>from_json(json_string)</code>","text":"<p>Converts a JSON string to an in-memory BART model.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>`str`</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p> required"},{"location":"python_docs/api/bcf.html","title":"BCF","text":""},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel","title":"<code>stochtree.bcf.BCFModel</code>","text":"<p>Class that handles sampling, storage, and serialization of stochastic forest models for causal effect estimation.  The class takes its name from Bayesian Causal Forests, an MCMC sampler originally developed in  Hahn, Murray, Carvalho (2020), but supports several sampling algorithms:</p> <ul> <li>MCMC: The \"classic\" sampler defined in Hahn, Murray, Carvalho (2020). In order to run the MCMC sampler,  set <code>num_gfr = 0</code> (explained below) and then define a sampler according to several parameters:<ul> <li><code>num_burnin</code>: the number of iterations to run before \"retaining\" samples for further analysis. These \"burned in\" samples  are helpful for allowing a sampler to converge before retaining samples.</li> <li><code>num_chains</code>: the number of independent sequences of MCMC samples to generate (typically referred to in the literature as \"chains\")</li> <li><code>num_mcmc</code>: the number of \"retained\" samples of the posterior distribution</li> <li><code>keep_every</code>: after a sampler has \"burned in\", we will run the sampler for <code>keep_every</code> * <code>num_mcmc</code> iterations, retaining one of each <code>keep_every</code> iteration in a chain.</li> </ul> </li> <li>GFR (Grow-From-Root): A fast, greedy approximation of the BART MCMC sampling algorithm introduced in Krantsevich, He, and Hahn (2023). GFR sampler iterations are  governed by the <code>num_gfr</code> parameter, and there are two primary ways to use this sampler:<ul> <li>Standalone: setting <code>num_gfr &gt; 0</code> and both <code>num_burnin = 0</code> and <code>num_mcmc = 0</code> will only run and retain GFR samples of the posterior. This is typically referred to as \"XBART\" (accelerated BART).</li> <li>Initializer for MCMC: setting <code>num_gfr &gt; 0</code> and <code>num_mcmc &gt; 0</code> will use ensembles from the GFR algorithm to initialize <code>num_chains</code> independent MCMC BART samplers, which are run for <code>num_mcmc</code> iterations.  This is typically referred to as \"warm start BART\".</li> </ul> </li> </ul> <p>In addition to enabling multiple samplers, we support a broad set of models. First, note that the original BCF model of Hahn, Murray, Carvalho (2020) is</p> \\[\\begin{equation*} \\begin{aligned} y &amp;= \\mu(X) + b_z(X) + \\epsilon\\\\ b_z(X) &amp;= (b_1 Z + b_0 (1-Z)) \\tau(X)\\\\ b_0, b_1 &amp;\\sim N(0, \\frac{1}{2})\\\\\\\\ \\mu(X) &amp;\\sim \\text{BART}(\\cdot)\\\\ \\tau(X) &amp;\\sim \\text{BART}(\\cdot)\\\\ \\epsilon &amp;\\sim N(0, \\sigma^2)\\\\ \\sigma^2 &amp;\\sim IG(\\nu, \\nu\\lambda) \\end{aligned} \\end{equation*}\\] <p>for continuous outcome \\(y\\), binary treatment \\(Z\\), and covariates \\(X\\).</p> <p>In words, there are two nonparametric mean functions -- a \"prognostic\" function and a treatment effect function -- governed by tree ensembles with BART priors and an additive (mean-zero) Gaussian error  term, whose variance is parameterized with an inverse gamma prior.</p> <p>The <code>BCFModel</code> class supports the following extensions of this model:</p> <ul> <li>Continuous Treatment: If \\(Z\\) is continuous rather than binary, we define \\(b_z(X) &amp; = \\tau(X, Z) = Z \\tau(X)\\), where the \"leaf model\" for the \\(\\tau\\) forest is essentially a regression on continuous \\(Z\\).</li> <li>Heteroskedasticity: Rather than define \\(\\epsilon\\) parameterically, we can let a forest \\(\\sigma^2(X)\\) model a conditional error variance function. This can be done by setting <code>num_trees_variance &gt; 0</code> in the <code>params</code> dictionary passed to the <code>sample</code> method.</li> </ul>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.sample","title":"<code>sample(X_train, Z_train, y_train, pi_train=None, X_test=None, Z_test=None, pi_test=None, num_gfr=5, num_burnin=0, num_mcmc=100, params=None)</code>","text":"<p>Runs a BCF sampler on provided training set. Outcome predictions and estimates of the prognostic and treatment effect functions  will be cached for the training set and (if provided) the test set.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>`np.array` or `pd.DataFrame`</code> <p>Covariates used to split trees in the ensemble. Can be passed as either a matrix or dataframe.</p> required <code>Z_train</code> <code>`np.array`</code> <p>Array of (continuous or binary; univariate or multivariate) treatment assignments.</p> required <code>y_train</code> <code>`np.array`</code> <p>Outcome to be modeled by the ensemble.</p> required <code>pi_train</code> <code>`np.array`</code> <p>Optional vector of propensity scores. If not provided, this will be estimated from the data.</p> <code>None</code> <code>X_test</code> <code>`np.array`</code> <p>Optional test set of covariates used to define \"out of sample\" evaluation data.</p> <code>None</code> <code>Z_test</code> <code>`np.array`</code> <p>Optional test set of (continuous or binary) treatment assignments. Must be provided if <code>X_test</code> is provided.</p> <code>None</code> <code>pi_test</code> <code>`np.array`</code> <p>Optional test set vector of propensity scores. If not provided (but <code>X_test</code> and <code>Z_test</code> are), this will be estimated from the data.</p> <code>None</code> <code>num_gfr</code> <code>`int`</code> <p>Number of \"warm-start\" iterations run using the grow-from-root algorithm (He and Hahn, 2021). Defaults to <code>5</code>.</p> <code>5</code> <code>num_burnin</code> <code>`int`</code> <p>Number of \"burn-in\" iterations of the MCMC sampler. Defaults to <code>0</code>. Ignored if <code>num_gfr &gt; 0</code>.</p> <code>0</code> <code>num_mcmc</code> <code>`int`</code> <p>Number of \"retained\" iterations of the MCMC sampler. Defaults to <code>100</code>. If this is set to 0, GFR (XBART) samples will be retained.</p> <code>100</code> <code>params</code> <code>`dict`</code> <p>Dictionary of model parameters, each of which has a default value.</p> <ul> <li><code>cutpoint_grid_size</code> (<code>int</code>): Maximum number of cutpoints to consider for each feature. Defaults to <code>100</code>.</li> <li><code>sigma_leaf_mu</code> (<code>float</code>): Starting value of leaf node scale parameter for the prognostic forest. Calibrated internally as <code>2/num_trees_mu</code> if not set here.</li> <li><code>sigma_leaf_tau</code> (<code>float</code> or <code>np.array</code>): Starting value of leaf node scale parameter for the treatment effect forest.      When treatment (<code>Z_train</code>) is multivariate, this can be either a <code>float</code> or a square 2-dimensional <code>np.array</code>      with <code>sigma_leaf_tau.shape[0] == Z_train.shape[1]</code> and <code>sigma_leaf_tau.shape[1] == Z_train.shape[1]</code>.     If <code>sigma_leaf_tau</code> is provided as a float for multivariate treatment, the leaf scale term will be set as a      diagonal matrix with <code>sigma_leaf_tau</code> on every diagonal. If not passed as an argument, this parameter is      calibrated internally as <code>1/num_trees_tau</code> (and propagated to a diagonal matrix if necessary).</li> <li><code>alpha_mu</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 for the prognostic forest.      Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>.</li> <li><code>alpha_tau</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 for the treatment effect forest.      Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>.</li> <li><code>alpha_variance</code> (<code>float</code>): Prior probability of splitting for a tree of depth 0 in the conditional variance model. Tree split prior combines <code>alpha_variance</code> and <code>beta_variance</code> via <code>alpha_variance*(1+node_depth)^-beta_variance</code>.</li> <li><code>beta_mu</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 for the prognostic forest.      Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>.</li> <li><code>beta_tau</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 for the treatment effect forest.      Tree split prior combines <code>alpha</code> and <code>beta</code> via <code>alpha*(1+node_depth)^-beta</code>.</li> <li><code>beta_variance</code> (<code>float</code>): Exponent that decreases split probabilities for nodes of depth &gt; 0 in the conditional variance model. Tree split prior combines <code>alpha_variance</code> and <code>beta_variance</code> via <code>alpha_variance*(1+node_depth)^-beta_variance</code>.</li> <li><code>min_samples_leaf_mu</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples, for the prognostic forest. Defaults to <code>5</code>.</li> <li><code>min_samples_leaf_tau</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples, for the treatment effect forest. Defaults to <code>5</code>.</li> <li><code>min_samples_leaf_variance</code> (<code>int</code>): Minimum allowable size of a leaf, in terms of training samples in the conditional variance model. Defaults to <code>5</code>.</li> <li><code>max_depth_mu</code> (<code>int</code>): Maximum depth of any tree in the mu ensemble. Defaults to <code>10</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>max_depth_tau</code> (<code>int</code>): Maximum depth of any tree in the tau ensemble. Defaults to <code>5</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>max_depth_variance</code> (<code>int</code>): Maximum depth of any tree in the ensemble in the conditional variance model. Defaults to <code>10</code>. Can be overriden with <code>-1</code> which does not enforce any depth limits on trees.</li> <li><code>a_global</code> (<code>float</code>): Shape parameter in the <code>IG(a_global, b_global)</code> global error variance model. Defaults to <code>0</code>.</li> <li><code>b_global</code> (<code>float</code>): Component of the scale parameter in the <code>IG(a_global, b_global)</code> global error variance prior. Defaults to <code>0</code>.</li> <li><code>a_leaf_mu</code> (<code>float</code>): Shape parameter in the <code>IG(a_leaf, b_leaf)</code> leaf node parameter variance model for the prognostic forest. Defaults to <code>3</code>.</li> <li><code>a_leaf_tau</code> (<code>float</code>): Shape parameter in the <code>IG(a_leaf, b_leaf)</code> leaf node parameter variance model for the treatment effect forest. Defaults to <code>3</code>.</li> <li><code>b_leaf_mu</code> (<code>float</code>): Scale parameter in the <code>IG(a_leaf, b_leaf)</code> leaf node parameter variance model for the prognostic forest. Calibrated internally as <code>0.5/num_trees</code> if not set here.</li> <li><code>b_leaf_tau</code> (<code>float</code>): Scale parameter in the <code>IG(a_leaf, b_leaf)</code> leaf node parameter variance model for the treatment effect forest. Calibrated internally as <code>0.5/num_trees</code> if not set here.</li> <li><code>sigma2_init</code> (<code>float</code>): Starting value of global variance parameter. Calibrated internally as in Sparapani et al (2021) if not set here.</li> <li><code>variance_forest_leaf_init</code> (<code>float</code>): Starting value of root forest prediction in conditional (heteroskedastic) error variance model. Calibrated internally as <code>np.log(pct_var_variance_forest_init*np.var((y-np.mean(y))/np.std(y)))/num_trees_variance</code> if not set.</li> <li><code>pct_var_sigma2_init</code> (<code>float</code>): Percentage of standardized outcome variance used to initialize global error variance parameter. Superseded by <code>sigma2</code>. Defaults to <code>0.25</code>.</li> <li><code>pct_var_variance_forest_init</code> (<code>float</code>): Percentage of standardized outcome variance used to initialize global error variance parameter. Default: <code>1</code>. Superseded by <code>variance_forest_init</code>.</li> <li><code>variable_weights_mean</code> (<code>np.</code>array<code>): Numeric weights reflecting the relative probability of splitting on each variable in the prognostic and treatment effect forests. Does not need to sum to 1 but cannot be negative. Defaults to</code>np.repeat(1/X_train.shape[1], X_train.shape[1])<code>if not set here. Note that if the propensity score is included as a covariate in either forest, its weight will default to</code>1/X_train.shape[1]<code>. A workaround if you wish to provide a custom weight for the propensity score is to include it as a column in</code>X_train<code>and then set</code>propensity_covariate<code>to</code>'none'<code>and adjust</code>keep_vars_mu<code>and</code>keep_vars_tau`` accordingly.</li> <li><code>variable_weights_variance</code> (<code>np.array</code>): Numeric weights reflecting the relative probability of splitting on each variable in the variance forest. Does not need to sum to 1 but cannot be negative. Defaults to uniform over the columns of <code>X_train</code> if not provided.</li> <li><code>keep_vars_mu</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be included in the prognostic (<code>mu(X)</code>) forest. Defaults to <code>None</code>.</li> <li><code>drop_vars_mu</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be excluded from the prognostic (<code>mu(X)</code>) forest. Defaults to <code>None</code>. If both <code>drop_vars_mu</code> and <code>keep_vars_mu</code> are set, <code>drop_vars_mu</code> will be ignored.</li> <li><code>drop_vars_variance</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be excluded from the variance (<code>sigma^2(X)</code>) forest. Defaults to <code>None</code>. If both <code>drop_vars_variance</code> and <code>keep_vars_variance</code> are set, <code>drop_vars_variance</code> will be ignored.</li> <li><code>keep_vars_tau</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be included in the treatment effect (<code>tau(X)</code>) forest. Defaults to <code>None</code>.</li> <li><code>drop_vars_tau</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be excluded from the treatment effect (<code>tau(X)</code>) forest. Defaults to <code>None</code>. If both <code>drop_vars_tau</code> and <code>keep_vars_tau</code> are set, <code>drop_vars_tau</code> will be ignored.</li> <li><code>drop_vars_variance</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be excluded from the variance (<code>sigma^2(X)</code>) forest. Defaults to <code>None</code>. If both <code>drop_vars_variance</code> and <code>keep_vars_variance</code> are set, <code>drop_vars_variance</code> will be ignored.</li> <li><code>keep_vars_variance</code> (<code>list</code> or <code>np.array</code>): Vector of variable names or column indices denoting variables that should be included in the variance (<code>sigma^2(X)</code>) forest. Defaults to <code>None</code>.</li> <li><code>num_trees_mu</code> (<code>int</code>): Number of trees in the prognostic forest. Defaults to <code>200</code>.</li> <li><code>num_trees_tau</code> (<code>int</code>): Number of trees in the treatment effect forest. Defaults to <code>50</code>.</li> <li><code>num_trees_variance</code> (<code>int</code>): Number of trees in the ensemble for the conditional variance model. Defaults to <code>0</code>. Variance is only modeled using a tree / forest if <code>num_trees_variance &gt; 0</code>.</li> <li><code>sample_sigma_global</code> (<code>bool</code>): Whether or not to update the <code>sigma^2</code> global error variance parameter based on <code>IG(a_global, b_global)</code>. Defaults to <code>True</code>.</li> <li><code>sample_sigma_leaf_mu</code> (<code>bool</code>): Whether or not to update the <code>tau</code> leaf scale variance parameter based on <code>IG(a_leaf, b_leaf)</code> for the prognostic forest.      Cannot (currently) be set to true if <code>basis_train</code> has more than one column. Defaults to <code>True</code>.</li> <li><code>sample_sigma_leaf_tau</code> (<code>bool</code>): Whether or not to update the <code>tau</code> leaf scale variance parameter based on <code>IG(a_leaf, b_leaf)</code> for the treatment effect forest.      Cannot (currently) be set to true if <code>basis_train</code> has more than one column. Defaults to <code>True</code>.</li> <li><code>propensity_covariate</code> (<code>str</code>): Whether to include the propensity score as a covariate in either or both of the forests. Enter <code>\"none\"</code> for neither, <code>\"mu\"</code> for the prognostic forest, <code>\"tau\"</code> for the treatment forest, and <code>\"both\"</code> for both forests.      If this is not <code>\"none\"</code> and a propensity score is not provided, it will be estimated from (<code>X_train</code>, <code>Z_train</code>) using <code>BARTModel</code>. Defaults to <code>\"mu\"</code>.</li> <li><code>adaptive_coding</code> (<code>bool</code>): Whether or not to use an \"adaptive coding\" scheme in which a binary treatment variable is not coded manually as (0,1) or (-1,1) but learned via      parameters <code>b_0</code> and <code>b_1</code> that attach to the outcome model <code>[b_0 (1-Z) + b_1 Z] tau(X)</code>. This is ignored when Z is not binary. Defaults to True.</li> <li><code>b_0</code> (<code>float</code>): Initial value of the \"control\" group coding parameter. This is ignored when <code>Z</code> is not binary. Default: <code>-0.5</code>.</li> <li><code>b_1</code> (<code>float</code>): Initial value of the \"treated\" group coding parameter. This is ignored when <code>Z</code> is not binary. Default: <code>0.5</code>.</li> <li><code>random_seed</code> (<code>int</code>): Integer parameterizing the C++ random number generator. If not specified, the C++ random number generator is seeded according to <code>std::random_device</code>.</li> <li><code>keep_burnin</code> (<code>bool</code>): Whether or not \"burnin\" samples should be included in predictions. Defaults to <code>False</code>. Ignored if <code>num_mcmc == 0</code>.</li> <li><code>keep_gfr</code> (<code>bool</code>): Whether or not \"warm-start\" / grow-from-root samples should be included in predictions. Defaults to <code>False</code>. Ignored if <code>num_mcmc == 0</code>.</li> <li><code>keep_every</code> (<code>int</code>): How many iterations of the burned-in MCMC sampler should be run before forests and parameters are retained. Defaults to <code>1</code>. Setting <code>keep_every = k</code> for some <code>k &gt; 1</code> will \"thin\" the MCMC samples by retaining every <code>k</code>-th sample, rather than simply every sample. This can reduce the autocorrelation of the MCMC samples.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>BCFModel</code> <p>Sampled BCF Model.</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.predict_tau","title":"<code>predict_tau(X, Z, propensity=None)</code>","text":"<p>Predict CATE function for every provided observation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array or DataFrame</code> <p>Test set covariates.</p> required <code>Z</code> <code>array</code> <p>Test set treatment indicators.</p> required <code>propensity</code> <code>`np.array`</code> <p>Optional test set propensities. Must be provided if propensities were provided when the model was sampled.</p> <code>None</code> <p>Returns:</p> Type Description <code>array</code> <p>Array with as many rows as in <code>X</code> and as many columns as retained samples of the algorithm.</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.predict_variance","title":"<code>predict_variance(covariates, propensity=None)</code>","text":"<p>Predict expected conditional variance from a BART model.</p> <p>Parameters:</p> Name Type Description Default <code>covariates</code> <code>array</code> <p>Test set covariates.</p> required <code>covariates</code> <code>array</code> <p>Test set propensity scores. Optional (not currently used in variance forests).</p> required <p>Returns:</p> Type Description <code>tuple of `np.array`</code> <p>Tuple of arrays of predictions corresponding to the variance forest. Each array will contain as many rows as in <code>covariates</code> and as many columns as retained samples of the algorithm.</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.predict","title":"<code>predict(X, Z, propensity=None)</code>","text":"<p>Predict outcome model components (CATE function and prognostic function) as well as overall outcome for every provided observation.  Predicted outcomes are computed as <code>yhat = mu_x + Z*tau_x</code> where mu_x is a sample of the prognostic function and tau_x is a sample of the treatment effect (CATE) function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array or DataFrame</code> <p>Test set covariates.</p> required <code>Z</code> <code>array</code> <p>Test set treatment indicators.</p> required <code>propensity</code> <code>`np.array`</code> <p>Optional test set propensities. Must be provided if propensities were provided when the model was sampled.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple of np.array</code> <p>Tuple of arrays with as many rows as in <code>X</code> and as many columns as retained samples of the algorithm.  The first entry of the tuple contains conditional average treatment effect (CATE) samples,  the second entry contains prognostic effect samples, and the third entry contains outcome prediction samples.  The optional fourth array contains variance forest samples.</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.to_json","title":"<code>to_json()</code>","text":"<p>Converts a sampled BART model to JSON string representation (which can then be saved to a file or  processed using the <code>json</code> library)</p> <p>Returns:</p> Type Description <code>`str`</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p>"},{"location":"python_docs/api/bcf.html#stochtree.bcf.BCFModel.from_json","title":"<code>from_json(json_string)</code>","text":"<p>Converts a JSON string to an in-memory BART model.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>`str`</code> <p>JSON string representing model metadata (hyperparameters), sampled parameters, and sampled forests</p> required"},{"location":"python_docs/demo/index.html","title":"StochTree Python API Demo","text":"<p>Overview of the <code>stochtree</code> python library's functionality</p>"},{"location":"python_docs/demo/causal_inference.html","title":"Causal Inference Demo Notebook","text":"<p>Load necessary libraries</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom stochtree import BCFModel\nfrom sklearn.model_selection import train_test_split\n</pre> import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from stochtree import BCFModel from sklearn.model_selection import train_test_split <p>Generate sample data</p> In\u00a0[2]: Copied! <pre># RNG\nrng = np.random.default_rng()\n\n# Generate covariates and basis\nn = 1000\np_X = 5\nX = rng.uniform(0, 1, (n, p_X))\npi_X = 0.25 + 0.5*X[:,0]\nZ = rng.binomial(1, pi_X, n).astype(float)\n\n# Define the outcome mean functions (prognostic and treatment effects)\nmu_X = pi_X*5 + 2*X[:,2]\ntau_X = (X[:,1]*2 - 1)\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\ny = mu_X + tau_X*Z + epsilon\n</pre> # RNG rng = np.random.default_rng()  # Generate covariates and basis n = 1000 p_X = 5 X = rng.uniform(0, 1, (n, p_X)) pi_X = 0.25 + 0.5*X[:,0] Z = rng.binomial(1, pi_X, n).astype(float)  # Define the outcome mean functions (prognostic and treatment effects) mu_X = pi_X*5 + 2*X[:,2] tau_X = (X[:,1]*2 - 1)  # Generate outcome epsilon = rng.normal(0, 1, n) y = mu_X + tau_X*Z + epsilon <p>Test-train split</p> In\u00a0[3]: Copied! <pre>sample_inds = np.arange(n)\ntrain_inds, test_inds = train_test_split(sample_inds, test_size=0.5)\nX_train = X[train_inds,:]\nX_test = X[test_inds,:]\nZ_train = Z[train_inds]\nZ_test = Z[test_inds]\ny_train = y[train_inds]\ny_test = y[test_inds]\npi_train = pi_X[train_inds]\npi_test = pi_X[test_inds]\nmu_train = mu_X[train_inds]\nmu_test = mu_X[test_inds]\ntau_train = tau_X[train_inds]\ntau_test = tau_X[test_inds]\n</pre> sample_inds = np.arange(n) train_inds, test_inds = train_test_split(sample_inds, test_size=0.5) X_train = X[train_inds,:] X_test = X[test_inds,:] Z_train = Z[train_inds] Z_test = Z[test_inds] y_train = y[train_inds] y_test = y[test_inds] pi_train = pi_X[train_inds] pi_test = pi_X[test_inds] mu_train = mu_X[train_inds] mu_test = mu_X[test_inds] tau_train = tau_X[train_inds] tau_test = tau_X[test_inds] <p>Run BCF</p> In\u00a0[4]: Copied! <pre>bcf_model = BCFModel()\nbcf_model.sample(X_train, Z_train, y_train, pi_train, X_test, Z_test, pi_test, num_gfr=10, num_mcmc=100, params={\"keep_every\": 5})\n</pre> bcf_model = BCFModel() bcf_model.sample(X_train, Z_train, y_train, pi_train, X_test, Z_test, pi_test, num_gfr=10, num_mcmc=100, params={\"keep_every\": 5}) <p>Inspect the MCMC (BART) samples</p> In\u00a0[5]: Copied! <pre>forest_preds_y_mcmc = bcf_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis = 1, keepdims = True)\ny_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(y_test,1), y_avg_mcmc), axis = 1), columns=[\"True outcome\", \"Average estimated outcome\"])\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bcf_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis = 1, keepdims = True) y_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(y_test,1), y_avg_mcmc), axis = 1), columns=[\"True outcome\", \"Average estimated outcome\"]) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3))) plt.show() In\u00a0[6]: Copied! <pre>forest_preds_tau_mcmc = bcf_model.tau_hat_test\ntau_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis = 1, keepdims = True)\ntau_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(tau_test,1), tau_avg_mcmc), axis = 1), columns=[\"True tau\", \"Average estimated tau\"])\nsns.scatterplot(data=tau_df_mcmc, x=\"True tau\", y=\"Average estimated tau\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\nplt.show()\n</pre> forest_preds_tau_mcmc = bcf_model.tau_hat_test tau_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis = 1, keepdims = True) tau_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(tau_test,1), tau_avg_mcmc), axis = 1), columns=[\"True tau\", \"Average estimated tau\"]) sns.scatterplot(data=tau_df_mcmc, x=\"True tau\", y=\"Average estimated tau\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3))) plt.show() In\u00a0[7]: Copied! <pre>forest_preds_mu_mcmc = bcf_model.mu_hat_test\nmu_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis = 1, keepdims = True)\nmu_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(mu_test,1), mu_avg_mcmc), axis = 1), columns=[\"True mu\", \"Average estimated mu\"])\nsns.scatterplot(data=mu_df_mcmc, x=\"True mu\", y=\"Average estimated mu\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\nplt.show()\n</pre> forest_preds_mu_mcmc = bcf_model.mu_hat_test mu_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis = 1, keepdims = True) mu_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(mu_test,1), mu_avg_mcmc), axis = 1), columns=[\"True mu\", \"Average estimated mu\"]) sns.scatterplot(data=mu_df_mcmc, x=\"True mu\", y=\"Average estimated mu\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3))) plt.show() In\u00a0[8]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bcf_model.num_samples),axis=1), np.expand_dims(bcf_model.global_var_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"])\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bcf_model.num_samples),axis=1), np.expand_dims(bcf_model.global_var_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"]) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\") plt.show() In\u00a0[9]: Copied! <pre>b_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bcf_model.num_samples),axis=1), np.expand_dims(bcf_model.b0_samples,axis=1), np.expand_dims(bcf_model.b1_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Beta_0\", \"Beta_1\"])\nsns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_0\")\nsns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_1\")\nplt.show()\n</pre> b_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bcf_model.num_samples),axis=1), np.expand_dims(bcf_model.b0_samples,axis=1), np.expand_dims(bcf_model.b1_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Beta_0\", \"Beta_1\"]) sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_0\") sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_1\") plt.show()"},{"location":"python_docs/demo/causal_inference.html#causal-inference-demo-notebook","title":"Causal Inference Demo Notebook\u00b6","text":""},{"location":"python_docs/demo/supervised_learning.html","title":"Supervised Learning Demo Notebook","text":"<p>Load necessary libraries</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom stochtree import BARTModel\nfrom sklearn.model_selection import train_test_split\n</pre> import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from stochtree import BARTModel from sklearn.model_selection import train_test_split <p>Generate sample data</p> In\u00a0[2]: Copied! <pre># RNG\nrandom_seed = 1234\nrng = np.random.default_rng(random_seed)\n\n# Generate covariates and basis\nn = 1000\np_X = 10\np_W = 1\nX = rng.uniform(0, 1, (n, p_X))\nW = rng.uniform(0, 1, (n, p_W))\n\n# Define the outcome mean function\ndef outcome_mean(X, W):\n    return np.where(\n        (X[:,0] &gt;= 0.0) &amp; (X[:,0] &lt; 0.25), -7.5 * W[:,0], \n        np.where(\n            (X[:,0] &gt;= 0.25) &amp; (X[:,0] &lt; 0.5), -2.5 * W[:,0], \n            np.where(\n                (X[:,0] &gt;= 0.5) &amp; (X[:,0] &lt; 0.75), 2.5 * W[:,0], \n                7.5 * W[:,0]\n            )\n        )\n    )\n\n# Generate outcome\nepsilon = rng.normal(0, 1, n)\ny = outcome_mean(X, W) + epsilon\n\n# Standardize outcome\ny_bar = np.mean(y)\ny_std = np.std(y)\nresid = (y-y_bar)/y_std\n</pre> # RNG random_seed = 1234 rng = np.random.default_rng(random_seed)  # Generate covariates and basis n = 1000 p_X = 10 p_W = 1 X = rng.uniform(0, 1, (n, p_X)) W = rng.uniform(0, 1, (n, p_W))  # Define the outcome mean function def outcome_mean(X, W):     return np.where(         (X[:,0] &gt;= 0.0) &amp; (X[:,0] &lt; 0.25), -7.5 * W[:,0],          np.where(             (X[:,0] &gt;= 0.25) &amp; (X[:,0] &lt; 0.5), -2.5 * W[:,0],              np.where(                 (X[:,0] &gt;= 0.5) &amp; (X[:,0] &lt; 0.75), 2.5 * W[:,0],                  7.5 * W[:,0]             )         )     )  # Generate outcome epsilon = rng.normal(0, 1, n) y = outcome_mean(X, W) + epsilon  # Standardize outcome y_bar = np.mean(y) y_std = np.std(y) resid = (y-y_bar)/y_std <p>Test-train split</p> In\u00a0[3]: Copied! <pre>sample_inds = np.arange(n)\ntrain_inds, test_inds = train_test_split(sample_inds, test_size=0.5)\nX_train = X[train_inds,:]\nX_test = X[test_inds,:]\nbasis_train = W[train_inds,:]\nbasis_test = W[test_inds,:]\ny_train = y[train_inds]\ny_test = y[test_inds]\n</pre> sample_inds = np.arange(n) train_inds, test_inds = train_test_split(sample_inds, test_size=0.5) X_train = X[train_inds,:] X_test = X[test_inds,:] basis_train = W[train_inds,:] basis_test = W[test_inds,:] y_train = y[train_inds] y_test = y[test_inds] <p>Run BART</p> In\u00a0[4]: Copied! <pre>bart_model = BARTModel()\nparam_dict = {\"num_chains\": 3}\nbart_model.sample(X_train=X_train, y_train=y_train, basis_train=basis_train, X_test=X_test, basis_test=basis_test, num_gfr=10, num_mcmc=100, params=param_dict)\n</pre> bart_model = BARTModel() param_dict = {\"num_chains\": 3} bart_model.sample(X_train=X_train, y_train=y_train, basis_train=basis_train, X_test=X_test, basis_test=basis_test, num_gfr=10, num_mcmc=100, params=param_dict) <p>Inspect the MCMC (BART) samples</p> In\u00a0[5]: Copied! <pre>forest_preds_y_mcmc = bart_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis = 1, keepdims = True)\ny_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(y_test,1), y_avg_mcmc), axis = 1), columns=[\"True outcome\", \"Average estimated outcome\"])\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bart_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis = 1, keepdims = True) y_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(y_test,1), y_avg_mcmc), axis = 1), columns=[\"True outcome\", \"Average estimated outcome\"]) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3))) plt.show() In\u00a0[6]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bart_model.num_samples),axis=1), np.expand_dims(bart_model.global_var_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"])\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bart_model.num_samples),axis=1), np.expand_dims(bart_model.global_var_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"]) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\") plt.show() <p>Compute the test set RMSE</p> In\u00a0[7]: Copied! <pre>np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc),2)))\n</pre> np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc),2))) Out[7]: <pre>np.float64(1.1615790535541024)</pre> <p>Run BART</p> In\u00a0[8]: Copied! <pre>bart_model = BARTModel()\nX_train_aug = np.c_[X_train, basis_train]\nX_test_aug = np.c_[X_test, basis_test]\nbart_model.sample(X_train=X_train_aug, y_train=y_train, X_test=X_test_aug, num_gfr=10, num_mcmc=100)\n</pre> bart_model = BARTModel() X_train_aug = np.c_[X_train, basis_train] X_test_aug = np.c_[X_test, basis_test] bart_model.sample(X_train=X_train_aug, y_train=y_train, X_test=X_test_aug, num_gfr=10, num_mcmc=100) <p>Inspect the MCMC (BART) samples</p> In\u00a0[9]: Copied! <pre>forest_preds_y_mcmc = bart_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis = 1, keepdims = True)\ny_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(y_test,1), y_avg_mcmc), axis = 1), columns=[\"True outcome\", \"Average estimated outcome\"])\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bart_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis = 1, keepdims = True) y_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(y_test,1), y_avg_mcmc), axis = 1), columns=[\"True outcome\", \"Average estimated outcome\"]) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3))) plt.show() In\u00a0[10]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bart_model.num_samples),axis=1), np.expand_dims(bart_model.global_var_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"])\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bart_model.num_samples),axis=1), np.expand_dims(bart_model.global_var_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"]) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\") plt.show() <p>Compute the test set RMSE</p> In\u00a0[11]: Copied! <pre>np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc),2)))\n</pre> np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc),2))) Out[11]: <pre>np.float64(1.274249041100934)</pre> <p>Run BART</p> In\u00a0[12]: Copied! <pre>bart_model = BARTModel()\nbart_model.sample(X_train=X_train, y_train=y_train, X_test=X_test, num_gfr=10, num_mcmc=100)\n</pre> bart_model = BARTModel() bart_model.sample(X_train=X_train, y_train=y_train, X_test=X_test, num_gfr=10, num_mcmc=100) <p>Inspect the MCMC (BART) samples</p> In\u00a0[13]: Copied! <pre>forest_preds_y_mcmc = bart_model.y_hat_test\ny_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis = 1, keepdims = True)\ny_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(y_test,1), y_avg_mcmc), axis = 1), columns=[\"True outcome\", \"Average estimated outcome\"])\nsns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\")\nplt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\nplt.show()\n</pre> forest_preds_y_mcmc = bart_model.y_hat_test y_avg_mcmc = np.squeeze(forest_preds_y_mcmc).mean(axis = 1, keepdims = True) y_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(y_test,1), y_avg_mcmc), axis = 1), columns=[\"True outcome\", \"Average estimated outcome\"]) sns.scatterplot(data=y_df_mcmc, x=\"Average estimated outcome\", y=\"True outcome\") plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3))) plt.show() In\u00a0[14]: Copied! <pre>sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bart_model.num_samples),axis=1), np.expand_dims(bart_model.global_var_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"])\nsns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\nplt.show()\n</pre> sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(bart_model.num_samples),axis=1), np.expand_dims(bart_model.global_var_samples,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"]) sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\") plt.show() <p>Compute the test set RMSE</p> In\u00a0[15]: Copied! <pre>np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc),2)))\n</pre> np.sqrt(np.mean(np.power(y_test - np.squeeze(y_avg_mcmc),2))) Out[15]: <pre>np.float64(2.0131734941429755)</pre>"},{"location":"python_docs/demo/supervised_learning.html#supervised-learning-demo-notebook","title":"Supervised Learning Demo Notebook\u00b6","text":""},{"location":"python_docs/demo/supervised_learning.html#demo-1-using-w-in-a-linear-leaf-regression","title":"Demo 1: Using <code>W</code> in a linear leaf regression\u00b6","text":""},{"location":"python_docs/demo/supervised_learning.html#demo-2-including-w-as-a-covariate-in-the-standard-constant-leaf-bart-model","title":"Demo 2: Including <code>W</code> as a covariate in the standard \"constant leaf\" BART model\u00b6","text":""},{"location":"python_docs/demo/supervised_learning.html#demo-3-omitting-w-entirely","title":"Demo 3: Omitting <code>W</code> entirely\u00b6","text":""}]}